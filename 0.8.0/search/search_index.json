{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"General/","title":"Home","text":""},{"location":"General/#what-is-the-handbook","title":"What is the handbook?","text":"<p>The handbook is a collection of resources for the BHK Lab. It is intended to be a resource for all BHK Lab members to learn about the lab, its projects, and how to get involved.</p> <p>Insipiration:</p> <ul> <li>Koesterlab Handbook</li> <li>Candice Morey Lab Handbook</li> <li>Baby Lab Handbook</li> <li>Lowe Power Lab Handbook</li> <li>Vortex Lab Handbook</li> </ul> <p>If you are new to the lab, make sure you start by reviewing all pages in this section (see the left sidebar) and in the Onboarding section.</p>"},{"location":"General/#quick-links","title":"Quick Links","text":"<p>Below are some frequently accessed resources on the handbook that you may find helpful:</p> Frequently Asked Question Link How do I set up the VPN again? Configuring VPN Help, I have a meeting presentation coming up! Presenting(Tip: Don\u2019t forget to review the Meeting Owl page) Uh\u2026 what\u2019s an SOW again? Summary of Work I'm on H4H and I don't know what I'm doing. Using HPC4H I need some advice and I don't know who to ask! Lab Member Expertise There's something else I need that's not on this handbook. Submitting an Issue"},{"location":"General/#want-to-contribute","title":"Want to Contribute?","text":"<p>If you're interested in contributing to the BHK Lab Handbook, we encourage you to check out our Contributing Section.</p> <p>Whether you're looking to report an issue, add new content, or suggest an enhancement, the Contributing Section has all the resources you need to get started. Your contributions help make this handbook a valuable tool for the entire lab community!</p> <p>This handbook is under active development and open-source!</p> <p>Visit the GitHub repository to see the latest updates and contribute to the project.</p>"},{"location":"General/lab_mission/","title":"Mission Statement","text":"<p>At BHK Lab, we are on a mission to revolutionize precision oncology by developing cutting-edge computational tools and predictive models. Our goal is to identify new cancer vulnerabilities and improve the delivery of precision medicine to patients.</p>"},{"location":"General/lab_mission/#research-focus","title":"Research Focus","text":"<p>Dive into the world of pharmacogenomics and imaging analysis using the power of machine learning and deep learning. We are all about using innovative approaches to understand cancer better and find new ways to combat it.</p>"},{"location":"General/lab_mission/#our-values","title":"Our Values","text":"<ul> <li> <p>Research Transparency: We believe in open science and transparency in our work.</p> </li> <li> <p>Reproducibility: Our research is solid, reproducible, and ready for peer review.</p> </li> <li> <p>Reusability: We make sure our tools and models can be easily reused by others to advance cancer research.</p> </li> </ul>"},{"location":"General/lab_mission/#inclusion-diversity-equity-and-accessibility-idea","title":"Inclusion, Diversity, Equity, and Accessibility (IDEA)","text":"<p>We are committed to making science accessible to everyone. Inclusion, diversity, equity, and accessibility are not just buzzwords for us; they are the guiding principles of our lab.</p>"},{"location":"General/lab_mission/#lab-culture","title":"Lab Culture","text":"<p>Step into our world of collaboration and creativity! We're a diverse team from various disciplines, coming together to tackle one of the biggest challenges in medicine and we thrive on supporting each other's ideas.</p> <p>Join us in our quest to slay cancer, one line of code at a time! \ud83d\ude80 \ud83e\uddec \ud83d\udcbb</p>"},{"location":"General/Awards_Achievements_Tracking/","title":"Awards &amp; Achivements Tracking","text":"<p>At BHK Lab, we encourage all members to regularly update their accomplishments. Whether you've attended a conference, received an award, or achieved any notable milestone, we want to celebrate and recognize your success! This also helps our lab administrators fill out activity reports for our lab.</p> <p>Please fill out the BHKLab Accomplishment Tracker Form to submit your awards and achievements. Keeping this information up to date helps highlight your contributions both within the lab and in the broader research community.</p> <p>Note</p> <p>This form must be filled out for each award or achievement individually.</p> <p>Thank you for helping us showcase the incredible work happening at BHK Lab!</p>"},{"location":"General/Code_Of_Conduct/","title":"BHKLab Code of Conduct","text":"<p>TODO::</p>"},{"location":"General/Communications/bhklab_calendar/","title":"BHKLab Calendar","text":"<p>The BHKLab Calendar is managed by <code>bhklab.research@gmail.com</code> and contains events for </p> <ul> <li>Mandatory lab meetings</li> <li>One-on-one meetings with BHK</li> <li>Any other events the lab should be aware of</li> </ul> <p>Your BHKLab Gmail account will be added to the BHKLab Calendar as part of your Onboarding. Make sure you check this calendar and your BHKLab email regularly so you are aware of any changes to the regular schedule or invitations to your assigned presentation days.</p> <p>Note</p> <p>The BHKLab Calendar will not automatically be added to your Google Calendar. Once you've been added to the BHKLab-Members group, visit the group's Conversations and open the email with subject line \"BHKLAB Research has shared a calendar with you\" </p> <p>You should see an email like the one below including the link to add the calendar to your Google Calendar.</p> <p></p>"},{"location":"General/Communications/bhklab_gmail/","title":"BHKLab Gmail","text":"<p>In the BHKLab, we primarily use Google Drive for document creation, storage, and sharing and Google Calendar for all lab meeting scheduling. To improve collaboration, communication, and security, we require lab members to create a dedicated BHKLab Gmail.</p> <p>This email will be used for</p> <ul> <li>All lab document handling on Google Drive</li> <li>Sharing the BHKLab Google calendar</li> <li>Scheduling presentations for lab meetings and journal club meetings</li> </ul> <p>You are also welcome to use it to create accounts for tools related to your lab work, such as Miro or Paperpile.</p> <p>Note</p> <p>Your specified preferred contact email from your onboarding will be used for the majority of communications, but lab documents and calendar events must be accessed with your BHKLab Gmail account.</p> <p>When your time in the BHKLab is complete, control of the account will be transferred to the lab manager so documents can be accessed and transferred as needed.</p>"},{"location":"General/Communications/bhklab_gmail/#creating-your-bhklab-gmail-account","title":"Creating your BHKLab Gmail account","text":"<p>Creation of this email should be done during your BHKLab Onboarding when you completed the Onboarding Form, but we will describe it again here. If you have created the account already, you can skip to step 2.</p> <ol> <li> <p>Follow the steps described on the Create a Gmail Account page.</p> <p>a. The format of your email address will be </p> <pre><code>bhklab.firstnamelastname@gmail.com\n</code></pre> <p>(ex. bhklab.johnsmith@gmail.com) </p> </li> <li> <p>Once you have created the account, we recommend setting up a dedicated browser profile for it. Here are instructions for setting up a profile in:</p> <ul> <li>Google Chrome</li> <li>Mozilla Firefox</li> <li>Safari</li> </ul> </li> <li> <p>Your BHKLab Gmail will be added to the BHKLab Members Google Group. This will grant you access to lab-wide Google Documents.</p> </li> </ol> <p>Your BHKLab Gmail account is now ready to use!</p>"},{"location":"General/Communications/bhklab_gmail/#integrating-bhklab-gmail-with-other-tools","title":"Integrating BHKLab Gmail with other tools","text":"<p>We recommend adding your BHKLab Gmail account to your regular email app or setting up email forwarding to make it easier to keep on top of shared lab documents and updates to calendar events.</p>"},{"location":"General/Communications/bhklab_gmail/#add-gmail-account-to-your-email-app","title":"Add Gmail account to your email app","text":"<p>Please note, you will need to complete this process for each device you want to receive lab emails on.</p> <ul> <li>Outlook - Quick start: Add an email account to Outlook</li> <li>Gmail - Add another email account to the Gmail app</li> <li>Mail on Mac (Apple) - Add email accounts in Mail on Mac</li> </ul>"},{"location":"General/Communications/bhklab_gmail/#set-up-email-forwarding","title":"Set up email forwarding","text":"<p>Follow the steps described on the Automatically forward Gmail messages to another account page.</p>"},{"location":"General/Communications/bhklab_gmail/#bhklab-google-drive","title":"BHKLab Google Drive","text":"<p>Your BHKLab Google Drive is where you will store all of your lab documents. Any work related to your lab projects must be stored here. In specific instances, documents may be shared with or from the <code>bhklab.research@gmail.com</code> account - this is one of our administrative accounts that allows us to keep track of lab documents. </p> <p>If your document needs to be shared with other lab members, you may use their BHKLab Gmail account OR the BHKLab Google Group to make the document accessible to the entire lab.</p> <p>Warning</p> <p>Personal Gmail accounts should not be used for lab documents unless you are a short-term member of the lab (less than 1 month).</p>"},{"location":"General/Communications/bhklab_gmail/#file-sharing","title":"File Sharing","text":"<p>If your document needs to be shared with other lab members, you may use their BHKLab Gmail account OR the BHKLab Google Group to make the document accessible to the entire lab. If more than three people need access to the document, we recommend sharing it with the BHKLab Members group instead.</p>"},{"location":"General/Communications/bhklab_gmail/#to-share-a-document-with-the-google-group","title":"To share a document with the Google Group:","text":"<ol> <li>Open the document you wish to share from your Google Drive.</li> <li>Click the Share button in the top right corner of the document.</li> <li> <p>In the \"Add people, groups, and calendar events\" text box, enter</p> <pre><code>bhklab-members@googlegroups.com\n</code></pre> </li> <li> <p>Set the permission level as makes sense for the document.</p> </li> <li>Uncheck the Notify people option.</li> <li>Click Share.</li> </ol>"},{"location":"General/Communications/bhklab_gmail/#if-you-need-to-share-multiple-documents-or-folders","title":"If you need to share multiple documents or folders:","text":"<ol> <li>In the directory menu, select all the documents and/or folders you wish to share (<code>Shift + Click</code> to select multiple).</li> <li>Right click on the selected items and select <code>Share</code> and then <code>Share</code> again.</li> <li> <p>In the \"Add people, groups, and calendar events\" text box, enter</p> <pre><code>bhklab-members@googlegroups.com\n</code></pre> </li> <li> <p>Set the permission level as makes sense for each document. These will need to be set for each document or folder.</p> </li> <li>Uncheck the Notify people option.</li> <li>Click Share.</li> </ol> <p>This sharing method is useful for transferring ownership of existing lab documents on your personal Gmail account to your BHKLab Gmail account.</p> <p>Note</p> <p>For documents and folders related to manuscripts or grants, or any other specified instance, ownership still needs to be transferred to <code>bhklab.research@gmail.com</code> in addition to sharing with the BHKLab Members group.</p>"},{"location":"General/Communications/bhklab_gmail/#bhklab-calendar","title":"BHKLab Calendar","text":"<p>The BHKLab Calendar is managed by <code>bhklab.research@gmail.com</code> and contains events for </p> <ul> <li>Mandatory lab meetings</li> <li>One-on-one meetings with BHK</li> <li>Any other events the lab should be aware of</li> </ul> <p>Your BHKLab Gmail account will be added to the BHKLab Calendar as part of your Onboarding. Make sure you check this calendar and your BHKLab email regularly so you are aware of any changes to the regular schedule or invitations to your assigned presentation days.</p> <p>Note</p> <p>The BHKLab Calendar will not automatically be added to your Google Calendar. Once you've been added to the BHKLab-Members group, visit the group's Conversations and open the email with subject line \"BHKLAB Research has shared a calendar with you\" </p> <p>You should see an email like the one below including the link to add the calendar to your Google Calendar.</p> <p></p>"},{"location":"General/Communications/bhklab_gmail/#integrating-your-bhklab-gmail-calendar-with-other-tools","title":"Integrating your BHKLab Gmail Calendar with other tools","text":"<p>Warning</p> <p>You cannot view the BHKLAB calendar with a non-BHKLab Gmail account in the browser version of Google Calendar. Do not add any personal Gmail accounts to the BHKLAB calendar to try to do so.</p> <ul> <li> <p>If you added the email to Outlook on your Desktop, the calendar should be added automatically. Click on the Calendar icon on the far left side of the window, click the dropdown for your BHKLab Gmail account in the left panel, and toggle on the BHKLAB calendar under Other Calendars.</p> </li> <li> <p>If you are using the Google Calendar app on your phone, you can add the BHKLab Gmail account to view the BHKLAB calendar.</p> </li> <li> <p>If you are adding it to the Apple iCal app, open System Settings, add the BHKLab Gmail to your Internet Accounts, and make sure Calendars is toggled on. If it still isn't showing up, go to the Google Sync Settings and make sure BHKLAB is selected.</p> </li> </ul>"},{"location":"General/Communications/slack/","title":"Slack","text":"<p>Daily intra-lab communications are done via Slack. We recommend using your BHKLab Gmail as your Slack e-mail as that will allow you to easily integrate with other lab tools such as the shared calendar.</p>"},{"location":"General/Communications/slack/#general-rules","title":"General Rules","text":"<p>Your communications on Slack should be professional. In particular what you post should comply with the Slack Terms of Service as well as the code of conduct of your institution.</p> <ul> <li>UHN Code of Conduct</li> <li>UofT Code of Conduct</li> </ul> <p>Being a jerk is strongly discouraged. </p> <p>Additionally, the lab Slack workspace is divided in to channels. Please use the channels for their intended purposes and following the guidelines below. </p>"},{"location":"General/Communications/slack/#channels","title":"Channels","text":"<p>The Slack has four main channels - <code>#general</code>, <code>#random</code>, <code>#publications</code>, and <code>#conferences-workshops</code>. We give guidelines for posting in each of these below. The rules for posting in  <code>#publications</code> are specific and require special attention.</p>"},{"location":"General/Communications/slack/#general","title":"General","text":"<p>The <code>#general</code> channel is where most lab-wide notices are posted. The content of <code>#general</code> is confined to the professional functioning of the BHK Lab and will contain things like:</p> <ul> <li>Deadline reminders (such as those for the Summary of Work)</li> <li>Scientific talks that may be of interest to lab members.</li> <li>Scheduling of lab-wide events (e.g., lab retreats, hackathons, etc.)</li> <li>Group food orders.</li> </ul> <p>For team-specific posting please use the proper channel (<code>#pharmacogenomics</code> or <code>#radiomics</code>). </p>"},{"location":"General/Communications/slack/#random","title":"Random","text":"<p><code>#random</code> is, as the name suggests, a catch-all channel for content that may be of interest to the whole lab but is not related to the professional activities. Examples of the kinds of things that get posted here are:</p> <ul> <li>Links to artistic events or side projects</li> <li>Events that may be of interest (e.g., food festivals, concerts.)</li> </ul>"},{"location":"General/Communications/slack/#publications","title":"Publications","text":"<p>We use the <code>#publications</code> channel exclusively to share publications and (briefly) discuss those that are shared. Publications should in one of the following categories:</p> <ol> <li>Papers related to ongoing BHK Lab projects.</li> <li>Papers of interest to the research areas of the BHK Lab and our collaborators. </li> </ol> <p>These categories can be interpretedly broadly. For example, if you have a computational method paper that you think could be useful for a pharmacogenomics or radiomics problem, feel free to share it. If you suspect the connection between the paper and BHK Lab research is non-obvious, you should include a comment explaining how you see a connection. Conversely, posting links to papers from math.AG, for example, without comment is unwise. </p> <p>Links to publications must follow the formatting guidelines below.</p>"},{"location":"General/Communications/slack/#posting-to-publications","title":"Posting to Publications","text":"<p>If you have a paper that you wish to share with the lab you must post it as a hyperlink where the text of the link is the title of the paper. Simply posting the link is Very Not Good and you'll be asked to fix it. </p> <p>Example</p> <p>Suppose you wanted to share this paper. </p> <p>A proper post to <code>#publications</code> would be:</p> <p>Efficient Evolutionary Models with Digraphons</p> <p>Posting  <code>https://arxiv.org/pdf/2104.12748</code> or <code>Efficient Evolutionary Models with Digraphons https://arxiv.org/pdf/2104.12748</code> will result in someone (usually Ben) asking you to correct it.</p> Slack Shortcut for Links <p>Cmd/Ctrl + Shift + U will open the \"Add Link\" menu.</p> <p>Additional Requirements - Threads</p> <ul> <li> <p>If the publication you link has restricted access, reply to your post in a thread with a PDF of the paper attached.</p> </li> <li> <p>If you want to comment on or discuss a paper posted by a lab member, respond in a thread. We want to keep the channel as just a list of publications to the extent possible.</p> </li> </ul>"},{"location":"General/Communications/slack/#conferences-and-workshops","title":"Conferences And Workshops","text":"<p>The <code>#conferences-workshops</code> channel exists for posting links to upcoming conferences and workshops which might be of interest for BHK Lab Members. </p>"},{"location":"General/Lab_Space/","title":"The BHK Lab Space","text":"<p>The BHKLab is located in room 11-401 of the Princess Margaret Cancer Research Tower (PMCRT) in the MARS Discovery District. We share this space with other research labs under the Princess Margaret Research umbrella. Our lab space is to the left when you enter the room.</p> <p>For directions to the lab, please see the BHKLab Onboarding Policy.</p>"},{"location":"General/Lab_Space/#accessing-the-lab","title":"Accessing the Lab","text":"<p>To access the lab, you must have your UHN photo ID card and PMCRT access card. Please see the UHN Onboarding Policy for instructions on how to obtain these cards.</p> Pro Tip: Forgot your PMCRT Access Card? <p>If you have forgotten your PMCRT Access Card, you can go to the PMCRT Security Office on the main floor across from the PMCRT elevators and ask to borrow an access card for the day. They require that you leave a piece of photo ID that you can pick back up at the end of the day when you return the access card.</p>"},{"location":"General/Lab_Space/#lab-workspace","title":"Lab Workspace","text":"<p>We have four rows of workspaces available in the lab. You will be assigned a workspace during your BHKLab onboarding process.</p> <p></p>"},{"location":"General/Lab_Space/#lab-equipment-inventory","title":"Lab Equipment Inventory","text":"<p>In the lab, each workspace is typically equipped with a monitor, keyboard, and a mouse. Unless otherwise specified, you are expected to bring in your own laptop. We utilize cloud-based services for computing and data storage, so your project work shouldn't take up too much space on your laptop.</p> <p>To keep track of the lab hardware, we use a Lab Equipment Inventory spreadsheet. Each piece of equipment in the lab is labelled with a circular sticker, numbered, and logged in the spreadsheet. You can review the spreadsheet to confirm which pieces of equipment you have access to. </p> Note: Accessing the Lab Equipment Inventory <p>This spreadsheet is only accessible to BHKLab members via their BHKLab Gmail account. Please contact your lab coordinator if you need access.</p> Warning: Missing Equipment <p>If a piece of equipment at your workspace is missing from the spreadsheet, or vice versa, please contact your lab coordinator.</p>"},{"location":"General/Lab_Space/#other-lab-space-resources","title":"Other Lab Space Resources","text":"<ul> <li>Washrooms are located on either end of the floor on every floor of the PMCRT. The women's washroom is on the south side (right from the elevators) and the men's washroom is on the north side (left from the elevators).</li> <li>There are kitchen facilities at either end of the floor on every floor of the PMCRT. A water dispenser is in the sink in both kitchens.</li> <li>The mini fridge to the left of the lab door and the kettle on top of it is available for use by lab members.</li> </ul>"},{"location":"General/Manuscripts/","title":"Manuscript Writing Guidelines","text":"<p>This guide outlines key steps for preparing a manuscript in the BHK Lab to streamline review and feedback from Benjamin and co-authors.</p>"},{"location":"General/Manuscripts/#1-initial-setup","title":"1. Initial Setup","text":"Step Action 1 Create a shared Google Drive folder for all manuscript materials 2 Share it with <code>bhaibeka@gmail.com</code>, co-authors, and add <code>bhklab.research@gmail.com</code> as Editor 3 Transfer ownership to <code>bhklab.research@gmail.com</code> 4 Organize into subfolders: Figures, Tables, Supplementary, Code, etc."},{"location":"General/Manuscripts/#2-add-legend-highlighting-guide-at-the-top-of-manuscript","title":"2. Add Legend &amp; Highlighting Guide at the Top of Manuscript","text":"<p>Add the following legend at the top of your manuscript:</p> Purpose Style Shared Directory \"[Google Drive URL]\" Editable Figures \"[Miro URL]\" Figures Tables_Supplementary_Info \"[Overleaf URL]\" Computer Code \"[GitHub URL]\" Executable Container \"[Code Ocean URL] (optional, if applicable)\" References \"[Paperpile link]\" <p>Then, use these highlighting rules consistently throughout the manuscript draft:</p> <p></p>"},{"location":"General/Manuscripts/#3-manuscript-format-checklist","title":"3. Manuscript Format Checklist","text":"Element Requirement Font Arial 11 Section Titles Use <code>HEADING 1</code> (uppercase) Subtitles Use <code>HEADING 2</code> Figures Created in Miro Tables Use Google Docs for small; Sheets or CSV for large Figure, Table, Supplementary References Placed in (parentheses) e.g. (Figure x), not \"Figure x shows...\" Captions Start with one bolded sentence summarizing its content. A figure/table caption must be sufficiently detailed to be understood on its own (i.e., without reading the rest of the manuscript). Supplementary Info Store in Overleaf (optional, if applicable) Code &amp; Scripts Upload to BHK GitHub Data Store and run in Code Ocean capsule (optional, if applicable) References Use Paperpile"},{"location":"General/Manuscripts/#4-author-institution-formatting","title":"4. Author &amp; Institution Formatting","text":"<ul> <li>List authors under the title with superscript numbers.</li> </ul> <p>Example</p> <p>Authors:    Sarah Lee\u00b2, Benjamin Haibe-Kains\u00b9 \u00b3</p> <p>Affiliations:    \u00b9 Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada    \u00b2 Department of Computer Science, University of Toronto, Toronto, Ontario, Canada    \u00b3 Vector Institute for Artificial Intelligence, Toronto, Ontario, Canada</p> List of common collaborators <pre><code>  Here is a list of departments and institutions that the BHK lab often collaborates with or works for:\n\n  1. Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada.  \n  2. Department of Medical Biophysics, University of Toronto, Toronto, Ontario, Canada.  \n  3. Joint Department of Medical Imaging, University of Toronto, Toronto, Canada  \n  4. Faculty of Medicine, University of Toronto, Toronto, Ontario, Canada.  \n  5. Vector Institute for Artificial Intelligence, Toronto, Ontario, Canada.\n</code></pre> <p>\ud83d\udcdd Note</p> <p>Before making any changes to formatting, structure, or manuscript components, please consult with the project lead and the corresponding author(s). All updates should align with the target journal\u2019s submission guidelines and policies.</p> <p>\ud83d\udccc Follow these guidelines to ensure fast, focused feedback from Benjamin. Consistency helps everyone involved.</p>"},{"location":"General/Manuscripts/cover_letter/","title":"Cover Letter","text":"<p>A strong cover letter introduces your manuscript, highlights its importance, and explains why it fits the journal.</p> <p>Keep it concise and aligned with the journal\u2019s scope. Your goal is to communicate:</p> <ul> <li>What the study is about  </li> <li>Why it matters  </li> <li>Why it fits the journal and its readership  </li> </ul>"},{"location":"General/Manuscripts/cover_letter/#what-to-include","title":"What to Include","text":"<ul> <li>Greeting: Address the editor by name if known.  </li> <li>Opening: Date, journal name, manuscript title, and article type (e.g., research, review).  </li> <li>Summary: Briefly describe the research question, key methods, and main findings.  </li> <li>Impact: Explain how your study advances the field and aligns with the journal\u2019s scope.  </li> <li>Closing: Identify the corresponding author and confirm compliance with journal policies.  </li> </ul>"},{"location":"General/Manuscripts/cover_letter/#paragraph-breakdown","title":"Paragraph Breakdown","text":"<p>First paragraph: include the title of your manuscript and the type of manuscript it is (e.g. review, research, case study). Then briefly explain the background to your study, the question you sought out to answer and why.</p> <p>Second paragraph: you should concisely explain what was done, the main findings and why they are significant.</p> <p>Third paragraph: here you should indicate why the readers of the journal would be interested in the work. Take your cues from the journal\u2019s aims and scope. For example if the journal requires that all work published has broad implications explain how your study fulfills this. It is also a good idea to include a sentence on the importance of the results to the field.</p>"},{"location":"General/Manuscripts/cover_letter/#required-statements","title":"Required Statements","text":"<ul> <li>\u201cThis manuscript has not been published elsewhere and is not under consideration by another journal.\u201d  </li> <li>\u201cAll authors have approved the manuscript and agree with its submission to [Journal Name].\u201d</li> </ul> <p>Use the letter to confidently position your work. Editors often read it before the manuscript itself.</p>"},{"location":"General/Manuscripts/cover_letter/#example-structure","title":"Example Structure","text":"<p>Example</p> <p>March 18, 2021</p> <p>Dear Editors,</p> <p>We are pleased to submit our manuscript entitled \u201cOrchestrating and sharing large multimodal data for transparent and reproducible research\u201d as an Article in the Nature Communications Journal. </p> <p>One of the fundamental challenges in research is ensuring that a study is fully reproducible, from data retrieval, to processing and subsequent analysis. With the continuous advances in biotechnology, including high-throughput molecular, pharmacological and radiological profiling, the intrinsic complexity of the data increased, making the computational pipelines used to process these data more intricate and difficult to replicate. Moreover, the development of new processing pipelines is allowing researchers to process the data in multiple ways, often extracting complementary information from the same assay. Importantly, with biotechnologies getting easier and cheaper to use, datasets are growing fast and are often updated, making it challenging for the community to keep track of all versions of the data. Therefore, there is a need for greater transparency, reproducibility and flexibility in the processing of large multimodal data to realize their full potential for research.</p> <p>To address these issues, we developed ORCESTRA (www.orcestra.ca), a cloud-based platform that provides a transparent, reproducible, and flexible computational framework for processing and sharing large multimodal data. The ORCESTRA platform integrates genomic, pharmacological and radiological profiles of biological samples through the orchestration of automated processing pipelines to curate customized and fully documented data objects for future analyses. Our workflows utilize a data processing and versioning tool enabling curation, standardization and flexible normalization of large multimodal data, focusing on, but not limited to, cancer research using high-throughput biotechnologies. Users can customize their data object generation by selecting different versions of a dataset and molecular profile data that is processed with multiple genomic tools, all with full transparency, as the data provenance of each data object is fully documented and objects are automatically assigned a Digital Object Identifier (DOI).</p> <p>We believe ORCESTRA will be a valuable contribution to the biomedical field, and will be of great interest to the readers of the Nature Communications journal. All authors have read and approved the final version of the manuscript. I confirm that the content of this manuscript has not been published previously and has not been submitted to another Journal. We have made our research fully reproducible by sharing all the data and documented code.   We thank you in advance for considering our manuscript for publication in the Journal.</p> <p>Sincerely, Benjamin Haibe-Kains Senior Scientist, Princess Margaret Cancer Centre Associate Professor, University of Toronto University Health Network</p>"},{"location":"General/Manuscripts/cover_letter/#sources","title":"Sources:","text":"<ul> <li>https://www.springer.com/gp/authors-editors/authorandreviewertutorials/submitting-to-a-journal-and-peer-review/cover-letters/10285574 </li> <li>https://support.nature.com/en/support/solutions/articles/6000245673-cover-letter-for-your-manuscript</li> </ul>"},{"location":"General/Manuscripts/manuscript_review_presentations/","title":"Manuscript Review Presentations","text":"<p>This section describes the goals, format and expectations of manuscript review in the lab.</p>"},{"location":"General/Manuscripts/manuscript_review_presentations/#goal","title":"Goal","text":"<p>This is to ensure the manuscript updates are discussed regularly, preferably with Benjamin attending.</p> <p>Manuscripts status can be found here - List Ongoing Papers BHK Lab</p>"},{"location":"General/Manuscripts/manuscript_review_presentations/#how-it-works","title":"How it works","text":"<ul> <li>Presenter is usually the first author. This person will be assigned in the first lab meeting of the month. This is usually based on completion status of manuscripts (usually closest to publishing goes first)</li> <li>Time the presentation so that you have at least 30 minutes for discussion</li> <li>Ensure you differentiate new updates vs. updates from your last manuscript presentation</li> <li>If multiple co-first authors are present, you can divide the results based on your individual contributions</li> </ul>"},{"location":"General/Manuscripts/manuscript_review_presentations/#what-to-present","title":"What to present","text":"<ul> <li> <p>Following the required structure of the manuscript you are submitting to, arrange the analysis. </p> </li> <li> <p>Please use the template as a guide -  Research Dissemination Update Template - DO NOT EDIT</p> </li> </ul> <p>Pro Tip</p> <p>Use a notebook or texteditor to capture feedback and suggestions from the lab. Or you can request Sisira/Ben to share the AI-generated meeting notes.</p>"},{"location":"General/Manuscripts/response_to_reviewers/","title":"Response to Reviewers","text":"<p>When revising a manuscript based on peer review, a well-structured response document is essential. It shows that you've addressed the feedback thoughtfully and improved the manuscript accordingly.</p>"},{"location":"General/Manuscripts/response_to_reviewers/#tips-for-writing","title":"Tips for Writing","text":"<ul> <li>Be clear, respectful, and constructive.  </li> <li>Acknowledge valid criticisms, and explain your rationale when you disagree.  </li> <li>Reference exact changes made in the revised manuscript.</li> </ul>"},{"location":"General/Manuscripts/response_to_reviewers/#format","title":"Format","text":"<ul> <li>Reviewer comments: Use black text to quote reviewer feedback.</li> <li>Your responses: Use blue text to clearly separate your reply.</li> <li>Number or bullet each comment for clarity.</li> <li>Refer to line numbers, sections, or figures in your revised manuscript wherever relevant.</li> </ul>"},{"location":"General/Manuscripts/response_to_reviewers/#example-structure","title":"Example Structure","text":"<p>Reviwer comments are in plain text, responses are bold and italicized</p> <p>Short Example</p> <p>Reviewer 1 The introduction is vague about the specific hypothesis being tested. </p> <p>Thank you for the feedback. We revised the second paragraph of the Introduction to clearly state the hypothesis: \"We hypothesized that [...].\" (Lines 45\u201350)</p> <p>Long Example</p> <p>Reviewer 2</p> <p>This is a well written manuscript presenting artificial intelligence approaches to define bimodal gene expression as a tool to determine drug sensitivity in cancer. </p> <p>The premise of the work is that bimodal gene assessment would be a superior predictor of drug sensitivity. This work could deeply influence drug development and enhance the preclinical target validation in a comprehensive and robust way. </p> <p>The authors present extensive data demonstrating the role of AI in assessing bimodal gene expression as a comprehensive biomarker strategy. </p> <p>There are many interesting findings in this manuscript, but there are several areas of uncertainty and statements that should be addressed to place these findings in context of current and past clinical drug development and its relevance. </p> <p>We thank the reviewer for her/his positive assessment of our manuscript. We have added our response to each comment below.</p> <p>Introduction:</p> <p>1) The introduction is somewhat unclear, are the authors focusing on gene expression as predictive markers of drug sensitivity assessment tools or prognostic markers as some of the examples that listed may suggest? </p> <p>The focus of the manuscript is on genes with bimodal expression as candidate predictive biomarkers of drug sensitivity. The examples showcase the usefulness of bimodal genes in various biomarker settings as a motivation for their relevance. We have enhanced the relevant Introduction section to remove any confusion. </p> <p>Revised manuscript - Section:</p> <p>For example, estrogen receptor (ESR1) bimodal expression defines two biological states within breast cancer patients. These states have been used to stratify breast cancer patients into the clinically-relevant subtypes (ER+/-) and derive treatment decisions. Another example in cancer genomics is the use of 73 bimodal genes within ovarian cancer to define molecular subtypes with distinct survival rate 12. We also have shown that epithelial-to-mesenchymal transition (EMT) related genes were found to be bimodal pan-cancer and predictive of response to statin class of drugs 13.</p> <p>\ud83d\udccc Use both the cover letter and the response to reviewers to frame your manuscript in the best light and demonstrate professionalism throughout the review process.</p>"},{"location":"General/Meetings/","title":"Meetings","text":"<p>Here\u2019s an overview of mandatory meetings to attend. See lab calendar for locations as this might vary as per room availability. Please check with your supervisor/mentor about additional meetings where you are required to join.</p> Lab-Wide Meetings Frequency Date and time Slack channel Lab meeting Weekly Tuesdays, 10.30-12 p.m. #general #random Journal club Bi-Weekly Wednesdays, 12-1 p.m. #general #random Manuscript Review Monthly (or as needed) Last Wednesday of the month, 12-1 p.m. <p>The following meetings are specific to disciplines in the lab and are mandatory for only members of that discipline. Other lab members are welcome to attend these meetings.</p> Discipline-specific Meetings Frequency Date and time Slack channel Radiomics team meeting Biweekly Tuesdays, 2-3 p.m. #radiomics PGx team meeting Biweekly Tuesdays, 2-3 p.m. #pharmacogenomics Software Developer team meeting Monthly Mondays, 12-1 p.m. #software-developer <p>We use The Meeting Owl for our meetings so they can be ran hybridly. See the Meeting Owl Basics page to learn more about how to use it.</p>"},{"location":"General/Meetings/Radiomics_meeting/","title":"Radiomics Meeting","text":""},{"location":"General/Meetings/Radiomics_meeting/#time-and-room","title":"Time and Room","text":"<p>Radiomics meeting happens every 2 weeks on Tuesday, 2:00 - 3:00pm at PMCRT 11-710.</p>"},{"location":"General/Meetings/Radiomics_meeting/#agenda","title":"Agenda","text":"<ul> <li>Flash updates: 5-10 minutes/person on what they\u2019re working on.</li> <li>Q&amp;A for project related challenges.</li> <li>Award/presentation updates, if any.</li> <li>Data updates: New ORCESTRA announcements for datasets, dataset suggestions, etc.  </li> <li>Grant updates.</li> </ul>"},{"location":"General/Meetings/Radiomics_meeting/#attendees","title":"Attendees","text":"<p>Attendance is mandatory for Radiomics lab members.</p>"},{"location":"General/Meetings/Radiomics_meeting/#resources","title":"Resources","text":"<p>The meeting notes need to be filled out for every meeting in the Radiomics meeting - coverage document.</p>"},{"location":"General/Meetings/Software_dev_meeting/","title":"Software Dev Meeting","text":""},{"location":"General/Meetings/Software_dev_meeting/#time-and-room","title":"Time and Room","text":"<p>Software Development team holds a virtual meeting on the last Monday of every month from 12p.m to 1 p.m. If you're a part of Software development team and would like to join, please ask Matthew to add you to the meeting invite.</p>"},{"location":"General/Meetings/Software_dev_meeting/#agenda","title":"Agenda","text":"<ul> <li>Provide updates on current tasks, including progress, completed work, and any blockers or challenges encountered.</li> <li>Review and prioritize new tasks based on urgency and impact.</li> <li>Brainstorm innovative and efficient ideas that could benefit the project, along with their potential impact.</li> <li>Q&amp;A for project and data related challenges.</li> <li>Award, presentation, and hackathon updates, if any</li> </ul>"},{"location":"General/Meetings/Software_dev_meeting/#attendees","title":"Attendees","text":"<p>Attendance is mandatory for all software developers in the lab</p>"},{"location":"General/Meetings/Software_dev_meeting/#resources","title":"Resources","text":"<p>The meeting notes need to be filled out for every meeting in the Software dev meeting - coverage document.</p>"},{"location":"General/Meetings/journal_club/","title":"Journal Club","text":"<p>Welcome to Journal Club! </p> <p>Whether you\u2019re new to the lab or looking for a refresher, this guide will help you navigate Journal Club (JC)\u2014our weekly session to explore research papers, exchange ideas, and grow as scientists. Let\u2019s dive in!  </p>"},{"location":"General/Meetings/journal_club/#what-is-journal-club","title":"What Is Journal Club?","text":"<p>Journal Club is a bi-weekly session designed to:</p> <ul> <li>Discuss: Explore research papers relevant to our lab or scientific interests.</li> <li>Analyze: Evaluate the strengths and weaknesses of each study.</li> <li>Collaborate: Share insights on how the paper could impact or inspire our work.</li> <li>Apply: Optionally consider incorporating the findings or methods into your projects.</li> </ul> <p>It\u2019s all about learning, critiquing, and getting inspired.</p>"},{"location":"General/Meetings/journal_club/#when-and-where","title":"When and Where?","text":"<ul> <li>Day: Wednesdays, usually the first and third week of the month  </li> <li>Time: 12:00\u20131:00 PM  </li> <li>Location: PMCRT 11-710  </li> <li>Format: Two presenters (30 minutes each)  </li> </ul>"},{"location":"General/Meetings/journal_club/#how-journal-club-works","title":"How Journal Club Works?","text":"<ul> <li> <p>Presenters:</p> <ul> <li>Option 1: Each presenter discusses a different paper.</li> <li>Option 2: Both presenters focus on the same paper, with one highlighting its strengths and the other critiquing its weaknesses.</li> </ul> </li> <li> <p>Interactive Elements:</p> <ul> <li>Dumb Questioner: A participant is randomly chosen to ask a \u201cdumb\u201d question, encouraging open and fun discussions.</li> <li>Audience Poll: For shared papers, attendees vote on whether the positive or critical perspective was more compelling.</li> </ul> </li> </ul>"},{"location":"General/Meetings/journal_club/#how-to-present-at-journal-club","title":"How to Present at Journal Club?","text":"<ol> <li> <p>Choose a paper:</p> <ul> <li>Select a paper that interests you or aligns with the lab\u2019s focus.</li> <li>Ensure its relevance and potential impact on ongoing projects.</li> </ul> </li> <li> <p>Share the paper:</p> <ul> <li>Upload the paper to Slack at least one week before the meeting for participants to review.</li> </ul> </li> <li> <p>Prepare your slides:</p> <ul> <li>Title Slide: Include the full paper title, complete author list, and what journal the paper was published in. Screenshotting the article webpage is allowed and encouraged!</li> <li>Introduction: Why did you choose this paper?  </li> <li>Methods: How was the study conducted?  </li> <li>Results:  What are the key findings?  </li> <li>Key Takeaways: What are the main insights from the study?  </li> <li>Your Perspective: What worked well? What could be improved?  </li> </ul> </li> <li> <p>Presentation day:</p> <ul> <li>On the day of the meeting, you are responsible for retrieving the Meeting Owl and ensuring that it is set up for the meeting. See the Meeting Owl Basics page to learn more about how to use it.</li> </ul> <p>Warning</p> <p>If you are planning on presenting remotely, please connect with someone onsite to ensure the Meeting Owl is set up for you. </p> </li> </ol>"},{"location":"General/Meetings/journal_club/#audience-role","title":"Audience Role","text":"<p>Even if you\u2019re not presenting, your participation is essential!  </p> <ul> <li>Read the paper beforehand, if possible.  </li> <li>Engage: Ask questions and share your insights.  </li> <li>Be Open: Every perspective adds value.  </li> </ul>"},{"location":"General/Meetings/journal_club/#resources","title":"Resources","text":"<ul> <li>After presenting, upload your slides to the JC Slides List to maintain a shared record for future reference.</li> </ul> <p>Journal Club is a great way to stay curious, collaborate, and learn together. We look forward to seeing you there! </p>"},{"location":"General/Meetings/lab_meeting/","title":"Lab Meeting","text":""},{"location":"General/Meetings/lab_meeting/#details","title":"Details","text":"<p>Time: Every Tuesday from 10:30 AM to 12:00 PM.</p> <p>Venue: PMCRT 4-204.</p> <p>Presenter(s): One.</p>"},{"location":"General/Meetings/lab_meeting/#status-updates","title":"Status Updates","text":"<p>Every lab meeting begins with a 15-20 minute status update. See pre lab meeting catchup slides for a list of status update materials.</p>"},{"location":"General/Meetings/lab_meeting/#meeting-objectives","title":"Meeting Objectives","text":"<p>During lab meetings, you may present the research idea and feasibility, preliminary analyses, progress of your work, or any topics that require feedback from the lab.</p> <p>You will be sent an email notifying you of the date of presentation as per the lab schedule. If you want to (re)schedule a lab meeting, please contact the lab coordinator after finding a person to switch your presentation with.</p> <p>On the day of the meeting, you are responsible for retrieving the Meeting Owl and ensuring that it is set up for the meeting. See the Meeting Owl Basics page to learn more about how to use it.</p> <p>Warning</p> <p>If you are planning on presenting remotely, please connect with someone onsite to ensure the Meeting Owl is set up for you. </p>"},{"location":"General/Meetings/lab_meeting/#slides","title":"Slides","text":"<p>It is best to follow a standard slide format:</p> <ul> <li>Agenda</li> <li>Introduction</li> <li>Methods</li> <li>Results</li> <li>Challenges</li> <li>Key Takeaway/Learning</li> <li>Future Directions</li> </ul> <p>Tip</p> <p>Feel free to use the BHK lab-wide templates for the slide deck:</p> <ul> <li> <p>BHKLab External Presentation Template</p> </li> <li> <p>BHKLab Internal Presentation Template</p> </li> </ul>"},{"location":"General/Meetings/owl_basics/","title":"The Meeting Owl","text":"<p>The Owl is a device that works as video and audio input for all meetings conducted in the BHK lab. It is pivotal to include the Owl in all lab meetings to bridge the gap between the users tuning in remote to the users on site.</p>"},{"location":"General/Meetings/owl_basics/#how-to-findobtain-the-owl","title":"How to find/obtain the Owl","text":"<ol> <li>Go to Jermiah's work station (W-16).</li> <li>Grab the key located on the bottom side of Jermiah's desk.</li> <li>Using the key open the top drawer of the filing cabinet under Jermiah's workstation and grab the key labelled \"Owl key\".</li> <li>Use this new key to open the drawer labelled 'OWL' on the filing cabinet under the right end of the whiteboards and pull out the green Owl duffle bag.</li> </ol>"},{"location":"General/Meetings/owl_basics/#how-to-use-the-owl","title":"How to use the Owl","text":"<p>The Owl has two major setup components. </p> <ol> <li> <p>The Owl must be connected to an electrical outlet and also be connected to your computer via a USB connection. There is a USB-C dongle in the end pocket of the duffle bag for this purpose. Both wires plug into the bottom of the Owl.</p> </li> <li> <p>Once the Owl is connected to power and your computer, set it down in a spot that will be able to easily see/hear everyone participating in the meeting physically. If you're hosting the meeting you also want to verify that the Owl is being used as the audio and video input to the call.</p> </li> </ol> <p>Tip</p> <p>Make sure you are not using a blurred or photo background when using the Owl.</p>"},{"location":"General/Meetings/pgx_meeting/","title":"Pharmacogenomics Meeting","text":"<p>The goal of this meeting is to discuss individual PGx projects, check for existing data, models, brainstorm issues/ideas - maybe someone in the team has already found a solution earlier. </p>"},{"location":"General/Meetings/pgx_meeting/#meeting-frequency","title":"Meeting frequency","text":"<p>Once or twice a month, 1 hour max</p>"},{"location":"General/Meetings/pgx_meeting/#meeting-style","title":"Meeting style","text":"<p>Rotating presenters from different subteams/projects listed on Miro can self-assign themselved one week before the meeting. If you do not see your project here, please add.</p> <p>Slack reminders are set up to <code>#pharmacogenomics</code> channel a week in advance.</p> <p>Presenters self assign yourself one week in advance by adding to PGx Meeting - coverage</p>"},{"location":"General/Meetings/pgx_meeting/#general-agenda","title":"General agenda","text":"<ul> <li>Present your work with figures and visuals, avoid verbal-only updates (20 mins)</li> <li>Brainstorming or group discussion (20 mins)</li> <li>Wrap-up, mediated by Sisira/lead (10 mins)</li> <li>Summary of discussion</li> <li>Action items for the presented project</li> </ul>"},{"location":"General/Meetings/pgx_meeting/#meeting-ground-rules","title":"Meeting Ground Rules","text":"<ol> <li>No meeting unless there is at least 1 presenter signed up a week before. </li> <li>If not, any PGx meember can cancel the meeting by deleting the PGx meeting on the lab calendar.</li> <li>If there is no meeting, add async updates on Google doc tracker.</li> <li>All active PGx members should attend unless there are valid reasons. This should be communicated with Sisira/lead beforehand.</li> <li>Respect time: Stick to time limits to keep the agenda moving.</li> </ol>"},{"location":"General/Meetings/project_tracking/","title":"Project Tracking","text":""},{"location":"General/Meetings/project_tracking/#for-the-first-10-15-minutes-of-every-lab-meeting-the-lab-discusses-the-progress-of-the-projects","title":"For the first 10-15 minutes of every lab meeting, the lab discusses the progress of the projects.","text":"<p>When it is your turn to present, you must include a copy of this slide at the beginning of your slide deck.  We will be using it to go through the progress of the projects.</p> <p>We use four different spreadsheets to track updates. Each of them will be linked in their subsection below. </p>"},{"location":"General/Meetings/project_tracking/#to-dos-deadlines-for-bhk","title":"To Dos &amp; Deadlines for BHK","text":"<p>If there are any upcoming tasks or deadlines that you need Ben to address, this spreadsheet is where you add your requests. The following are the columns in the spreadsheet:</p> <ul> <li>Task: Add a general descriptive title for the task.</li> <li>Deadline: Add the deadline for this task.</li> <li>BHK review deadline: Add the deadline for when you need Ben to review the task.</li> <li>Lab Members: The lab members involved in this task. (If it's just yourself, only write your name. If it's multiple people, write all their names.)</li> <li>Comments: Add any additional comments that you think are necessary.</li> <li>Relevant links: Add any relevant links that are necessary for this task.</li> </ul>"},{"location":"General/Meetings/project_tracking/#manuscript-progress","title":"Manuscript Progress","text":"<p>If you have any updates on the progress of your manuscript, this spreadsheet is where you add your updates. The following are the columns in the spreadsheet:</p> <ul> <li>Lead Author (Mandatory): Add the name of the lead author of the manuscript. There can be multiple names if there are multiple lead authors working on this manuscript.</li> <li>Paper (Mandatory): Add the title of the manuscript.</li> <li>URL to the Folder (Mandatory): Add the URL to the folder where the manuscript and relevant files, such as figures, are stored.</li> <li>Status: A drop-down menu where you can select the status of the manuscript. For example, if the manuscript has been submitted, you can select 'Submitted to journal'</li> <li>Comments: Add any additional comments that you think are necessary. This will include any updates on the progress of the manuscript, such as if there are experiments being done, any review necessary, etc. This will typically be updated during pre-lab meeting overview.</li> </ul>"},{"location":"General/Meetings/project_tracking/#grants-award-applications","title":"Grants &amp; Award Applications","text":"<p>Information and updates regarding any grants or awards the lab is applying for can be found in this spreadsheet.  The following are the columns in the spreadsheet:</p> <ul> <li>Proposed project/idea: Add a general descriptive title for the project or idea.</li> <li>Name of the funding agency: Add the name of the funding agency you are applying to.</li> <li>Link to funding call: Add the URL to the funding call.</li> <li>Year of Submission: Add the year you are submitting the application.</li> <li>Application Type: Add the type of application you are submitting. For example, if you are submitting a grant, select 'Grant' from the drop-down menu.</li> <li>Applicants: Add the names of the applicants involved in this application. (If it's just yourself, only write your name. If it's multiple people, write all their names.)</li> <li>Name of PI: Add the name of the PI for this application.</li> <li>Name of Co-PIs: Add the names of the Co-PIs for this application.</li> <li>Deadline for Abstract: Add the deadline for the abstract submission.</li> <li>Deadline for LOI: Add the deadline for the Letter of Intent submission.</li> <li>Deadline for Full Application: Add the deadline for the full application submission.</li> <li>Status by Action: Add the current status of the application using the drop-down menu.</li> <li>Link to the Folder: Add the URL to the folder where the application and relevant files are stored.</li> <li>Comments: Add any additional comments that you think are necessary.</li> </ul>"},{"location":"General/Meetings/project_tracking/#data-access-status","title":"Data Access Status","text":"<p>Throughout your time in the lab, you may want to request access to additional datasets. This spreadsheet is where you can track the status of your data access requests. The following are the columns in the spreadsheet:</p> <ul> <li>Dataset Name: Add the name of the dataset you are requesting access to.</li> <li>Request Link: Add the URL to the request form or page, or where you can find more information on how to request access.</li> <li>Data Type: Add the type of data you are requesting access to; for example: RNA-seq, clinical data, etc.</li> <li>Lab Project: Add the lab project that this dataset is relevant to.</li> <li>Model Type: Add the type of model this data is using; for example: mouse model, cell line, patient data, etc.</li> <li>Processed Data: Add whether the data is raw or processed.</li> <li>Name of Requestor: Add the name of the person requesting access to the data.</li> <li>Lab Personnel in Charge: Add the name of the lab personnel in charge of this data request.</li> <li>Priority: Add the priority level of this data request.</li> <li>Submission Status: Add the status of the submission using the drop-down menu; for example, if the request has been approved, you can select 'Approved'.</li> <li>Data Location: Add the location of the data once the request has been approved; for example, if the data is stored in H4H, GCP, etc.</li> <li>Comments: Add any additional comments that you think are necessary.</li> </ul>"},{"location":"General/Presentations/","title":"Presentation Standards","text":""},{"location":"General/Presentations/#talk-standards","title":"Talk Standards","text":""},{"location":"General/Presentations/#key-slides","title":"Key Slides","text":"<ul> <li>Title slide: Include your name, date, affiliation, event/conference, and contact information (e.g. email). Optional: include the BHK Lab logo and others (e.g. UHN, University of Toronto, etc)</li> <li>Acknowledgements slide: Include names of lab members (or can reference 'BHK Lab') and other collaborators along with your institutional affilications/funding sources (e.g. UHN, CIHR, NSERC)</li> </ul>"},{"location":"General/Presentations/#other-slides","title":"Other slides","text":"<ul> <li>All slides should include slide numbers</li> <li>Slide headings should be short (1-2 lines max) and descriptive</li> <li>Use visuals whenever possible (feel free to take from existing presentations in the database)</li> </ul>"},{"location":"General/Presentations/#poster-standards","title":"Poster Standards","text":""},{"location":"General/Presentations/#components","title":"Components","text":"<ul> <li>Heading: Include title, author(s) name, supervisor name (if student poster) across the top border</li> <li>Abstract (optional): Brief paragraph (~250 words) outlining purpose, methods, results, and conclusions</li> <li>Introduction: Background information pertinent to your project to help audience understand motivation. Outline the goal, objective(s), and hypothesis(es) of your research.</li> <li>Materials and Methods: Brief outline of materials and methods used in your work, listed clearly and logically.</li> <li>Results: Present data in photographic, graphical, or tabular form. Include descriptive figure titles and avoid lengthy captions. Be sure to not include abbreviations not explained in the text.</li> <li>Discussion: Address results and describe relevance to objective(s) and hypothesis(es).</li> <li>Conclusions: Stated clearly and concisely, addressing project objectives and stating overall significance.</li> <li>References: All references (publications)</li> <li>Acknowledgements: Any key lab members, collaborators, and funding sources.</li> </ul>"},{"location":"General/Presentations/#text-sizing","title":"Text Sizing","text":"<p>Size of lettering must be large enough to be legible from approximately 2m. Use a clear and simple font between 18 point and 30 point in size. We recommend the following for Arial:</p> <ul> <li>18 point: best viewed at 1m (for figure titles, legends, acknowledgements, etc)</li> <li>24 point: best viewed at 2m (for main text)</li> <li>30 point: best viewed at 3m (for section headings)</li> </ul>"},{"location":"General/Presentations/#suggestions","title":"Suggestions","text":"<ul> <li>Avoid overloading with text, use bullet points where possible</li> <li>Whenever appropriate, use clear diagrams, figures, and tables (e.g. materials and methods)</li> </ul>"},{"location":"General/Presentations/database/","title":"Presentations Database","text":"<p>This page contains links to previously created presentations and is intended to support the creation of future presentations.</p>"},{"location":"General/Presentations/database/#2024-presentations","title":"2024 Presentations","text":"<p>Radiomics and Pharmacogenomics: From Research To Clinic (Slide Deck)</p> <p> Joint Symposium in Cancer Biology and Ecosystem, October 2024</p> <p>Pharmacogenomics Data Analysis (Slide Decks)</p> <p> Canadian Bioinformatics Workshop, October 2024</p> <p>An Open Science Approach to Computational Pharmacogenomics (Slide Deck)</p> <p> RECOMB/ISCB Regulatory &amp; Systems Genomics/DREAM conference, October 2024</p> <p>An Open Science Approach to Drug Response Prediction in Sarcoma (Slide Deck)</p> <p> 2024 NLMSF-SPAGN International LMS Research Roundtable, September 2024</p> <p>Development of Chromatin Accessibility Liquid Biopsy Biomarkers for Breast Cancer Drug Response Prediction (Slide Deck)</p> <p> 2024 Collaborative Breast Research Internal Award, June 2024</p> <p>Hallmarks of Drug Response Models: A Qualitative Framework to Evaluate Multivariable Predictive Biomarkers (Slide Deck)</p> <p> AACR Annual Meeting - San Diego, April 2024</p> <p>AI for Clinical Trials (Slide Deck)</p> <p> Amplitude, April 2024</p>"},{"location":"General/Presentations/database/#previous-presentations","title":"Previous Presentations","text":"<p>For presentations from 2023 and before, please visit the BHKLab website</p>"},{"location":"General/Presentations/database/#other-templates","title":"Other templates","text":"<p>Poster Template for Princess Margaret Cancer Centre (Drive link to template)</p>"},{"location":"General/Presentations/tools/","title":"Presentation Tools","text":"<p>This page describes standards, tools, and resources for creating presentations for the lab.</p>"},{"location":"General/Presentations/tools/#google-sheets","title":"Google Sheets","text":"<p>All external presentations (conferences, workshops, external working groups), should be created on Google Sheets directly by or shared to the <code>bhklab.research@gmail.com</code> account.</p> <p>Internal presentations, such as lab meetings and journal clubs, should also be created on Google Sheets.</p>"},{"location":"General/Presentations/tools/#templates","title":"Templates","text":"<p>Several slide templates have already been developed for the lab: </p> <ul> <li>BHKLab Internal Presentations Template</li> <li>BHKLab External Presentations Template</li> </ul>"},{"location":"General/Presentations/tools/#miro","title":"Miro","text":"<p>Miro is a collaborative white board platform that enables simple diagram and multi-figure panel creation for presentations and/or manuscripts. For diagrams that will be used and updated by others in the lab, please request a Miro Board be created via the BHK Lab Miro account.</p>"},{"location":"General/Presentations/tools/#images-and-icons","title":"Images and Icons","text":"<p>The following are open-source biological imaging databases and platforms. The BHK Lab holds a premium Flaticon account. If Biorender and/or BioArt are used, please remember to add them to the acknowledgements.</p> <ul> <li>Flaticon</li> <li>Biorender</li> <li>BioArt</li> </ul>"},{"location":"General/Presentations/tools/#slido","title":"Slido","text":"<p>Slido is an interactive polling platform that can be easily integrated into Google Sheets. We recommend using this platform to improve engagement in presentations, particularly workshops or other relevant use cases.</p>"},{"location":"General/Projects/","title":"Projects","text":""},{"location":"General/Summary_Of_Work/","title":"Summary of Work Tutorial","text":"<p>Every week, you are required to fill out a Summary of Work (SOW) document that will be reviewed by Ben. This is especially important when Ben is away from the lab, so he can stay up to date on your project progress.</p> <p>This tutorial will walk you through how to set up a Google Drive SOW directory and yearly document, and how to write your weekly SOW.</p>"},{"location":"General/Summary_Of_Work/#prerequisites","title":"Prerequisites","text":"<p>Visit the Google Drive website and ensure you are logged into the correct account.</p> Use the Correct Google Account <p>This tutorial will utilize the Google Drive associated with the BHKLab Gmail account that you would have set up during the  BHKLab Onboarding process. Ensure you are logged into the correct account before continuing.</p> <p>If you are a short-term employee (less than one month), you may complete the instructions below with a personal Gmail account.</p>"},{"location":"General/Summary_Of_Work/#setting-up-your-sow-space","title":"Setting Up Your SOW Space","text":"Note <p>This section will need to be repeated in January of each year.</p>"},{"location":"General/Summary_Of_Work/#directory-creation","title":"Directory Creation","text":"<p>In your BHKLab Google Drive, create a folder called <code>SOW</code>. In the sharing menu (1), add <code>bhklab-admin@googlegroups.com</code> as an Editor.</p> <ol> <li><code>right-click</code> on the folder and select <code>Share</code>.</li> </ol> <p>Within this folder, create another folder and name it the current year (e.g. <code>2025</code>).</p>"},{"location":"General/Summary_Of_Work/#document-creation","title":"Document Creation","text":"<p>Next, open the SOW template document. In the File menu, select <code>Make a copy</code>.</p> <ul> <li>Name the copy as <code>&lt;Your Name&gt;-SOW-&lt;Year&gt;</code>. (e.g. <code>John-SOW-2025</code>).</li> <li>Select the year folder you just created as the destination.</li> <li>Check the box for <code>Share it with the same people</code></li> <li>Click <code>Make a copy</code></li> </ul> <p>Once created, make sure the sharing permissions include <code>bhklab-admin@googlegroups.com</code>.</p>"},{"location":"General/Summary_Of_Work/#add-your-page-to-the-main-sow-tracker","title":"Add Your Page to the Main SOW Tracker","text":"<p>In the SOW for BHK document, find your job title. Under this heading, insert a File smart chip (1). To find your SOW file, start typing your name and it should appear as one of the options.</p> <ol> <li>type <code>@</code> and start typing the name of the file you created (i.e. <code>John-SOW-2025</code>). Click on the file when it appears in the dropdown.</li> </ol> <p>This is where Ben will access your SOW from weekly.</p>"},{"location":"General/Summary_Of_Work/#creating-your-sow-page","title":"Creating Your SOW Page","text":""},{"location":"General/Summary_Of_Work/#set-up-this-weeks-tab","title":"Set up this week's tab","text":"<p>Open your SOW document you copied in the Document Creation section.</p> <ol> <li>In the document tab menu on the left hand side, duplicate an existing SOW page.</li> <li>Drag the duplicated tab to the top of the list.</li> <li>Rename the tab to be the current week's dates.</li> </ol>"},{"location":"General/Summary_Of_Work/#writing-your-sow","title":"Writing Your SOW","text":"<p>Date: Set the dates at the top of the page to the first and last day of the current week (usually Monday and Friday).</p> <p>Win: Detail work done on projects during the week.</p> <p>Needs Input: Any questions or concerns for BHK. BHK will respond to these on Slack.</p> <p>Focus: What you plan to work on next week.</p> <p>Important links: Links to anything you mentioned in the above sections.</p> <p>BHK Read Marker: For BHK use only.</p> <p>Note</p> <p>If you copied the previous week\u2019s SOW, make that this is set to unchecked. Unchecked:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Checked:\u00a0\u00a0\u00a0</p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/","title":"Confluence SOW Tutorial (outdated)","text":"<p>Outdated Warning</p> <p>This tutorial is outdated and no longer in use. Please refer to the Google Drive SOW Tutorial for the most up-to-date instructions.</p> <p>This tutorial will walk you through how to access Confluence, set up your Page Tree, and to write your weekly Summary of Work (SOW). </p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#logging-on-to-confluence","title":"Logging on to Confluence","text":"<p>Make sure you are connected to either  </p> <ol> <li>the UHN-wireless-corporate wifi if you are on-site  </li> <li>the UHN VPN via GlobalProtect or otherwise if you are off-site or the UHN-wireless-corporate wifi is unavailable</li> </ol> <p>Note</p> <p>You cannot connect to the VPN if you are connected to the UHN-wireless corporate wifi </p> <p>Head to this link: BHKLab SOW Confluence Space. You should see a page like this:  </p> <p>Log in using your UHN username (usually your TID) and your UHN password.</p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#navigating-to-the-bhklab-sow-space","title":"Navigating to the BHKLab-SOW Space","text":"<p>The link from step 2 should take you directly to the BHKLab-SOW space once you\u2019re logged in that looks like this: </p> <p>If you end up on a different page, navigate to the Spaces dropdown and select BHKLab-SOW: </p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#finding-your-section-in-the-page-tree","title":"Finding your section in the Page Tree","text":"<p> On the left side of the BHKLab-SOW page, find the PAGE TREE section and find the position name that you fall under. </p> <p>This will likely be the only one with an arrow (&gt;) next to it. </p> <p>Click this to list the files under this page, which should be a page with your first and last name.</p> <p>Note</p> <p>If your name does not exist under the position you can open, stop here and contact a lab manager. They will set up the page appropriately for you.</p> <p>Click the page with your name on it. </p> <p>You should now see a page like this with your name as the title.</p> <p></p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#setting-up-the-year-page-for-your-sows","title":"Setting up the year page for your SOWs","text":"<p>While on the page with Your Name on it, click the blue Create button at the top of the screen.</p> <p>You should now see a blank page that you can edit.  </p> <p>Complete the following:</p> <ol> <li> <p>In the Page title section, enter the year and your initials in the format YYYY - YN</p> </li> <li> <p>Click on the red lock icon next to the DRAFT label. This will open a Restrictions menu. Click on the inherited view restrictions link (it will be blue). </p> </li> <li> <p>Under Your Name, confirm that BHK and a lab manager\u2019s names are listed along with your own. This ensures that no one else can view your SOWs besides them. </p> <p>Warning</p> <p>If BHK and Sisira are not listed, contact a lab manager to have them set up the permissions properly.</p> </li> <li> <p>Click the blue Publish button at the bottom right of the screen.</p> </li> </ol> <p>You should now see the page you just published listed under Your Name on the Page Tree. This is where you will navigate to to create your SOWs each week. When a new year starts, you will neeed to repeat this section to create a new year page. </p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#creating-your-sow-page","title":"Creating your SOW Page","text":"<p>Navigate to the page under your name with the current year and your initials. There are two options to create this week\u2019s SOW:    </p> <p>Note</p> <p>If this is your first ever SOW, follow From Create template</p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#from-create-template","title":"From Create template","text":"<ol> <li> <p>Click on the blue ellipses (...) button next to Create at the top of the page.</p> </li> <li> <p>From the Create pop-up menu, scroll down and find the template labeled BHKLab-SOW.</p> <p>a. Select this and click Create on the bottom right of the pop-up. </p> </li> </ol>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#from-previous-weeks-sow","title":"From previous week\u2019s SOW","text":"<ol> <li> <p>In the page tree, click on your last published SOW</p> </li> <li> <p>In the top right, click on the ellipses (...) and select Copy from the dropdown menu </p> </li> <li> <p>In the Copy page menu, make sure the parent page is set to the right year and click the blue Copy button </p> </li> </ol>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#writing-your-sow","title":"Writing your SOW","text":"<p>Date: Set the dates in the yellow box to the first and last day of the current week (usually Monday and Friday)</p> <p>Page Title: Write the dates exactly as shown in the yellow Date box and your initials</p> <p>Note</p> <p>Note: the initials must be included in the title as Confluence won\u2019t accept pages with the exact same name (e.g. you and another labmate post an SOW without initials)**  </p> <p>Win: Detail work done on projects during the week</p> <p>Needs Input: Any questions or concerns for BHK. BHK will respond to these on Slack.</p> <p>Focus: What you plan to work on next week</p> <p>Important links: Links to anything you mentioned in the above sections.</p> <p>BHK Read Marker: For BHK use only. </p> <p>Note</p> <p>If you copied the previous week\u2019s SOW, make that this is set to unchecked. Unchecked:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Checked:\u00a0\u00a0\u00a0</p> <p>Once completed, hit the blue Publish button in the bottom right or use <code>Ctrl + S</code> or <code>Cmd + S</code>. Your SOW should now be located under the year - your initials page in the Page Tree at the bottom of your SOW list. THIS NEEDS TO BE MOVED TO THE TOP. </p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#reordering-your-sow-pages","title":"Reordering your SOW pages","text":"<p>Navigate to Space Tools by either: </p> <ol> <li> <p>Clicking on the ellipses (...) menu at the top right of the SOW page and selecting View in Hierarchy</p> </li> <li> <p>Clicking Space tools at the bottom of the PAGE TREE section and selecting Reorder pages</p> </li> </ol> <p>On the Space Tools page, click and drag your most recent SOW and move it so it is directly below the year - initials page:     Before:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0After:  </p> <p>Click on the latest SOW to confirm it\u2019s been properly moved.</p> <p>You have now completed your SOW for the week!</p>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#draft-sow","title":"Draft SOW","text":"<p>If your SOW page is not showing up in the PAGE TREE, it has likely been saved to your page drafts.</p> <ol> <li> <p>Click the Confluence logo in the top left of the site.</p> </li> <li> <p>You should now be on the homepage with Recently worked on files listed. Your draft SOW will be listed here if it was saved. </p> </li> <li> <p>Click on it to continue editing and make sure to hit the Publish button in the bottom right. </p> </li> </ol>"},{"location":"General/Summary_Of_Work/old_confluence_instructions/#turning-off-notifications","title":"Turning off Notifications","text":"<p>If you want to turn off email notifications from Confluence: </p> <ol> <li> <p>Click on your profile photo in the top right and select Settings.</p> </li> <li> <p>Select Email under the left panel labeled Your Settings.</p> </li> <li> <p>Select Edit at the bottom of the page, and uncheck Recommended Updates and/or Daily Updates.</p> </li> </ol>"},{"location":"contributing/","title":"Contributing to the Lab Handbook","text":"<p>Thank you for your interest in contributing to the Lab Handbook!</p> <p>This section will guide you through the process of setting up your development environment, installing dependencies, and building the documentation.</p>"},{"location":"contributing/#contributing-principles","title":"Contributing Principles","text":"<p>The Lab Handbook is a community-driven effort, and we welcome contributions from everyone. Here are some principles we follow when reviewing and approving contributions:</p>"},{"location":"contributing/#by-the-lab-for-the-lab","title":"By the lab, for the lab","text":"<ul> <li>Pages can be created, edited, and reviewed by anyone in the lab</li> <li>Every page is understandable by anyone in the lab</li> <li>Maintenance and upkeep is a responsibility shared by everyone in the lab</li> </ul>"},{"location":"contributing/#kiss-keep-it-simple-stupid","title":"KISS - Keep It Simple, Stupid","text":"<p>(or Keep It Short and Simple)</p> <ul> <li>We strive to keep the handbook focused and concise to make it easy for anyone to understand</li> <li>We avoid unnecessary complexity</li> </ul>"},{"location":"contributing/#dry-dont-repeat-yourself","title":"DRY - Don't Repeat Yourself","text":"<ul> <li>Saves time and energy</li> <li>If details already exist on another page or website, link to it, don\u2019t copy paste</li> <li>When making a page, remember it might get referred to somewhere else!</li> </ul>"},{"location":"contributing/#for-contributors","title":"For Contributors","text":"<p>Below are some common questions to help you find the right guidance for specific contributing tasks.</p>"},{"location":"contributing/#where-should-i-start-if-i-want-to-contribute","title":"Where should I start if I want to contribute?","text":"<p>If you\u2019re new to contributing, start with the Prerequisites page to set up your environment and install necessary dependencies.</p>"},{"location":"contributing/#how-do-i-submit-an-issue-for-a-problem-i-found","title":"How do I submit an issue for a problem I found?","text":"<p>If you\u2019ve identified an issue, refer to the Submitting Issues page.  It provides details on how to use our issue templates and submit effective bug reports or feature requests.</p>"},{"location":"contributing/#how-can-i-add-new-content-to-the-handbook","title":"How can I add new content to the handbook?","text":"<p>To learn about adding or editing content, check out the Adding Content page.  This guide explains how to structure and format your additions to fit seamlessly into the handbook.</p>"},{"location":"contributing/#what-steps-are-involved-in-the-review-process","title":"What steps are involved in the review process?","text":"<p>The Reviewing a Contribution page outlines the review process, including best practices for constructive feedback and steps for both reviewers and contributors.</p>"},{"location":"contributing/#where-can-i-view-all-the-changes-made-to-the-handbook","title":"Where can I view all the changes made to the handbook?","text":"<p>To see recent updates and changes to the documentation, visit the Changelog page for a record of all documented changes.</p> <p>By following these resources, you'll have everything you need to contribute effectively to the Lab Handbook. If you have additional questions, feel free to reach out to the maintainers or submit a question through the Submitting Issues page.</p>"},{"location":"contributing/#for-maintainers","title":"For Maintainers","text":""},{"location":"contributing/#what-should-i-know-about-merging-pull-requests","title":"What should I know about merging pull requests?","text":"<p>For guidance on merging contributions, visit the Merging PR page.  It explains our review process, how to address review comments, and merge a PR successfully.</p>"},{"location":"contributing/#how-do-i-prepare-for-releasing-and-deploying-changes","title":"How do I prepare for releasing and deploying changes?","text":"<p>Check out the Release &amp; Deployment page to learn about our release procedures, automated workflows, and deployment to GitHub Pages.</p>"},{"location":"contributing/adding_content/","title":"Adding Content","text":""},{"location":"contributing/adding_content/#introduction","title":"Introduction","text":"<p>This document will guide you through the process of adding a new page to the handbook.</p> <p>Tip</p> <p>The handbook is built using MkDocs and Material for MkDocs. These tools also have extensive documentation and guides for contributing to a Mkdocs project.</p> <p>Please refer to their respective documentation first for any questions you might have.</p>"},{"location":"contributing/adding_content/#adding-content-to-the-documentation","title":"Adding Content to the Documentation","text":"<p>The documentation is written in Markdown and can be found in the <code>docs</code> directory.</p> <p>Here are the steps to add new content to the documentation:</p>"},{"location":"contributing/adding_content/#1-create-a-branch-for-your-changes","title":"1. Create a branch for your changes","text":"Note: Naming your branch <p>A branch is a way to work on a new feature or bug fix without affecting the main branch. The standard for this project is to use the following format:</p> <p><pre><code>$ &lt;author-ID&gt;/&lt;purpose-of-branch&gt;\nOR\n$ &lt;author-ID&gt;/&lt;issue-reference&gt;\n</code></pre> Where <code>&lt;author-ID&gt;</code> can be a GitHub username or an alias.</p> <p><code>&lt;purpose-of-branch&gt;</code> is a short description of the changes you are making. <code>&lt;issue-reference&gt;</code> is the number of the issue you are working on.</p> <p>For example:</p> <pre><code>$ jjjermiah/adding-getting-started-page\nOR\n$ jjjermiah/13-docs-finish-tutorial-for-page-review\n</code></pre> <p>To create a new branch and switch to it, run the following command:</p> <pre><code>git checkout -b &lt;branch-name&gt;\n$ git checkout -b jjjermiah/adding-getting-started-page\n</code></pre> <p>If you already have a named branch, you can switch to it with the following command:</p> <pre><code>git switch &lt;branch-name&gt;\n$ git switch jjjermiah/adding-getting-started-page\n</code></pre>"},{"location":"contributing/adding_content/#2-add-your-new-content-to-the-docs-directory","title":"2. Add your new content to the <code>docs</code> directory","text":"How do I know where to create my file? <p>The command below will create an empty Markdown file called <code>my_new_page.md</code> in the <code>docs/onboarding_offboarding</code> directory. The relative path to the <code>docs</code> directory, will be the link to your new page.  i.e the link to your new page will be <code>&lt;website-url&gt;/handbook/onboarding_offboarding/my_new_page/</code></p> <p>Let's say you want to add a new page to the <code>Onboarding/Offboarding</code> section. You would add a new file to the <code>docs/onboarding_offboarding</code> directory.</p> <pre><code>$ touch docs/onboarding_offboarding/my_new_page.md\nYou should now see a new file at `docs/onboarding_offboarding/my_new_page.md`.\n</code></pre> <p>You may need to add your page to the <code>.pages</code> file</p> <p>If you are adding a new page to the handbook, you may need to add the new page to the <code>.pages</code> file that lives in the same directory in which your new page is located. This file is used to generate the navigation menu for the handbook.</p> <p>To add your new page to the <code>.pages</code> file, open the file and add the relative path to your new page. For example, if you added a new page to the <code>onboarding_offboarding</code> directory, you would add the following line (highlighted in green) to the <code>onboarding_offboarding/.pages</code> file:</p> <pre><code>title: Onboarding / Offboarding\n\nnav:\n    - Onboarding\n    - Offboarding\n+   - onboarding_offboarding/my_new_page.md\n</code></pre> <p>This will add a link to your new page in the navigation menu.</p> <p>To learn more about how to actually write content, see the Handbook MkDocs Page and Handbook Markdown page.</p>"},{"location":"contributing/adding_content/#3-preview-your-changes","title":"3. Preview your changes","text":"<p>The following is a <code>pixi task</code> that will start a local server and preview the documentation at <code>http://localhost:8001</code> (aka <code>http://127.0.0.1:8001</code>).</p> <pre><code>$ pixi run serve\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\n...\nINFO    -  [08:55:05] Serving on http://127.0.0.1:8001/handbook/\n</code></pre> <p>You should see your changes appear at <code>http://127.0.0.1:8001/handbook/onboarding/my_new_page/</code></p> <p>Tip</p> <p>You can set the handbook website to automatically open in your default browser by using the <code>-o</code> flag:</p> <pre><code>pixi run serve -o\n</code></pre> <p>About the port number</p> <p>By default, we host the local site on port <code>8001</code> because it is more likely to be unused and available for the local server to use. In the case that you would like to manually specify a different port (e.g. if it's in use by something else), you can use the <code>-a</code> flag after <code>pixi run serve</code>.</p> <p>For example, to run on port <code>1234</code>: <pre><code>pixi run serve -a localhost:1234\n</code></pre></p>"},{"location":"contributing/adding_content/#4-commit-and-push-your-changes-to-your-branch","title":"4. Commit and push your changes to your branch","text":"<pre><code>git add .\ngit commit -m \"Add new getting started page\"\ngit push --set-upstream origin jjjermiah/adding-getting-started-page\n</code></pre>"},{"location":"contributing/adding_content/#5-create-a-pr","title":"5. Create a PR","text":"<p>Create a pull request (PR) to merge your changes into the main branch. Request a review from a maintainer.</p> <p>See the section on Reviewing a Contribution for more information.</p>"},{"location":"contributing/changelog/","title":"Handbook Changes Over Time","text":""},{"location":"contributing/changelog/#changelog","title":"Changelog","text":""},{"location":"contributing/changelog/#080-2025-08-05","title":"0.8.0 (2025-08-05)","text":""},{"location":"contributing/changelog/#features","title":"Features","text":"<ul> <li>add about the lab space info to General (#199) (24f53e0)</li> <li>add admonition describing how to deal with failing bioconductor packages (#238) (562a689)</li> <li>add basic and advanced resources to R page (#210) (9dd401c)</li> <li>add imaging data sources (#177) (da89d88)</li> <li>add instructions on submitting budget to the UHN Grants office (#179) (931c279)</li> <li>add lab off-boarding policy and admin instructions (#178) (fd8d383)</li> <li>add Med-ImageTools to Disciplines/Imaging/Tools index page (#157) (ff05b2c)</li> <li>add ML learning resources and tools links (#215) (1835dec)</li> <li>add MOLAB dataset to imaging data sources (#266) (ab41c19)</li> <li>Add MultiAssayExperiment document (#196) (b291012)</li> <li>add onboarding admin checklist (#223) (e6cc0a3)</li> <li>add page for quickstart on H4H (#259) (7aeb9bd)</li> <li>add page of resource links for JavaScript web dev (#214) (1ab2cf1)</li> <li>add pixi package manager page (#234) (27a85b8)</li> <li>add Plots section and Sankey page (#254) (163c171)</li> <li>add quick links and pointers for new members to homepage (#190) (77d074b)</li> <li>add section about Mac specific step for SSH forwarding setup for lab server (#152) (14e532d)</li> <li>add steps to after exit interview section (#239) (5e8834d)</li> <li>add TreatmentResponseExperiment tutorial with diagrams (#250) (9d84ad6)</li> <li>add walkthrough for accessing PSet Data (#253) (1424ebc)</li> <li>added awards and achievements tracking under general (#194) (5bacb4c)</li> <li>Added compute engine page (#258) (6c77dd5)</li> <li>Added guide for lab mentors (#252) (90a1ce7)</li> <li>added literature review tools section (#235) (e7e5a0e)</li> <li>added page to explain orcestra version controlling (#195) (1770998)</li> <li>Added pages from med-imagenet to disciplines/Imaging (#86) (0c3a226)</li> <li>Added pages on PSets, Dimensionality Reduction, PCA, and Clustering.  (#251) (c5a2e10)</li> <li>added pgx meeting info (#188) (e820a40)</li> <li>Added Resources tab (#153) (ef6ea07)</li> <li>Added vertex ai introduction (#198) (5104ff3)</li> <li>Adding content to the JavaScript/TypeScript page to provide a general understanding and usage in the lab (#154) (bb81a74)</li> <li>adding grants section (#176) (1a27101)</li> <li>adding manuscript guideline (#208) (659158a)</li> <li>Adding Slack page to communications tab (#197) (52ed882)</li> <li>Addition of google analytics for usage tracking (#183) (f3be6a1)</li> <li>address documentation build warnings and errors (#205) (d7546ae)</li> <li>Guanqiaofeng/adding kallistso pipeline page (#193) (8b5953f)</li> <li>migrate bioinformatics and SQL education resources (#216) (0d3e5dd)</li> <li>migrate Software Development educational resources (#212) (2341357)</li> <li>radiomics software pages (#265) (74d04ce)</li> <li>rewrite clinical trial curation pages (#206) (5505e4a)</li> <li>Sisiranair/adding resources section (#226) (9e0f56a)</li> <li>software development meeting page (#191) (1242693)</li> <li>update radiomics page (#189) (9bc4670)</li> <li>When to use buckets for serving images/videos page (#175) (8942145)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>correct example sbatch file by removing partition specification and changing mail-type START to BEGIN (#261) (4600a38)</li> <li>GCP Vertex AI for ml (#201) (07e2f07)</li> <li>minor_fixes_for_onboarding (#224) (91c4dc8)</li> <li>remove duplicate kallisto page, correct spelling errors (#203) (a638c11)</li> <li>typo of bhklab.archive email, typo in Project Data (84c1997)</li> <li>typos on Offboarding page (#232) (84c1997)</li> </ul>"},{"location":"contributing/changelog/#070-2025-02-14","title":"0.7.0 (2025-02-14)","text":""},{"location":"contributing/changelog/#features_1","title":"Features","text":"<ul> <li>Add \"What is Machine Learning\" page to the handbook (#84) (34b3fab)</li> <li>Add Communications section and BHKLab Gmail page (#131) (648743d)</li> <li>add Introduction to git slides to version control docs and fix up quick git tips (#123) (8eb99fa)</li> <li>add lab server page to Remote_Development (#136) (142eaec)</li> <li>Add new lab expertise page (#145) (bd3be40)</li> <li>added note about adding the BHKLab calendar to your BHKLab Gmail with the email (aa72aa8)</li> <li>Adding a new page for STAR alignment (#93) (ed2e6de)</li> <li>Google Cloud Platform page added (#92) (b2fea5c)</li> <li>make all tables sortable (#147) (a89b5ae)</li> <li>migrate information from the Employee Onboarding Policy (#127) (def834d)</li> <li>update BHKLAB calendar setup (#143) (35d1dff)</li> <li>update BHKLab Gmail page (#135) (0f2dddd)</li> <li>update-SOW-instructions (#138) (b4cee1f)</li> <li>updated BHKLab Calendar instructions (#142) (aa72aa8)</li> <li>updates to the journal club page (#139) (de22192)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>PGx meetings are biweekly (#133) (4777bcd)</li> <li>update BHKLab Google group to be Members instead of Google Accou\u2026 (#134) (51a7ec2)</li> <li>update BHKLab Google group to be Members instead of Google Accounts and update hyperlink (51a7ec2)</li> </ul>"},{"location":"contributing/changelog/#060-2024-11-27","title":"0.6.0 (2024-11-27)","text":""},{"location":"contributing/changelog/#features_2","title":"Features","text":"<ul> <li>add a page for DESeq (#91) (73d51b6)</li> <li>add angular commit description to contributing (#100) (cd6ee02)</li> <li>add concurrency settings to GitHub workflows to prevent workflows pushing to gh-pages at the same time from failing (6735fd5)</li> <li>add documentation on code-reviews, and embed the presented slides  (#111) (e03e6ef)</li> <li>add presentation tools and database (#108) (9c81b17)</li> <li>add QIPCM image retrieval guide (#102) (55cebe5)</li> <li>add table of contents to Confluence SOW tutorial page (#99) (d73f29b)</li> <li>added \"basics of RNAseq\" under Disciplines/Bioinformatics/Data_Types (#88) (c6390aa)</li> <li>added journal club md (#89) (1f9557f)</li> <li>Added Owl page under General/Meetings/ (#90) (3982918)</li> <li>added page on BHKlab meeting under General/Meetings/ (#85) (f534b03)</li> <li>added project tracking  (#94) (a1f250e)</li> <li>added radiomics-meeting-page under General/Meetings (#83) (680f1d4)</li> <li>added sow tutorial page under General (#82) (b8ce45c)</li> <li>Added tidyverse page under <code>software_development/languages/R</code> (#87) (1a9c2a1)</li> <li>cgeady add slicer info (#109) (8779235)</li> <li>Creating vpn page (#95) (d5c9d8a)</li> <li>migrate Clinical Trial Curation page from BHKLab Confluence (#105) (fe2386b)</li> <li>update directory structure with all section heading pages for assigned pages for tutorial (#58) (72f314a)</li> <li>updates to RNA-Seq information (#122) (92607f3)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>add ... to all .pages (b0d375f)</li> <li>fix <code>pixi run serve</code> warnings (#107) (40c23c6)</li> <li>incremented default port to 8001 (#120) (b086865)</li> <li>Rename Hackathon Page Request to hackathon_page_request (#66) (a37262f)</li> <li>Rename hackathon_page_request to hackathon_page_request.md (0f3f821)</li> <li>update information in Contributing section (#121) (c623418)</li> </ul>"},{"location":"contributing/changelog/#050-2024-11-06","title":"0.5.0 (2024-11-06)","text":""},{"location":"contributing/changelog/#features_3","title":"Features","text":"<ul> <li>add footnotes support and enhance social links in MkDocs configuration (1a4dd35)</li> <li>add guiding principles for contributing to the handbook (#52) (31e196e)</li> <li>add header autohide feature, reorganize markdown extensions (fc994d8)</li> <li>add markdownlint configuration and ignore rules for improved linting (0fe0d8a)</li> <li>enhance contributing documentation with clearer instructions and new examples (6d7cc6b)</li> <li>link hpc4health site under software development (#51) (f7361aa)</li> <li>update onboarding / offboarding section (#53) (3c86b05)</li> </ul>"},{"location":"contributing/changelog/#040-2024-11-04","title":"0.4.0 (2024-11-04)","text":""},{"location":"contributing/changelog/#features_4","title":"Features","text":"<ul> <li>add Markdown and MkDocs documentation sections for improved clarity and guidance (90d0e7e)</li> <li>add repository name to mkdocs configuration and update icon reference (6bc7cd1)</li> <li>improve clarity in contributing guide by refining section headings and adding notes (f8b37f1)</li> <li>refactor software development documentation structure with new sections on development environment and related tools (05eb33b)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>formatting (5c25c0e)</li> <li>formatting (5c25c0e)</li> <li>git config --local only if we checkout (f819f89)</li> <li>lint files (2c958fb)</li> <li>revise handbook introduction and contribute guidelines for clarity and better onboarding experience (2c958fb)</li> </ul>"},{"location":"contributing/changelog/#031-2024-10-29","title":"0.3.1 (2024-10-29)","text":""},{"location":"contributing/changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>use github action for deployment (f48ce66)</li> </ul>"},{"location":"contributing/changelog/#030-2024-10-29","title":"0.3.0 (2024-10-29)","text":""},{"location":"contributing/changelog/#features_5","title":"Features","text":"<ul> <li>enhance release workflow to publish version of docs using mike after PR release (72345a7)</li> <li>use mike (ec14831)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>rename redirect (252f3e2)</li> <li>update docs to explain mike (72345a7)</li> </ul>"},{"location":"contributing/changelog/#020-2024-10-29","title":"0.2.0 (2024-10-29)","text":""},{"location":"contributing/changelog/#features_6","title":"Features","text":"<ul> <li>Add issue templates (#28) (0a2c861)</li> <li>Add release and deployment documentation; update pixi.lock for dependencies and versions (27bd2fd)</li> <li>Create detailed guide for submitting issues, including templates and tips for effective reporting (ac69542)</li> <li>replace discipline temp files with index.md pages, organize disciplines (ae1bb27)</li> <li>Update issue templates for bug reports, content updates, documentation questions, and enhancements (30ee8d5)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>broken links in disciplines home tab (9393314)</li> <li>CI to fetch from depth 0 (dbcae75)</li> <li>remove deploy task so users dont mess it up (92fb375)</li> <li>update git push command in adding_content.md to set upstream for new branch; fixes #30 (df76944)</li> </ul>"},{"location":"contributing/changelog/#010-2024-10-25","title":"0.1.0 (2024-10-25)","text":""},{"location":"contributing/changelog/#features_7","title":"Features","text":"<ul> <li>add Conventional PR name check (#22) (beb1149)</li> <li>add documentation on Contributing (156c47e)</li> <li>add new documentation for code reviews and merging process, improving contributing guidelines for maintainers (#23) (156c47e)</li> <li>major updates (76ce766)</li> <li>Set up project structure with MkDocs, added configuration, documentation files, and specified dependencies in pixi.toml (0c17072)</li> <li>update structure (ceb66dd)</li> </ul>"},{"location":"contributing/changelog/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>add more info on merging PRs #24 (5307eda)</li> <li>format admonition (beb5d11)</li> </ul>"},{"location":"contributing/changelog/#miscellaneous-chores","title":"Miscellaneous Chores","text":"<ul> <li>release 0.1.0 (8102126)</li> </ul>"},{"location":"contributing/conventional_commits/","title":"Conventional Commits","text":"<p>We follow the Conventional Commits specification for commit messages. This helps us automate our release process and keep our commit history clean.</p> <p>This style is mandatory for merging pull requests on the handbook, but are recommended for all commits.</p> <p>The main points are summarized below, but you can read the full spec here.</p>"},{"location":"contributing/conventional_commits/#commit-message-format","title":"Commit Message Format","text":"<p>Each commit message consists of a header, a body and a footer. The header has a special format that includes a <code>&lt;type&gt;</code>, a <code>&lt;scope&gt;</code> and a <code>&lt;summary&gt;</code>:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;short summary in present tense&gt;\n\n[optional body: explains motivation for the change]\n\n[optional footer(s): note BREAKING CHANGES here, and issues to be closed]\n</code></pre> <p>The <code>&lt;scope&gt;</code> of the header is optional and provides context for where the change was made. It can be anything relevant to your package or development workflow (e.g., it could be the module or function - name affected by the change).</p> <p><code>&lt;type&gt;</code> refers to the kind of change made and is usually one of:</p> <ul> <li><code>feat</code>: A new feature.</li> <li><code>fix</code>: A bug fix.</li> <li><code>docs</code>: Documentation changes.</li> <li><code>style</code>: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc).</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature.</li> <li><code>perf</code>: A code change that improves performance.</li> <li><code>test</code>: Changes to the test framework.</li> <li><code>build</code>: Changes to the build process or tools.</li> <li><code>ci</code>: Changes to CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs).</li> <li><code>chore</code>: Other changes that don't modify src or test files.</li> </ul> <p>Other types may be defined per project, but these are the most common.</p>"},{"location":"contributing/conventional_commits/#sources","title":"Sources","text":"<ol> <li>Angular Commit Format Reference Sheet - Brian Clements</li> <li>Origin of Angular Commit Style - AngularJS Git Commit Guidelines</li> <li>Py-Pkgs tutorial - Automatic version bumping using Angular Commit Style</li> </ol>"},{"location":"contributing/merging_pr/","title":"Merging Pull Requests","text":"<p>For Maintainers</p> <p>Only a subset of members of the BHK Lab organization can merge pull requests. If you are a maintainer, you can merge a pull request by following the steps below.</p>"},{"location":"contributing/merging_pr/#validating-the-pull-request","title":"Validating the Pull Request","text":"<p>Before merging a pull request, it is important to validate the following:</p> <ol> <li>Does the pull request have a title that is descriptive and concise?</li> <li>Do all the Github Actions pass?</li> <li>Has the pull request been reviewed by at least one member of the lab?</li> <li>Are there any merge conflicts?</li> <li>Does the PR add files that should not be added or mistakes? (i.e <code>.DS_Store</code>)</li> </ol>"},{"location":"contributing/merging_pr/#merge-strategy","title":"Merge Strategy","text":"<p>Traditionally, when merging a pull request, all the commits from the pull request are added to the main branch as individual commits along with a merge commit (usually like <code>Merge pull request #123 from user/branch</code>).</p> <p>This is called a Merge Commit.</p> Merge commit example <p>We use the Squash Merge strategy for merging pull requests to the lab handbook.</p> <p>Squash merges are a way to combine multiple commits into a single commit. Instead of seeing all the author's individual commits in the main branch's commit history, you can see a single commit summarizing all the changes.</p> Squash merge example <p>More information on squash merges can be found in the GitHub Docs on Squash Merges</p> <p>This has a few benefits:</p> <ol> <li>It keeps the commit history clean and organized.</li> <li>It reduces the number of commits in the main branch, making it easier to manage.</li> <li>Avoids the <code>Merge branch 'main' into main</code> commit that is created when merging    a pull request in favor of using the <code>PR</code> title as the commit message.</li> </ol>"},{"location":"contributing/merging_pr/#merging-a-pull-request","title":"Merging a Pull Request","text":"<p>To merge a pull request, follow these steps:</p>"},{"location":"contributing/merging_pr/#click-on-the-squash-and-merge-button","title":"Click on the \"Squash and merge\" button","text":""},{"location":"contributing/merging_pr/#modify-the-commit-message-as-needed","title":"Modify the commit message as needed","text":"<p>By default, the commit message will be the title of the pull request. The body of the commit message will include all the commits from the pull request.</p> <p></p> <p>As you can see, there may be some commits that are not relevant to the pull request.</p> <p>Feel free to modify the body of the commit message to include only the relevant commits.</p> <p>Note</p> <p>Only the PR Title needs to follow the Conventional Commits specification. The commit message body can be modified to include any relevant commits.</p> <p></p>"},{"location":"contributing/merging_pr/#verify-github-pages-deployment","title":"Verify GitHub Pages Deployment","text":"<p>After merging the pull request, the changes will be deployed to GitHub Pages. Check the website at <code>https://bhklab.github.io/handbook/</code> to verify that the changes are correct.</p> <p>Note</p> <p>To view the latest merged PR changes, ensure you are on the <code>dev</code> version of the site. The version dropdown selector is located next to the site title.</p>"},{"location":"contributing/merging_pr/#delete-the-branch","title":"Delete the branch","text":"<p>Once the pull request is merged, delete the branch.</p>"},{"location":"contributing/prerequisites/","title":"Prerequisites","text":""},{"location":"contributing/prerequisites/#installing-pixi","title":"Installing Pixi","text":"<p>Pixi is a tool for managing conda environments and dependencies. To install Pixi, visit the Pixi website and follow the instructions specific to your operating system. The Pixi documentation is an extensive resource for learning how to use Pixi.</p> <p>Running the following command in your terminal should verify installation.</p> <pre><code>$pixi --version\npixi 0.34.0\n</code></pre>"},{"location":"contributing/prerequisites/#cloning-the-repository","title":"Cloning the Repository","text":"<p>To begin, clone the repository to your local machine using the following command:</p> <pre><code>$ git clone https://github.com/bhklab/handbook.git\nCloning into 'handbook'...\n....\n....\n....\n...\n$ cd handbook\n</code></pre>"},{"location":"contributing/prerequisites/#installing-dependencies","title":"Installing Dependencies","text":"<p>Once you have cloned the repository, navigate to the project directory and install the dependencies:</p> <pre><code>$ pixi install\n\u2714 The default environment has been installed.\n</code></pre> <p>This will install the dependencies specified in the <code>pixi.toml</code> file.</p> <p>To add content to the handbook, see the Adding Content section.</p>"},{"location":"contributing/release_deployment/","title":"Release and Deployment","text":""},{"location":"contributing/release_deployment/#introduction","title":"Introduction","text":"<p>The handbook website is hosted on GitHub Pages. This document provides a detailed overview of how the deployment and release processes work, ensuring a smooth and automated workflow.</p>"},{"location":"contributing/release_deployment/#github-pages","title":"GitHub Pages","text":"<p>GitHub Pages is a service that allows you to host static websites directly from your GitHub repository.</p>"},{"location":"contributing/release_deployment/#how-it-works","title":"How It Works","text":"<p>Whenever changes are pushed to the <code>main</code> branch, a GitHub Action is triggered to automatically build and deploy the website.</p> <p>You can view the build and deployment action at this link.</p> <p>The automated workflow includes the following steps:</p> <ol> <li>Check Out the Repository: The GitHub Action checks out the latest code    from the <code>main</code> branch.</li> <li>Install Dependencies: Dependencies specified in the <code>pixi.toml</code> file    are installed.</li> <li>Build the Documentation: The action builds the site using MkDocs and    the configurations defined in your project.</li> <li>Deploy to GitHub Pages: The compiled site is deployed to the <code>gh-pages</code>    branch.</li> </ol> <p>Once the <code>gh-pages</code> branch is updated, GitHub Pages will automatically publish the latest version of the website.</p>"},{"location":"contributing/release_deployment/#releases-and-versioned-documentation","title":"Releases and Versioned Documentation","text":"<p>We leverage both <code>release-please</code> and <code>mike</code> to automate the release process and manage versioned documentation, making it easier to maintain version control, changelogs, and multiple documentation versions.</p>"},{"location":"contributing/release_deployment/#how-releases-work","title":"How Releases Work","text":"<p>When a pull request is merged into the <code>main</code> branch, a GitHub Action triggers the release process.</p> <p>You can view the release automation action at this link.</p> <p>Key aspects of this combined approach include:</p> <ul> <li>Automated Release Creation with <code>release-please</code>: The tool automatically    generates a release with changelogs and updates the version number based    on the changes merged into <code>main</code>.</li> <li>Dynamic Pull Request Updates: If additional changes are pushed to the   <code>main</code> branch after a pull request is created, the release PR will update to   include those changes, ensuring that the release captures all intended   updates.</li> <li>Controlled Release Process: Maintainers can merge changes into the   release PR only when they are ready to publish a new version, giving them   full control over the timing of each release.</li> <li>Versioned Documentation with <code>mike</code>: Once a new release is prepared,   <code>mike</code> is used to manage and deploy versioned documentation. This allows us   to provide a separate set of documentation for each release, maintaining   historical versions accessible on the website.</li> </ul> <p>This automated approach ensures consistency, reduces manual effort, and allows users to access documentation relevant to any specific version of the project.</p>"},{"location":"contributing/release_deployment/#manual-release-process","title":"Manual Release Process","text":"<p>If you need to release a new version of the documentation manually, you can follow these steps:</p> <ol> <li>Pull either the <code>main</code> branch or a specific release branch (e.g., <code>v0.1.0</code>) to your local machine.</li> </ol> Releasing a <code>main</code> branch as <code>dev</code>Release a specific version branch <pre><code>$ git pull origin main\nFrom https://github.com/bhklab/handbook\n* branch            main       -&gt; FETCH_HEAD\nAlready up to date.\n</code></pre> <pre><code>$ VERSION=v0.1.0\n$ git pull origin v$VERSION\nFrom https://github.com/bhklab/handbook\n* branch            v0.1.0     -&gt; FETCH_HEAD\nAlready up to date.\n</code></pre> <ol> <li>Use the <code>mike</code> command to deploy the documentation to the <code>gh-pages</code> branch.</li> </ol> Releasing a <code>main</code> branch as <code>dev</code>Release a specific version branch <pre><code>$ pixi run mike deploy --push dev devel\n</code></pre> <pre><code>$ pixi run mike deploy --push --update-aliases $VERSION latest\n</code></pre>"},{"location":"contributing/reviews/","title":"Reviewing a Contribution","text":"<p>Note</p> <p>This document provides an overview of the review process for contributions to the BHK Lab repositories. See the Code Review section for more information on code reviews.</p> <p>For more details, see the GitHub Docs on Pull Requests with Required Reviews.</p>"},{"location":"contributing/reviews/#introduction","title":"Introduction","text":"<p>Once a contribution is submitted, a Pull Request (PR) must be created. Before merging into the main branch, the PR must undergo a review process.</p>"},{"location":"contributing/reviews/#terminology","title":"Terminology","text":"<ul> <li>Author: The individual who submitted the contribution.</li> <li>Reviewer: The individual reviewing the contribution.</li> <li>Maintainer: The person responsible for merging the contribution after review.</li> </ul>"},{"location":"contributing/reviews/#understanding-a-review","title":"Understanding a Review","text":"<p>Reviews are discussions around the changes proposed in a PR. They allow for collaborative feedback, ensuring code quality and alignment with project standards.</p> <p>Tip</p> <p>Anyone can review a PR, including those who are not maintainers! If you see a PR from another author, and have suggestions for improvement, feel free to leave a review.</p> <p>For further reading, refer to the Official GitHub Documentation on PR Reviews.</p>"},{"location":"contributing/reviews/#review-statuses","title":"Review Statuses","text":"<p>When submitting a review, you can select from three statuses:</p> <ol> <li>Comment: Provide general feedback without explicitly approving or requesting changes.</li> <li>Approve: Indicate that the changes are acceptable, and approve merging the PR.</li> <li>Request Changes: Highlight issues that need to be addressed before the PR can be merged.</li> </ol>"},{"location":"contributing/reviews/#requesting-a-review-for-a-pull-request","title":"Requesting a Review for a Pull Request","text":"<p>After creating a PR, you can request specific individuals or teams to review it.</p> <ul> <li>Only members of the BHK Lab organization can request reviews from other members.</li> </ul> <p>For more information, check the Official GitHub Documentation on Requesting a Pull Request Review.</p>"},{"location":"contributing/reviews/#adding-to-an-existing-document","title":"Adding to an Existing Document","text":"<p>If a document already exists, you can add to it. Check the bottom of the page for information on current authors.</p>"},{"location":"contributing/reviews/#conclusion","title":"Conclusion","text":"<p>Effective reviews help ensure that contributions meet project standards, improve code quality, and facilitate knowledge sharing within the team. Whether you're an author, reviewer, or maintainer, understanding the review process is essential to contributing successfully.</p>"},{"location":"contributing/submitting_issues/","title":"Submitting Issues","text":""},{"location":"contributing/submitting_issues/#introduction","title":"Introduction","text":"<p>If you encounter any issues, have suggestions, or need clarifications about the documentation, you can submit an issue directly on our GitHub repository. We have a set of pre-defined issue templates to help categorize and address your concerns efficiently.</p> <p>Link: Submit and view issues</p>"},{"location":"contributing/submitting_issues/#available-issue-templates","title":"Available Issue Templates","text":"<p>When creating a new issue, you will be presented with the following options:</p> <p></p> <ol> <li> <p>Documentation Bug Report</p> <p>Use this template to report any errors or inconsistencies in the existing documentation. This could include typos, broken links, incorrect information, or anything that does not match the expected content.</p> </li> <li> <p>Content Update Request</p> <p>Choose this template if you need to request updates to existing content.  For example, if information is outdated or requires clarification,  use this template to suggest the necessary changes.</p> </li> <li> <p>Documentation Question</p> <p>Select this option to ask questions or seek clarification about specific  parts of the documentation. This is useful if you are unsure about how  certain sections apply to your work or if you need additional details  on a topic.</p> </li> <li> <p>Documentation Enhancement Request</p> <p>This template is for suggesting new content or improvements to existing  sections. If you think the documentation can be expanded or restructured  to better serve the users, please use this option.</p> </li> </ol>"},{"location":"contributing/submitting_issues/#how-to-submit-an-issue","title":"How to Submit an Issue","text":"<ol> <li>Go to the GitHub repository and navigate to the \"Issues\" tab.</li> <li>Click on the \"New Issue\" button.</li> <li>Select the appropriate issue template from the list.</li> <li>Fill out the template, providing as much detail as possible to help us    understand and address your issue.</li> <li>Submit the issue.</li> </ol> <p>For any issues that do not fit the existing templates, you can also select Open a blank issue to describe your concern freely.</p>"},{"location":"contributing/submitting_issues/#tips-for-submitting-effective-issues","title":"Tips for Submitting Effective Issues","text":"<ul> <li>Be Clear and Concise: Provide a detailed description but be direct.   Clear and specific information helps us address your issue faster.</li> <li>Include Links and Screenshots: If the issue relates to a specific   section, page, or example, include links and screenshots to provide context.</li> <li>Suggest Solutions: If you have an idea on how to fix or improve the   issue, let us know! Your suggestions can help expedite the process.</li> </ul>"},{"location":"disciplines/","title":"Disciplines","text":""},{"location":"disciplines/#data-science","title":"Data Science","text":"<p>Data Science focuses on extracting insights and knowledge from data using statistical, computational, and machine learning methods. This discipline enables informed decision-making in various fields, including research and industry.</p>"},{"location":"disciplines/#machine-learning","title":"Machine Learning","text":"<p>Machine Learning is a subfield of artificial intelligence to learn and improve from data without explicit programming. It is widely used in predictive modeling, pattern recognition, and automating complex processes.</p>"},{"location":"disciplines/#bioinformatics","title":"Bioinformatics","text":"<p>Bioinformatics is the application of computational methods to analyze, and interpret biological data. It plays a crucial role in genomics, proteomics, and other fields where large datasets are prevalent.</p>"},{"location":"disciplines/#pharmacogenomics","title":"Pharmacogenomics","text":"<p>Pharmacogenomics is the study of how genetic variations to tailor medications to individual genetic profiles. This approach enhances treatment efficacy and minimizes adverse effects.</p>"},{"location":"disciplines/#imaging","title":"Imaging","text":"<p>Imaging is the science of capturing and representing visual information for medical and scientific purposes. This discipline supports diagnostics, research, and data visualization through advanced imaging technologies.</p>"},{"location":"disciplines/Bioinformatics/","title":"What is Bioinformatics?","text":"<p>Bioinformatics, as related to genetics and genomics, is a scientific subdiscipline that involves using computer technology to collect, store, analyze and disseminate biological data and information, such as DNA and amino acid sequences or annotations about those sequences. Scientists and clinicians use databases that organize and index such biological information to increase our understanding of health and disease and, in certain cases, as part of medical care. (Source)</p>"},{"location":"disciplines/Bioinformatics/#introductory-resources","title":"Introductory Resources","text":"<ul> <li> <p>StatQuest with Josh Starmer -YouTube Channel </p> </li> <li> <p>Rafael Irizzary Lectures - Professor of Applied Statistics at Harvard - YouTube Channel</p> </li> <li> <p>Modern Statistics for Modern Biology - digital textbook by Susan Holmes and Wolfgang Huber</p> </li> </ul>"},{"location":"disciplines/Bioinformatics/Data_Types/rnaseq/","title":"RNA-seq","text":""},{"location":"disciplines/Bioinformatics/Data_Types/rnaseq/#what-is-rna-sequencing-rna-seq","title":"What is RNA Sequencing (RNA-seq)?","text":"<p>RNA sequencing (RNA-seq) is a high-throughput sequencing technology that allows scientists to map and quantify the transcriptome - essentially studying RNA molecules within cells. The transcriptome, or the RNA space, is comprised of messenger RNA (mRNA) and other non-coding RNAs (ncRNAs).</p> <p>Central Dogma of Molecular Biology: DNA \u2192 RNA \u2192 Protein </p> <p>While all RNAs are transcribed from RNA, only the mRNAs are further translated into protein, hence they are oftend referred to as the \"messengers\" that carries instructions from DNA to create proteins, or the functional units within the cells. ncRNAs have several roles relating to the regulation of gene expression and other cellular activities. By sequencing RNA, we get a snapshot of the transcriptome, including active genes, allowing us to understand how cells function and respond to different conditions.</p>"},{"location":"disciplines/Bioinformatics/Data_Types/rnaseq/#purpose-of-rna-seq-in-bioinformaticstranscriptomics","title":"Purpose of RNA-seq in Bioinformatics/Transcriptomics","text":"<p>Bioinformatics is the use of computational approaches to study biology, and transcriptomics focuses on studying RNA. Some purposes of RNA-seq in bioinformatic analysis include:</p> <p>1. Gene expression and activity</p> <p>Identifies the genes actively transcribed into mRNA in a given condition or cell type, giving an indication of which genes are \"switched on\". By quantifying the abundance of mRNA molecules, we can measure the activity levels of specific genes. RNA-Seq also allowed for examining post-transcriptional modifications such as alternative splicing, where RNA segments are rearranged or removed, influencing the diversity of proteins produced.</p> <p>2. ncRNA expression and activity</p> <p>Beyond mRNAs, several ncRNAs are of interest for cancer research. RNA-Seq also profiles the expression levels of these transcripts such that similar analyses can be performed as with mRNA.</p> <p>3. Differential RNA Expression</p> <p>Differential expression analysis can identify differences in RNA expression between different conditions (e.g. healthy vs diseased states, before and after treatment, etc). </p> <p>4. Biomarker Analysis</p> <p>RNA transcript expression can be associted with the response to different drugs, hence are often used as input features to identify biomarkers for cancer treatment.</p> <p>5. Subtype Identification</p> <p>Patterns of RNA transcript expression can be used to identify distinct clusters or subgroups within a given population. This approach is often used to identify cancer subtypes.</p> <p>6. Functional Enrichment</p> <p>Using databases such as Gene Ontology (GO), KEGG, Reactome, mSigDB, etc, sets of RNA transcripts (e.g. from differential expression analysis) can be linked to downstream effects on biological processes, signaling pathways, and overallc cellular function. </p>"},{"location":"disciplines/Bioinformatics/Data_Types/rnaseq/#how-is-rna-seq-data-obtained","title":"How is RNA-seq Data Obtained?","text":"<p>Here\u2019s a simplified process:</p> <ol> <li>RNA extraction: Scientists first isolate and collect RNA from cells or tissue samples.</li> <li>Synthesize cDNA: Due to the fragile nature of RNA, these transcripts are converted into complementary DNA (cDNA), which is more stable.</li> <li>Fragmentation: The cDNA is broken into smaller fragments that enable them to be sequenced.</li> <li>Sequencing of Fragments: Machines called sequencers read the fragments and identify the order of RNA nucleotides (A, U, G, C).</li> <li>Alignment: Sequenced fragments are aligned to a reference genome or transcriptome to get position-based mapping information, such as mapping mRNA transcripts to their respective genes.</li> <li>Quantification: With aligned fragments, transcript expression can be quantified.</li> </ol>"},{"location":"disciplines/Bioinformatics/Data_Types/rnaseq/#how-is-rna-seq-different-from-dna-sequencing","title":"How is RNA-seq Different from DNA Sequencing?","text":"Feature DNA Sequencing RNA Sequencing What is studied? DNA, the cell\u2019s blueprint. RNA, transcribed from DNA. Purpose Understand the structure of genes, mutations, and inheritance. Study gene and transcript activity Stable or Changing? DNA is mostly stable and the same in all cells. RNA levels vary depending on the cell\u2019s activity. Building Blocks A, T, G, C (Adenine, Thymine, Guanine, Cytosine). A, U, G, C (Uracil replaces Thymine in RNA). Output The sequence of an organism's entire genome (WGS) or specific parts (WES). A snapshot of all active RNA molecules."},{"location":"disciplines/Bioinformatics/Data_Types/rnaseq/#summary","title":"Summary","text":"<p>RNA-seq fits into the Central Dogma by focusing on RNA, the \"middle step\" between DNA and proteins. It provides a dynamic snapshot of gene and transcript activity, offering crucial insights into how cells work, respond to the environment, and contribute to diseases. By analyzing RNA, scientists can better understand the processes that sustain life and develop treatments for various conditions.</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/","title":"STAR Alignment Guide","text":""},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#introduction","title":"Introduction","text":"<p>STAR (Spliced Transcripts Alignment to a Reference) is a high-performance RNA-seq read aligner, widely used for mapping RNA sequencing data to reference genomes. It is optimized for speed and accuracy, especially for aligning spliced reads across exon-exon junctions.</p> <p>This guide explains how to set up and use STAR for RNA-seq read alignment.</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#loading-star-on-the-h4h-cluster","title":"Loading STAR on the H4H Cluster","text":"<p>To use STAR on the H4H cluster, load the module:</p> <pre><code>module load STAR\n</code></pre>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#verifying-installation","title":"Verifying Installation","text":"<p>After loading STAR, check the version to ensure compatibility with your pipeline:</p> <p><pre><code>STAR --version\n</code></pre> The output should display the version number, for example: <pre><code>STAR\\2.7.9a\n</code></pre></p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#preparing-for-star-alignment","title":"Preparing for STAR Alignment","text":"<p>Before using STAR, ensure you have the following files:</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#reference-genome-fasta-file","title":"Reference Genome FASTA File","text":"<ul> <li>Contains the DNA sequences of your reference genome.</li> <li>Example for GRCh38:</li> <li>Here: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/   <code>GCF_000001405.40_GRCh38.p14_genomic.fna.gz</code></li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#gene-annotation-file-gtf","title":"Gene Annotation File (GTF)","text":"<ul> <li>Includes gene structure (exons, introns, splice junctions).</li> <li>Example for GRCh38:</li> <li>Here: https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/   <code>GCF_000001405.40_GRCh38.p14_genomic.gtf.gz</code></li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#rna-seq-fastq-files","title":"RNA-seq FASTQ Files","text":"<ul> <li>Paired-end or single-end sequencing data.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#index","title":"Index","text":""},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#2-generating-a-genome-index","title":"2. Generating a Genome Index","text":"<p>STAR requires a genome index to efficiently align reads. Submit the following command in your SLURM script to generate it:</p> <pre><code>module load STAR/&lt;version_number&gt;\n# Replace &lt;version_number&gt; with the desired version, e.g., module load STAR/2.7.9a\n\nSTAR --runThreadN 8 \\\n     --runMode genomeGenerate \\\n     --genomeDir /path/to/genomeDir \\\n     --genomeFastaFiles genome.fa \\\n     --sjdbGTFfile annotations.gtf \\\n     --sjdbOverhang 100\n</code></pre>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#explanation-of-parameters","title":"Explanation of Parameters","text":"<ul> <li><code>--runThreadN</code>: Number of threads (adjust to available cores for faster processing).</li> <li><code>--genomeDir</code>: Directory where the genome indices will be stored.</li> <li><code>--genomeFastaFiles</code>: Path to the reference genome FASTA file(s). For multiple FASTA files, list them separated by spaces.</li> <li><code>--sjdbGTFfile</code>: Path to the GTF annotation file for splice junction information.</li> <li><code>--sjdbOverhang</code>: Read length minus 1 (e.g., for 101 bp reads, use 100).</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#3-aligning-rna-seq-reads","title":"3. Aligning RNA-seq Reads","text":"<p>After generating the genome index, you can align RNA-seq reads using STAR.</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#single-end-reads-example","title":"Single-End Reads Example:","text":"<pre><code>module load STAR/&lt;version_number&gt;\n# Replace &lt;version_number&gt; with the desired version, e.g., module load STAR/2.7.9a\n\nSTAR --runThreadN 8 \\\n     --genomeDir /path/to/genomeDir \\\n     --readFilesCommand gunzip \\\n     --readFilesIn sample_R1.fastq \\\n     --outFileNamePrefix sample \\\n     --quantMode GeneCounts \\\n     --outReadsUnmapped Fastx \\\n     --chimSegmentMin 10 \\\n     --outSAMtype BAM SortedByCoordinate\n</code></pre>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#paired-end-reads-example","title":"Paired-End Reads Example:","text":"<pre><code># Replace &lt;version_number&gt; with the desired version, e.g., module load STAR/2.7.9a\n\nSTAR --runThreadN 8 \\\n     --genomeDir /path/to/genomeDir \\\n     --readFilesCommand gunzip \\\n     --readFilesIn sample_R1.fastq sample_R2.fastq \\\n     --outFileNamePrefix sample \\\n     --quantMode GeneCounts \\\n     --outReadsUnmapped Fastx \\\n     --chimSegmentMin 10 \\\n     --outSAMtype BAM SortedByCoordinate\n</code></pre>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#explanation-of-parameters_1","title":"Explanation of Parameters","text":"<ul> <li> <p>----readFilesIn:  Input FASTQ files (single or paired-end).</p> </li> <li> <p>--outFileNamePrefix:  Prefix for output files.</p> </li> <li> <p>--outSAMtype:  Specifies output format (e.g., sorted BAM file).</p> </li> <li> <p>--runThreadN: Number of CPU threads to use. Adjust this based on the computational resources available on your cluster. Using more threads speeds up the process but requires more cores.</p> </li> <li> <p>--genomeDir: Path to the directory containing the pre-generated STAR genome index. This directory is created using the --runMode genomeGenerate command (see the section on generating the genome index).</p> </li> <li> <p>--readFilesCommand: Command used to preprocess the input FASTQ files. In this example, gunzip is specified, indicating that the input FASTQ files are compressed (.gz). For uncompressed files, this parameter is not needed.</p> </li> <li> <p>--readFilesIn: Specifies the input FASTQ file(s) for alignment. For single-end reads, provide one file; for paired-end reads, provide both files separated by a space (e.g., sample_R1.fastq sample_R2.fastq).</p> </li> <li> <p>--outFileNamePrefix: Specifies the prefix for all output files. STAR will append specific suffixes to this prefix to generate different output files (e.g., BAM files, logs).</p> </li> <li> <p>--quantMode: Enables quantification of reads at the gene level. Using GeneCounts generates a file (ReadsPerGene.out.tab) that provides read counts for each gene, useful for downstream expression analysis.</p> </li> <li>Output includes three columns for each gene:</li> <li>Uniquely mapped reads.</li> <li>Reads mapped to both strands.</li> <li> <p>Reads mapped to the opposite strand.</p> </li> <li> <p>--outSAMtype: Specifies the format of the output alignment file: </p> </li> <li>BAM Unsorted: Produces an unsorted BAM file.</li> <li> <p>BAM SortedByCoordinate: </p> <ul> <li>Produces a sorted BAM file based on genomic coordinates, which is typically required for downstream tools like featureCounts or visualization in genome browsers.</li> </ul> </li> <li> <p>--outReadsUnmapped: Specify whether to output unmapped reads. Options include Fastx to write unmapped reads in FASTQ format, which can be useful for troubleshooting or further analysis.</p> </li> <li> <p>chimSegmentMin: Minimum length of chimeric alignments (e.g., for detecting fusion transcripts). Defaults to 0, but setting a value like 10 can help identify chimeric reads.</p> </li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#star-output","title":"STAR Output","text":"<p>STAR generates several output files. Key files include:</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#log-files","title":"Log Files","text":"<ul> <li><code>Log.out</code>: General log with alignment summary.</li> <li><code>Log.final.out</code>: Detailed alignment statistics.</li> <li><code>Log.progress.out</code>: Progress of the alignment process, including percentage completion.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#alignment-files","title":"Alignment Files","text":"<ul> <li><code>Aligned.sortedByCoord.out.bam</code>: BAM file containing reads aligned to the reference genome, sorted by genomic coordinates.</li> <li><code>Aligned.out.bam</code>: BAM file containing reads aligned to the reference genome (unsorted).</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#gene-counts","title":"Gene Counts","text":"<ul> <li><code>ReadsPerGene.out.tab</code>: File containing gene-level read counts, with three columns for each gene:</li> <li>Uniquely mapped reads.</li> <li>Reads mapped to both strands.</li> <li>Reads mapped to the opposite strand.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#unmapped-reads","title":"Unmapped Reads","text":"<ul> <li><code>Unmapped.out.mate1</code>: FASTQ file containing unmapped reads from the first mate (if specified).</li> <li><code>Unmapped.out.mate2</code>: FASTQ file containing unmapped reads from the second mate (if specified).</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#splice-junctions","title":"Splice Junctions","text":"<ul> <li><code>SJ.out.tab</code>: File listing detected splice junctions, including information about their genomic coordinates and supporting read counts.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#chimeric-reads","title":"Chimeric Reads","text":"<ul> <li><code>Chimeric.out.sam</code>: SAM file containing chimeric (fusion) alignments, useful for identifying fusion transcripts.</li> </ul> <p>Several of these parameters are optional, for more details see: - https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf </p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#running-star-alignments-in-parallel-using-sbatch-array","title":"Running STAR Alignments in Parallel Using <code>sbatch --array</code>","text":"<p>When processing multiple RNA-seq samples, you can use SLURM's <code>--array</code> option to run jobs in parallel. This approach is efficient for handling batch alignment tasks.</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#1-preparing-input-files","title":"1. Preparing Input Files","text":"<p>Create a text file listing all your sample FASTQ file pairs, with one sample per line. For paired-end data, include both files separated by a space, like this:</p> <p><code>samples.txt</code>: <pre><code>sample1_R1.fastq.gz sample1_R2.fastq.gz sample2_R1.fastq.gz sample2_R2.fastq.gz sample3_R1.fastq.gz sample3_R2.fastq.gz\n</code></pre></p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#2-writing-the-sbatch-script","title":"2. Writing the <code>sbatch</code> Script","text":"<p>Here's an example <code>sbatch</code> script to run STAR for each sample in the list using an array job:</p> <p><code>run_star.sh</code>: <pre><code>#!/bin/bash\n#SBATCH --job-name=STAR_array\n#SBATCH --array=0-2                 # Set the range of job indices (adjust based on the number of samples)\n#SBATCH --ntasks=1                  # Number of tasks per job\n#SBATCH --cpus-per-task=8           # Number of CPUs per task\n#SBATCH --mem=61440M                # Memory allocation per job\n#SBATCH --p himem                   # Partition\n#SBATCH -t 07:00:00                 # Max runtime \n#SBATCH --output=logs/star_%A_%a.log # Log file for each task\n\n# Load STAR module\nmodule load STAR\n\n# Read the samples file to get the corresponding FASTQ files for this array task\nSAMPLES_FILE=\"samples.txt\"\nLINE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" $SAMPLES_FILE)\nFASTQ1=$(echo $LINE | cut -d ' ' -f 1)\nFASTQ2=$(echo $LINE | cut -d ' ' -f 2)\n\n# Specify the genome directory and output directory\nGENOME_DIR=\"/path/to/genomeDir\"\nOUTPUT_DIR=\"/path/to/output\"\n\n# Run STAR\nSTAR --runThreadN 8 \\\n     --genomeDir $GENOME_DIR \\\n     --readFilesCommand gunzip \\\n     --readFilesIn $FASTQ1 $FASTQ2 \\\n     --outFileNamePrefix $OUTPUT_DIR/sample_${SLURM_ARRAY_TASK_ID}_ \\\n     --outSAMtype BAM SortedByCoordinate\n</code></pre></p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#2-submitting-the-job","title":"2. Submitting the Job","text":"<p>Submit the job array with the following command:</p> <pre><code>sbatch run_star.sh\n</code></pre>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#explanation-of-parameters_2","title":"Explanation of Parameters","text":"<ul> <li><code>--array=0-2</code>: Specifies the range of indices for the job array. Adjust based on the number of lines in <code>samples.txt</code>. For example, if you have 10 samples, use <code>--array=0-9</code>.</li> <li><code>SLURM_ARRAY_TASK_ID</code>: A unique identifier for each array task, corresponding to the line in <code>samples.txt</code>.</li> <li><code>sed -n \"${SLURM_ARRAY_TASK_ID}p\"</code>: Extracts the line from <code>samples.txt</code> corresponding to the current array task.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/STAR/#4-logs-and-output","title":"4. Logs and Output","text":"<ul> <li>Logs: Logs for each task will be saved in the <code>logs/</code> directory with filenames like <code>star_JOBID_TASKID.log</code>.</li> <li>Output: Aligned BAM files and other outputs will be saved in the specified <code>OUTPUT_DIR</code>.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/deseq/","title":"DESeq2","text":"<p>DESeq2 is a popular R package used for analyzing RNA count data - transcriptomics. It is widely used for differentially expressed analysis (DE) between different conditions (e.g. WT vs. mutant). The package also integrates many powerful data processing and analysis tools.</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/deseq/#features","title":"Features:","text":"<p>Normalization: DESeq2 normalizes the count data to account for differences in sequencing depth and RNA composition. Statistical Modeling: It uses a negative binomial distribution to model the count data, which is appropriate for overdispersed count data. Differential Expression Analysis: DESeq2 provides statistical tests to identify genes that are differentially expressed between conditions. Visualization: The package includes functions for visualizing results, such as MA plots and heatmaps.  </p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/deseq/#example","title":"Example","text":"<p>One may follow the workflow template from below (data preprocessing and further analysis are needed, and they vary between different analyses): <pre><code># Create DESeqDataSet object\ndds &lt;- DESeqDataSetFromMatrix(countData = countData, ## your raw count\n                              colData = colData, ## your column metadata (i.e. sample/cell data)\n                              design = ~ condition) ## specifies the experimental design (e.g. conditions, treatments, etc.)\n\n# Run the DESeq2 pipeline\ndds &lt;- DESeq(dds)\nDEresults = results(dds)\nDEresults &lt;- DEresults[order(DEresults$padj),]\n\n# Extract results\nres &lt;- results(dds)\n\n## MA plot to check how well normalization works\nplotMA(dds)\n</code></pre> One can perform QC through PCA plot: <pre><code>rld &lt;- rlog(dds)\nDESeq2::plotPCA(rld, ntop = 500, intgroup = 'group') +\n  ylim(-50, 50) + theme_bw()\n</code></pre></p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/deseq/#visually-de-result-with-volcano-plot","title":"Visually DE result with volcano plot","text":"<pre><code>dds &lt;- DESeq(dds)\nDEresults = results(dds)\nlibrary(EnhancedVolcano)\n# DEseq object is S4 object - we need to convert it to a data frame (S3)\nDEresults &lt;- as.data.frame(DEresults)\n\nEnhancedVolcano(DEresults,\n                lab = row.names(DEresults),\n                x = 'log2FoldChange',\n                y = 'padj',\n                pCutoff = 5e-2,\n                FCcutoff = 1,\n                labSize = 2.5,\n                legendLabels=c('Not sig.',expression(paste('Log'[2],'FC')),'padj', expression(paste('padj &amp; Log'[2],'FC'))),\n                ylab = \"padj\")\n</code></pre>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/kallisto/","title":"Kallisto","text":"<p>Kallisto is a lightweight, ultra-fast RNA-seq quantification tool. It is mainly used for estimating transcript abundances directly from RNA-seq reads without performing full read alignment. A detailed tutorial is available here.</p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/kallisto/#key-features","title":"Key features","text":"<ul> <li>Alignment-free: Kallisto does not align reads base-by-base to the genome. Instead, it uses a technique called pseudoalignment to quickly determine which transcript reads are compatible with.</li> <li>Extremely fast: It is significantly faster than traditional aligners like STAR and HISAT2 because it skips full alignment.</li> <li>Quantification: Directly outputs transcript abundance estimates such as TPM (Transcripts Per Million).</li> <li>Low memory usage: Efficient enough to run on laptops or small servers.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/kallisto/#download","title":"Download","text":"<p>Kallisto is already installed on H4H.  If you would like to install it on your local computer, please follow these instructions. </p>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/kallisto/#workflow","title":"Workflow:","text":"<ul> <li>Step 1: Build a transcriptome index (from a reference transcriptome FASTA file) <pre><code>kallisto index -i transcriptome.idx transcripts.fa\n</code></pre></li> <li>Step 2: Quantify RNA-seq reads (Pseudoalignment and abundance estimation) <pre><code>kallisto quant -i transcriptome.idx -o output_dir -b 100 reads_1.fastq reads_2.fastq\n</code></pre> The option <code>-b 100</code> specifies 100 bootstrap samples for estimating quantification uncertainty.</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/kallisto/#outputs","title":"Outputs","text":"<ul> <li><code>abundance.tsv</code> \u2014 Main quantification table (TPM, estimated counts)</li> <li><code>abundance.h5</code> - Binary HDF5 file containing the same information as <code>abundance.tsv</code>, plus bootstrap results if bootstrapping was performed.</li> <li><code>run_info.json</code> \u2014 Summary of the Kallisto run (e.g., number of processed reads, parameters used).</li> </ul>"},{"location":"disciplines/Bioinformatics/Tools/RNAseq_Pipelines/kallisto/#usage","title":"Usage","text":"<ul> <li>A Snakemake Kallisto pipeline is available at H4H: <code>/cluster/projects/bhklab/pipelines/kallisto_snakemake_pipeline</code></li> <li>Details about pipeline setup and usage are available in the pipeline's <code>README</code> file. </li> </ul>"},{"location":"disciplines/Data_Science/","title":"Introduction","text":"<p>TODO:: Add a short description here</p>"},{"location":"disciplines/Data_Science/Data_Curation/","title":"Data Curation","text":""},{"location":"disciplines/Data_Science/Data_Curation/#overview","title":"Overview","text":"<p>Data curation is the process of preparing data for analysis. It involves identifying, cleaning, and transforming data to ensure its quality and usability. Data curation is an essential step in the data analysis process, as it helps to ensure that the data is accurate, complete, and relevant for the analysis.</p> <p>DataRaven has established standard operating procedures (SOPs) for different data types.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/","title":"Immuno-Oncology Clinical Trial Curation","text":"<p>Welcome to the Immuno-Oncology Curation Guide!</p> <p>Whether you're new to the lab or need a quick refresher, this guide walks you through the process of curating Immuno-Oncology (IO) clinical trial datasets into structured, analysis-ready R objects. The goal is to standardize raw and processed data into clean MultiAssayExperiment (MAE) objects for use in downstream analysis and collaborative research.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#what-is-io-curation","title":"What Is IO Curation?","text":"<p>IO curation is the process of transforming clinical trial data into reusable, analysis-ready formats compatible with R-based workflows.</p> <p>Each curated dataset includes two key components:</p> <ol> <li>Clinical metadata: Patient/sample-level information such as treatment, response, survival, demographics.  </li> <li>Molecular profiles: Expression data (RNA-seq or microarray), and when available SNV and CNA data. These are formatted as SummarizedExperiment (SE) or RangedSummarizedExperiment (RangedSE)  objects, depending on the assay type.</li> <li>Annotation data: Row-level annotations (e.g., Ensembl ID, gene name) are stored within each assay.</li> </ol> <p>We curate all data into the MultiAssayExperiment (MAE) format. All publicly available curated datasets are located on ORCESTRA. We recommend downloading one to explore the clinical metadata and molecular assay structure.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#step-by-step-workflow","title":"Step-by-Step Workflow","text":"<p>An example of a clinical data processing pipeline is available ICB_Van_Allen Snakemake.</p> <p>The standard curation process includes:</p> <ol> <li>Access and download source data (raw or processed)  </li> <li>Process or import molecular data (e.g., RNA-seq, SNV, CNA), ensuring standardized formats and identifiers  </li> <li>Process and clean clinical metadata, harmonizing variable names, response labels, and survival fields  </li> <li>Add standardized annotations (e.g., drug names, gene identifiers, tissue types)  </li> <li>Create <code>SE</code> or <code>RangedSE</code> objects, depending on assay type  </li> <li>Assemble the final <code>MAE</code> object, integrating all data components  </li> <li>Review a reference IO dataset, curated example on ORCESTRA.</li> </ol>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#1-download-source-data","title":"1. Download Source Data","text":"<p>Begin by reviewing the original publication to confirm study design, molecular assays, and whether the data is public or private.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#dataset-categories","title":"Dataset Categories","text":"<ul> <li>Private datasets: Stored internally (e.g., Box, institutional drives). May include PHI and require ethics approval.</li> <li>Public datasets: Available via GEO, dbGaP, Zenodo, and EGA.</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#11-molecular-data","title":"1.1 Molecular Data","text":"<p>If raw RNA (FASTQ files) are not available, look for processed files by modality:</p> <ul> <li>RNA-seq: TPM or count matrices (CSV, TSV, Excel)  </li> <li>RNA-seq: Isoform-level expression (optional but recommended)  </li> <li>Microarray: Normalized expression matrices (e.g., quantile normalized)</li> <li>DNA (SNV): VCF, MAF, or binary gene-level mutation calls  </li> <li>DNA (CNA): Gene-by-sample matrices or segment files</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#12-expression-and-mutation-data","title":"1.2 Expression and Mutation Data","text":"<p>RNA sequencing (RNA-seq) quantifies gene expression by aligning RNA reads to a reference genome.</p> <p>There are two options depending on data availability:</p> <ul> <li> <p>If only processed RNA-seq data is available:   Use the provided gene-level TPM or count matrices (CSV, TSV, or Excel format). Include isoform (transcript-level) data when available.</p> </li> <li> <p>If RNA-seq FASTQ files are available:   Use the kallisto Snakemake pipeline available on HPC4Health (H4H).   FASTQ files are typically stored at: <code>/cluster/projects/bhklab/rawdata/EGA/</code>   The pipeline is located in <code>pipelines/kallisto_snakemake_pipeline/</code>, with setup instructions in <code>README.md</code>. Expression values can be extracted using this script.</p> </li> </ul> <p>For microarray data, follow the same structure using quantile-normalized expression matrices.   For SNV data, use either pre-processed mutation calls, or extract SNVs directly from FASTQ files using appropriate variant-calling pipelines (e.g., WES and RNA-seq reference).</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#13-clinical-metadata","title":"1.3 Clinical Metadata","text":"<p>Clinical metadata should be collected as CSV or Excel files and should include:</p> <ul> <li>Patient/sample identifiers</li> <li>Treatment and response information</li> <li>OS/PFS time and event censoring (highly preferred)</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#2-process-molecular-data","title":"2. Process Molecular Data","text":"<p>You will need TPM values for downstream analysis, whether derived from raw FASTQ files or already processed expression data. The final output should be log-transformed TPM.</p> <ul> <li>If you have TPM, use:</li> </ul> <pre><code>log2(TPM + 0.001)\n</code></pre> <ul> <li>If you have raw counts, convert to TPM using:</li> </ul> <pre><code>GetTPM &lt;- function(counts, gene_length) {\n  x &lt;- counts / gene_length\n  return(t(t(x) * 1e6 / colSums(x)))\n}\n</code></pre> <p>Other data types: * SNV data: Binary gene \u00d7 sample matrix preferred * CNA data: Gene-level amplifications, deletions, or summary scores * Ensure row and column names are clean, and sample IDs are consistent across all data types * See helpful utility functions in the <code>ICB_Common/code</code> repository</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#3-process-clinical-data","title":"3. Process Clinical Data","text":"<p>Format clinical metadata as:</p> <ul> <li>Rows: patient/sample IDs  </li> <li>Columns: clinical attributes</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#31-mandatory-columns","title":"3.1 Mandatory Columns","text":"Column name Description Patientid This column contains unique patient identifiers treatmentid This column contains the treatment regimen of each patient. Individual drug names are separated by \":\" and standardized based on the lab's nomenclature. For example, the drug combo \"FAC\" is represented as \"5-fluorouracil:Doxorubicin:Cyclophosphamide\" response This column contains the response status of the patients to the given treatment - Responders (R) and Non-responders (NR) tissueid Cancer type standardized based on the lab's nomenclature from Oncotree. Example:  \u201cBreast\u201d survival_time_pfs/survival_time_os The time starting from taking the treatment to the occurrence of the event of interest. The event name like \"pfs\", \"os\" must be appended to survival_time to differentiate the survival measure. Example for data in this column: \u201c2.6\u201d survival_unit The unit in which the survival time is measured. If the event is measured in other units such as \u201cday\u201d, or \u201cyear\u201d, it must be converted to \"month\" for consistency event_occurred_pfs/event_occurred_os Binary measurement showing whether the event of interest occurred (1) or not (0).  The event name like \"pfs\", \"os\" must be appended to event_occurred to differentiate the survival measure <p>Note</p> <p>Common columns must be the first set of columns appearing in the metadata, followed by any additional columns. You may add other metadata columns available in the source data, but the standardized columns above should be present first.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#32-additional-columns","title":"3.2 Additional Columns","text":"<p>The table below shows the other common columns across the 19 ICB datasets curated</p> Column name Description type age Age source AMP Sum of total AMP/coverage; calculated from CNA values in-lab curation cancer_type Type of cancer tissue source CIN Calculated from CNA values in-lab curation CNA_tot Sum of total CNA/coverage; calculated from CNA values in-lab curation DEL Sum of total DEL/coverage; calculated from CNA values in-lab curation dna DNA sequencing type. eg: whole exome sequencing source dna_info Method for normalizing DNA sequencing data in-lab curation histo Histological info such as subtype source indel_nsTMB_perMb - in-lab curation indel_nsTMB_raw - in-lab curation indel_TMB_perMb - in-lab curation indel_TMB_raw - in-lab curation nsTMB_perMb - in-lab curation nsTMB_raw - in-lab curation recist Annotated using RECIST. The most commonly used responses are CR, PR, SD, PD. source response.other.info Same data as Responders (R) and Non-responders (NR) source rna Type of rna processed data. eg: TPM source rna_info Method for normalizing RNA sequencing data in-lab curation sex Sex of the patient - Male or Female source stage Cancer stage source survival_type PFS or OS or both (denoted by '/'). If both, added by in-lab curation in-lab curation TMB_perMb TMB per megabase (Mb) calculated where: TMB = mutns/target; mutns = number of non-synonymous mutations; and target = target size of the sequencing. See Supplementary Table S2 of PMID: 36055464 in-lab curation TMB_raw Tumor Mutation Burden raw values in-lab curation treatment Drug target or drug name source"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#4-add-annotations","title":"4. Add Annotations","text":"<p>Lab standardized annotation data are stored in BHKLab-Pachyderm's </p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#41-gene-annotations","title":"4.1 Gene Annotations","text":"<p>Check the gene annotation version used in the original dataset (typically stated in the reference paper or supplement).  </p> <p>Then download the matching file from the BHKLab Annotations repository. Using <code>Gencode.v19.annotation.RData</code> and <code>Gencode.v40.annotation.RData</code> files are preferred: </p> <ul> <li>Gencode v19 </li> <li>Gencode v40 </li> </ul> <p>Each <code>.RData</code> file includes<code>features_gene</code>, <code>features_transcript</code>, and <code>tx2gene</code>.  </p> <p>Note</p> <p>The goal is to retain as many genes as possible and match the original reference. Using a mismatched annotation version can lead to a loss of gene entries\u2014this is not preferred.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#42-drug-annotations","title":"4.2 Drug Annotations","text":"<p>Standardize treatment names using BHKLab\u2019s drug annotation files, using drugs_with_ids.csv.</p> <p>If the treatment is not listed there, search external databases such as PubChem to verify the correct drug name.</p> <p>Note</p> <p>For the <code>treatment</code> column, immunotherapy regimens are currently grouped into the following categories:</p> <ul> <li>PD-1/PD-L1: Immune checkpoint inhibitors targeting PD-1 or PD-L1</li> <li>CTLA4: Checkpoint inhibitors targeting CTLA-4</li> <li>IO+combo: Combination immunotherapy</li> <li>IO+chemo: Immunotherapy plus chemotherapy</li> <li>IO+targeted: Immunotherapy plus targeted therapy</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#43-tissue-annotations","title":"4.3 Tissue Annotations","text":"<p>Use OncoTree to map cancer types. If unmatched, perform manual review and map to standardized tissue categories.</p>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#5-create-se-or-rangedse","title":"5. Create SE or RangedSE","text":"<p>Use:</p> <ul> <li><code>SummarizedExperiment</code>: for expression or mutation matrices (TPM, SNV binary calls)</li> <li><code>RangedSummarizedExperiment</code>: for genomic ranges (e.g., VCFs with genomic coordinates)</li> </ul> <p>Each object should include:</p> <ul> <li><code>assay</code>: main data matrix (features \u00d7 samples)  </li> <li><code>rowData</code>: feature metadata (e.g., gene symbol, Ensembl ID)  </li> <li><code>colData</code>: sample-level metadata (clinical)</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#6-build-mae","title":"6. Build MAE","text":"<p>Integrate multiple assay types and clinical data into a single MAE object.</p> <p>Required Components:</p> <ul> <li><code>experiments()</code>: a list of <code>SE</code>/<code>RangedSE</code> objects (e.g., <code>expr</code>, <code>snv</code>, <code>cna</code>)  </li> <li><code>colData()</code>: the clinical metadata  </li> <li><code>sampleMap()</code>: map linking sample IDs to patients across assays</li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/IO_Clinical_Trial_Curation/#7-io-example-dataset","title":"7. IO Example Dataset","text":"<p>View the dataset online at ICB_Van_Allen \u2014 available on Orcestra.</p> <p>The following tabs are included:</p> <ul> <li> <p>Dataset Tab: Contains Gencode v19 annotations and related publication references. </p> </li> <li> <p>Pipeline Tab:  </p> <ul> <li>Commit: Key scripts are available available in this GitHub commit. Below is the structure of the folder:</li> </ul> <pre><code>\ud83d\udcc1  ICB_Van_Allen/\n\u251c\u2500\u2500 \ud83d\udcc4 Snakefile                       # Snakemake workflow combining all scripts\n\u2514\u2500\u2500 \ud83d\udcc1 scripts\n    \u251c\u2500\u2500 \ud83d\udcc4 format_downloaded_data.R   # Generates CLIN, EXPR, SNV input files\n    \u251c\u2500\u2500 \ud83d\udcc4 Format_CLIN.R              # Cleans and annotates clinical metadata\n    \u251c\u2500\u2500 \ud83d\udcc4 Format_EXPR.R              # Processes and logs RNA expression data\n    \u251c\u2500\u2500 \ud83d\udcc4 Format_SNV.R               # Cleans SNV mutation data\n    \u251c\u2500\u2500 \ud83d\udcc4 Format_CNA_seg.R           # Segmented CNA profiles \n    \u251c\u2500\u2500 \ud83d\udcc4 Format_CNA_gene.R          # Gene-level CNA profiles \n    \u2514\u2500\u2500 \ud83d\udcc4 Format_cased_sequenced.R   # Flags patients with RNA/CNA/SNV data\n</code></pre> <ul> <li>Script: Core functions for curating clinical and molecular data, ICB_Common</li> <li>Annotation: Source for drug, tissue and gene annotations files, Annotations</li> </ul> </li> </ul>"},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/","title":"Non-IO Clinical Trial Curation","text":"<p>This documentation describes the curation process of clinical trial data for non-immunotherapy datasets into a standardized R object.</p>"},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/#non-immunotherapy-datasets","title":"Non-immunotherapy datasets","text":""},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/#objective","title":"Objective","text":"<p>While most steps overlap between immunotherapy and non-immunotherapy dataset curation, it is important to understand the differences. The following details focuses on the current data elements and finally the differences.</p> <p>Currently, a non-ICB clinical dataset is curated into R's SummarizedExperiment (SE) object and not MAE because of the absence of multiple omics data. Sample code for curation can be found on Github.</p>"},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/#curation","title":"Curation","text":"<p>A non-ICB clinical data object contains the following data parts:</p> <ol> <li>Expression values or <code>Assay</code> data</li> <li>Clinical metadata: contains patient/sample metadata</li> </ol>"},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/#expression-values-or-assay-data","title":"Expression values or <code>Assay</code> data","text":"<p>Assay data contains genomic profiles of the patients. The data is usually processed in-house from raw files or in some instances, published processed data is used directly. For instance, gene expression profiles of the patients are typically generated by either microarray or RNA-seq platforms. In the BHK lab, we use Robust Multiarray Averaging (RMA) and CDF files from Brainarray for processing microarray data, and the Kallisto method for processing RNA-seq data, as mentioned in immunotherapy curation.</p>"},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/#clinical-metadata","title":"Clinical metadata","text":"<p>Any data pertaining to the samples or clinical response can be included in the Phenodata object. This is either fetched from public platforms like GEO if the data is public or upon request in case of confidentiality. Metadata sections in the SE objects include a few mandatory columns which are populated either by information from the other columns or the original published paper. NA is used to fill out columns for which no information is found. Each SE object includes additional metadata that may or may not be available in other SE objects.</p> <p>Mandatory columns are the same as immunotherapy <code>colData</code> (see above).</p>"},{"location":"disciplines/Data_Science/Data_Curation/Non_IO_Clinical_Trial_Curation/#gene-metadata","title":"Gene metadata","text":"<p>Similar to immunotherapy datasets, gene metadata for non-immunotherapy datasets is also obtained from Gencode annotations. \"Ensembl.v99.annotation.RData\" from \"Gencode.v33.annotation.RData\" is used for curating <code>rowData</code> of the SE object in non-immunotherapy datasets. Annotation data are available in BHKLab-Pachyderm's Annotation repository.</p>"},{"location":"disciplines/Data_Science/Visualization/","title":"Data Visualization","text":""},{"location":"disciplines/Data_Science/Visualization/#overview","title":"Overview","text":"<p>Data visualization is the process of representing data graphically to uncover patterns, trends, and insights. It involves selecting appropriate visual formats, designing clear and effective graphics, and ensuring that visualizations accurately reflect the underlying data. Effective data visualization helps communicate complex information in an accessible and meaningful way, supporting better analysis and decision-making.</p> <p>The lab has lots of experience generating visualizations for different types of data. Feel free to ask for help with what plots suit your data best, as well as example scripts.</p>"},{"location":"disciplines/Data_Science/Visualization/sankey/","title":"Sankey Plots in R","text":""},{"location":"disciplines/Data_Science/Visualization/sankey/#what-is-a-sankey-plot","title":"What is a Sankey Plot?","text":"<p>A Sankey plot is a flow diagram in which the width of the arrows is proportional to the magnitude of the flow. It\u2019s ideal for visualizing how quantities split, merge, or move between stages or categories. Sankey plots can also be variants like alluvial and bump charts, which emphasize different aspects of the data.</p> <p>Aside from the example below, a more comprehensive overview of Sankey plots can be found in the R Graph Gallery.</p>"},{"location":"disciplines/Data_Science/Visualization/sankey/#basic-example-with-networkd3","title":"Basic Example with networkD3","text":"<p>In this example, a super simple Sankey plot visualizes how patient sample might be split into training, testing, and validation sets.</p> <pre><code>library(tidyverse)\nlibrary(networkD3)\n\n# Define sample dataset: patient counts for each split\ndf_split &lt;- tibble(\n  set       = c(\"Total Patients\", \"Total Patients\", \"Total Patients\"),\n  subset    = c(\"Training\",       \"Testing\",        \"Validation\"),\n  count     = c(60,               20,               20)\n)\n\n# Create nodes: data.frame\nnodes &lt;- data.frame(name = unique(c(df_split$set, df_split$subset)))\n\n# Create links: map factor levels to node indices (zero-based)\nlinks &lt;- df_split |&gt;\n  mutate(\n    source = match(set,    nodes$name) - 1,\n    target = match(subset, nodes$name) - 1,\n    value  = count\n  ) |&gt;\n  select(source, target, value)\n\n# Draw the Sankey diagram\nsankeyNetwork(\n  Links     = links,\n  Nodes     = nodes,\n  Source    = \"source\",\n  Target    = \"target\",\n  Value     = \"value\",\n  NodeID    = \"name\",\n  fontSize  = 12,\n  nodeWidth = 30,\n  units     = \"patients\"\n)\n</code></pre> <p>Tip</p> <p>Use <code>?sankeyNetwork</code> to get specific details about the parameters and additional options for customizing the Sankey plot, such as how <code>links</code> and <code>nodes</code> should be defined.</p>"},{"location":"disciplines/Data_Science/Visualization/sankey/#references","title":"References","text":"<p>R Packages:</p> <ul> <li><code>networkD3</code></li> <li><code>ggalluvial</code></li> <li><code>ggsankey</code></li> </ul>"},{"location":"disciplines/Data_Science/Visualization/sankey/#additional-reading","title":"Additional Reading:","text":"<ul> <li>Differences with Sankey and Alluvial</li> <li>Comprehensive Sankey Tutorials</li> </ul>"},{"location":"disciplines/Imaging/","title":"Medical Image Formats","text":""},{"location":"disciplines/Imaging/#introduction","title":"Introduction","text":"<p>A medical image is a digital representation of the internal structure or function of an anatomic region, typically presented as an array of picture elements called pixels (2D) or voxels (volume pixels, 3D). This representation is a discrete mapping of numerical values to positions in space.</p> <p></p> <p>Image of Abraham Lincoln as a matrix of pixel values. (Source)</p> <p></p> <p>Section of abdominal computed tomography (CT) scan with a matrix of Hounsfield units (HU). (Source: Caryn Geady)</p> <p>The numerical values, known as intensity values, vary based on:</p> <ul> <li>Image type: MRI, CT, PET, etc.</li> <li>Acquisition method</li> <li>Reconstruction and post-processing</li> </ul>"},{"location":"disciplines/Imaging/#medical-image-metadata","title":"Medical Image Metadata","text":"<p>Medical images often come with metadata, which provides additional information about the image. This metadata is usually stored at the beginning of the image file as a \"header.\"</p>"},{"location":"disciplines/Imaging/#common-metadata-fields","title":"Common Metadata Fields","text":"<ul> <li>Image dimensions: Width, height, depth</li> <li>Voxel size: Spacing between voxels</li> <li>Origin: Location of the first voxel</li> <li>Orientation: Direction of x, y, and z axes</li> <li>Pixel depth: Bytes used to represent each voxel intensity</li> <li>Data type: Integer, floating-point, etc.</li> </ul> DICOM <pre><code>(0018,0015) CS BodyPartExamined = LUNG\n(0018,0050) DS SliceThickness = 3\n(0020,0037) DS ImageOrientationPatient = 1\\0\\0\\0\\1\\0\n(0020,1002) IS ImagesInAcquisition = 99\n(0028,0010) US Rows = 512\n(0028,0011) US Columns = 512\n(0028,0030) DS PixelSpacing = 0.9766\\0.9766\n(0028,0100) US BitsAllocated = 16\n(0028,0101) US BitsStored = 16\n(0028,0102) US HighBit = 15\n(0028,0103) US PixelRepresentation = 0\n</code></pre>"},{"location":"disciplines/Imaging/#pixel-data","title":"Pixel Data","text":"<p>The pixel data in a medical image file represents the actual image values, stored in a format specific to the image file.</p> <ul> <li>In fixed-size header formats, pixel data follows the header directly.</li> <li>In other formats, a marker or tag indicates the start of pixel data.</li> </ul>"},{"location":"disciplines/Imaging/#pixel-data-size","title":"Pixel Data Size","text":"\\[ Pixel Data Size = \\text{Rows} \\times \\text{Columns} \\times \\text{Pixel Depth (Bytes)} \\times \\text{Number of Frames} \\]"},{"location":"disciplines/Imaging/#image-file-size","title":"Image File Size","text":"\\[ Image File Size = \\text{Header Size} + \\text{Pixel Data Size} \\] Example Calculation: <p>For a DICOM image with the following parameters:</p> Parameter Value Rows 512 Columns 512 Pixel Depth 2 bytes (16-bit image) Number of Frames 32 (32 slices) \\[ \\text{Pixel Data Size} = 512 \\times 512 \\times 2 \\times 32 = 16,777,216 \\text{ bytes (or 16 MB)} \\] <p>Assuming the header size for this DICOM file is 1,024 bytes:</p> \\[ \\text{Image File Size} = 1,024 \\text{ bytes} + 16,777,216 \\text{ bytes} = 16,778,240 \\text{ bytes (or 16.01 MB)} \\]"},{"location":"disciplines/Imaging/#medical-image-file-formats","title":"Medical Image File Formats","text":""},{"location":"disciplines/Imaging/#categories-of-medical-image-formats","title":"Categories of Medical Image Formats","text":"<ol> <li> <p>Standardization Formats: Standardize images from diagnostic modalities.</p> <ul> <li>Example: DICOM</li> </ul> </li> <li> <p>Post-Processing Formats: Facilitate and strengthen post-processing analysis.</p> <ul> <li>Examples: Analyze, NIfTI, MINC</li> </ul> </li> </ol>"},{"location":"disciplines/Imaging/#configurations-for-storing-medical-images","title":"Configurations for Storing Medical Images","text":"<ul> <li> <p>Single File: Contains both metadata and image data, with metadata stored   at the beginning.</p> <ul> <li>Examples: DICOM, MINC, NIfTI</li> </ul> </li> <li> <p>Two Files: Metadata and image data stored separately.</p> <ul> <li>Example: Analyze (.hdr and .img)</li> </ul> </li> </ul>"},{"location":"disciplines/Imaging/Data_Sources/","title":"Data Sources","text":"<p>List of data sources for imaging data. Retrieved from publications, websites, and other sources.</p>"},{"location":"disciplines/Imaging/Data_Sources/#source-list","title":"Source List","text":"Source Name Source URL Data Type Description The Cancer Imaging Archive (TCIA) https://www.cancerimagingarchive.net/ CR, CT, DX, Histopathology, MG, MR, NM, PET, REG, RTSTRUCT, SEG, SR, US, The Cancer Imaging Archive (TCIA) is a service which de-identifies and hosts a large archive of medical images of cancer accessible for public download. Grand Challenge https://grand-challenge.org/challenges/ CT, Dermoscopy, Endoscopy, Fundus Photograph, Histology, IR, MG, MR, OCT, PET, US, XR A platform for end-to-end development of machine learning solutions in biomedical imaging. RAD IMAGE NET https://www.radimagenet.com/ CT, MR, US, XR RadImageNet is a large database of annotated medical images from multiple modalities and of multiple pathologies. The data can be licensed for commercial use. Mathematical Oncology Laboratory (MOLAB) https://molab.es/datasets-brain-metastasis-1/?type=metasrd MR, NIfTI segmentation, radiomic features A comprehensive dataset of annotated brain metastasis MR images with clinical and radiomic data EUCAIM https://dashboard.eucaim.cancerimage.eu/ CR, CT, DX, PET, MG, MR, NM, SEG, US Cancer Image Europe provides a robust, trustworthy platform for researchers, clinicians, and innovators to access diverse cancer images enabling the benchmarking, testing, and piloting of AI-driven technologies."},{"location":"disciplines/Imaging/Data_Sources/#legend","title":"Legend","text":"Acronym Name CR Computed Radiography CT Computed Tomography Dermoscopy Dermoscopy DX Digital Radiography Endoscopy Endoscopy Fundus Photograph Fundus Photograph Histology Histology IR Infrared MG Mammography MR Magnetic Resonance NIfTI Neuroimaging Informatics Technology Initiative NM Nuclear Medicine OCT Optical Coherence Tomography PET Positron Emission Tomography REG Registration RTSTRUCT Radiotherapy Structure SEG Segmentation SR Structured Report US Ultrasound XR X-Ray"},{"location":"disciplines/Imaging/Data_Types/DICOM/","title":"DICOM (Digital Imaging and Communications in Medicine)","text":"<p>TODO: add introduction to DICOM</p> <p>TODO: Add references if using images from online </p> <p></p> <p></p>"},{"location":"disciplines/Imaging/Data_Types/DICOM/#dicom-header-resources","title":"DICOM Header Resources","text":"<ul> <li>DICOM Standard Browser - Find meaning of specific DICOM tags</li> <li>Understanding DICOM</li> <li></li> </ul>"},{"location":"disciplines/Imaging/Data_Types/nifti/","title":"NIfTi Format","text":""},{"location":"disciplines/Imaging/Data_Types/nifti/#introduction-to-nifti","title":"Introduction to NIfTI","text":"<p>The NIfTI (Neuroimaging Informatics Technology Initiative) format is a derivative of the ANALYZE format, which was originally developed for medical imaging.</p>"},{"location":"disciplines/Imaging/Data_Types/nifti/#why-nifti","title":"Why NIfTI?","text":"<p>Before NIfTI, medical imaging data was stored in a variety of formats,  including ANALYZE, MINC, and DICOM [1].</p> <ul> <li>ANALYZE<ul> <li>ANALYZE is a proprietary format developed by the Mayo Clinic.</li> <li>ANALYZE files are not widely supported and are often difficult to work with.</li> </ul> </li> <li>MINC<ul> <li>MINC (Medical Image NetCDF) is a newer format developed by the National  Institutes of Health (NIH).</li> <li>MINC files are widely supported and can be easily shared and distributed.</li> </ul> </li> <li>DICOM<ul> <li>DICOM (Digital Imaging and Communications in Medicine) is a standardized  format for medical imaging data.</li> <li>DICOM files are widely supported and can be easily shared and distributed.</li> </ul> </li> </ul>"},{"location":"disciplines/Imaging/Data_Types/nifti/#general-information","title":"General Information","text":"<ul> <li>NIfTI files will typically be stored in a single file, with the extension <code>.nii</code> or <code>.nii.gz</code>.</li> <li>NIfTI files are typically used for medical imaging data, such as MRI, CT, PET, and fMRI.</li> <li>NIfTI files are widely supported and can be easily shared and distributed.</li> </ul>"},{"location":"disciplines/Imaging/Data_Types/nifti/#diagrams","title":"Diagrams","text":"<p>NIfTI-1: https://nifti.nimh.nih.gov/nifti-1/documentation/hbm_nifti_2004.pdf</p> <p>NIfTI structure diagram: https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1diagrams_v2.pdf</p>"},{"location":"disciplines/Imaging/Data_Types/nifti/#references-and-resources","title":"References and Resources","text":"<ol> <li>Medical Image File Formats, Michele Larobina, 2014. https://doi.org/10.1007/s10278-013-9657-9</li> <li>The NIFTI file format</li> <li>NIfTI-2</li> <li>NiBabel - NIfTI images</li> <li>NiBabel - Coordinate Systems</li> <li>NIFTI plain and simple</li> <li>NIfTI NIH site</li> </ol>"},{"location":"disciplines/Imaging/Radiomics/","title":"Radiomics","text":"<p>Radiomics is the high-throughput extraction of quantiative features from medical images for textural analysis that spatially characterises regions/volumes of interest and may provide insight into the underlying pathophysiology to support diagnosis, prognosis, and treatment planning (Barry et al., 2025). It bridges radiology and data science, allowing for a more detailed understanding of disease characteristics.</p> <p></p> <p>Radiomics analysis workflow (Source)</p>"},{"location":"disciplines/Imaging/Radiomics/#learn-the-foundations","title":"Learn the Foundations","text":"<ul> <li> <p>Intro to Python (CS50 Week 6)   Radiomics pipelines are often built in Python, so this course provides a solid foundation in the language.</p> </li> <li> <p>PyRadiomics Documentation   The official documentation for PyRadiomics, a robust open-source Python package for extracting engineered features from medical images.</p> </li> <li> <p>Deep Learning Specialization by Andrew Ng   A comprehensive course that covers the fundamentals of deep learning. Highly relevant for radiomics pipelines that involve neural networks for image feature extraction or outcome prediction.</p> </li> <li> <p>Lightning in 15 Minutes   A concise tutorial introducing PyTorch Lightning\u2014an efficient and reproducible way to structure deep learning projects, especially useful for radiomics model development.</p> </li> </ul>"},{"location":"disciplines/Imaging/Radiomics/#key-papers-and-case-studies","title":"Key Papers and Case Studies","text":"<ul> <li> <p>Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach \u2014 Nature Communications   This seminal paper is widely credited with sparking interest in radiomics by demonstrating how quantitative imaging features can decode tumor phenotype.</p> </li> <li> <p>Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer \u2014 Scientific Reports   A strong example of using engineered radiomic features to predict treatment outcomes in head-and-neck cancer.</p> </li> <li> <p>Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study \u2014 PLOS Medicine   Combines radiomics with deep learning to predict survival outcomes, showcasing the power of hybrid approaches.</p> </li> <li> <p>Artificial intelligence in radiology \u2014 Nature Reviews Cancer   A high-level overview of AI\u2019s role in modern radiology, including how radiomics fits into the broader clinical and research landscape.</p> </li> </ul>"},{"location":"disciplines/Imaging/Radiomics/Extraction_Tools/","title":"List of Radiomics Extractors and Frameworks","text":"<p>LIFEx: a freeware for radiomic feature calculation in multimodality imaging</p> <p>WORC: an open-source python package for the fully automatic execution of end-to-end radiomics pipelines</p> <p>ViSERA: a desktop software for radiomics techniques in medical imaging</p> <p>MIRP: a python package for quantitative analysis of medical images</p> <p>PyRadiomics: feature extraction python library</p> <p>O-RAW: Ontology-guided Radiomics Analysis Workflow </p> <p>PyRadGUI: A GUI based radiomics extractor software</p> <p>RadiomiX: imaging analysis software</p> <p>MaZda: A framework for biomedical image texture analysis and data</p> <p>AutoRadiomics: A Framework for Reproducible Radiomics Research</p> <p>MatRadiomics: radiomics freeware written in MATLAB and Python</p> <p>pyCERR:  Utilities are provided to extract, transform, organize metadata</p> <p>CaPTk:  for analysis of radiographic images of cancer</p> <p>mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging</p> <p>QIDS: Quantitative imaging decision support tool by HealthMyne</p> <p>DeepWise: Electronic medical record structuring, image annotation, AI lesion segmentation, automated feature extraction, automated deep learning tools</p>"},{"location":"disciplines/Imaging/Radiomics/Extraction_Tools/lifex/","title":"Local Image Features Extraction (LIFEx)","text":""},{"location":"disciplines/Imaging/Radiomics/Extraction_Tools/lifex/#documentation-links","title":"Documentation Links:","text":"<ol> <li>User Guide</li> <li>Features</li> <li>Scripts</li> <li>More</li> </ol>"},{"location":"disciplines/Imaging/Radiomics/Extraction_Tools/lifex/#sample-lifex-script","title":"Sample LIFEx Script:","text":"<p>To work with batch data, you can upload a .txt format script to the software. This script includes a path for the results file in addition to the path and configurations for each input image. A sample script is provided here. For more syntax details review Scripts.</p> <p>This script will perform feature extraction with the same configuration as the LIFEx GUI picture below, but on a batch of two series:</p> <p></p> <pre><code>## Lines with ## are comments\n\nLIFEx.Output.Directory=PATH_TO_RESULTS_DIR\n\n## _________________________________________________________________________________________________________________________\n\n## [Patient0] section\n\nLIFEx.Patient0.Series0=PATH_TO_SERIES_DIR\nLIFEx.Patient0.Series0.Operation0=Texture,true,false,false,1,3d,Absolute,10.0,400.0,-1000.0,3000.0,1.0,1.0,1.0\nLIFEx.Patient0.Roi0=PATH_TO_ROI_DIR\n\n## _________________________________________________________________________________________________________________________\n\n## [Patient1] section\n\nLIFEx.Patient1.Series0=PATH_TO_SERIES_DIR\nLIFEx.Patient1.Series0.Operation0=Texture,true,false,false,1,3d,Absolute,10.0,400.0,-1000.0,3000.0,1.0,1.0,1.0\nLIFEx.Patient1.Roi0=PATH_TO_ROI_DIR\n\n## _________________________________________________________________________________________________________________________\n</code></pre>"},{"location":"disciplines/Imaging/Radiomics/Extraction_Tools/lifex/#sample-python-script","title":"Sample Python Script:","text":"<p>To generate a LIFEx script, you can run a Python script. This Python sample code is for generating a LIFEx script from data with the following storage format:</p> <pre><code>|-- {DATASET_NAME}\n|   `-- {PatientID}\n|       |-- {ImageModality}_{SeriesInstanceUID}\n|       |   `-- {ImageModality}.nii.gz\n|       `-- {SegmentationModality}_{SeriesInstanceUID}\n|           `-- {ROI_name}.nii.gz\n</code></pre> <pre><code>import os\nfrom pathlib import Path\n\n# Path to output script to feed into LIFEx\noutput_script = Path(\"path/to/output/script.txt\")\n# Path to the lifex output (CSV file)\noutput_dir = Path(\"path/to/output/directory\")\n# Ensure the output directory exists\noutput_dir.mkdir(parents=True, exist_ok=True)\n# Path to the base directory containing patient folders\ndataset = Path(\"DATASET_NAME\")\n\npatient_ids = [p.name for p in dataset.iterdir() if p.is_dir()]\n\nwith open(output_script, \"w\") as f:\n    # Write header\n    f.write(\"## LIFEx script generated by BHKLab\\n\\n\")\n    f.write(f\"LIFEx.Output.Directory={output_dir}\\n\\n\")\n    f.write(\"##_________________________________________________________________________________\\n\\n\")\n    patient_index = 0\n    for patient in patient_ids:\n        patient_path = dataset / patient\n        if patient_path.is_dir():\n            inner_folders = [p for p in patient_path.iterdir() if p.is_dir()]\n            series_path = None\n            roi_path = None\n\n            for folder in inner_folders:\n                if folder.name.startswith(\"{ImageModality}\"):\n                    series_path = folder\n                elif folder.name.startswith(\"{SegmentationModality}\"):\n                    roi_path = folder\n                else:\n                    continue\n\n            if series_path and roi_path:\n                # Write patient section\n                f.write(f\"## [Patient{patient_index}] section\\n\\n\")\n                f.write(f\"LIFEx.Patient{patient_index}.Series0={series_path}\\n\")\n\n                # There can be multiple Operations or ROIs\n                f.write(f\"LIFEx.Patient{patient_index}.Series0.Operation0=Texture,true,false,false,1,3d,Absolute,10.0,400.0,-1000.0,3000.0,1.0,1.0,1.0\\n\")\n                f.write(f\"LIFEx.Patient{patient_index}.Roi0={roi_path}\\n\\n\")\n                f.write(\"##_________________________________________________________________________________\\n\\n\")\n                patient_index += 1\n\nprint(f\"LIFEx script written to {output_script}\")\n</code></pre>"},{"location":"disciplines/Imaging/Tools/","title":"Image Processing Tools","text":""},{"location":"disciplines/Imaging/Tools/#med-imagetools","title":"Med-ImageTools","text":"<p>Med-Imagetools is a free, open-source image processing tool for medical images developed by the BHK Lab.</p> <ul> <li>Publication: Med-ImageTools: An open-source Python package for robust data processing pipelines and curating medical imaging data</li> <li> Source code</li> <li> Documentation</li> </ul>"},{"location":"disciplines/Imaging/Tools/#3d-slicer","title":"3D Slicer","text":"<ul> <li>Slicer is an open-source platform with many built-in (and optional) plug-ins for image viewing and processing</li> <li>To install, visit: https://download.slicer.org/</li> <li>[Slicer.org](https://www.slicer.org/) is the site for 3D Slicer, offering downloads, training, and documentation.</li> </ul>"},{"location":"disciplines/Imaging/Tools/QIPCM/","title":"QIPCM","text":"<p>The Quantiative Imaging for Personalized Cancer Medicine (QIPCM) program provides end-to-end testing and analysis support for clinical trials to improve consistency and reliability in clinical trial data.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/","title":"Querying Data from the QIPCM PACS","text":"<p>Once you've been added to a project's delegation log, the team at QIPCM will grant you access to the data. At this time of this writing, QIPCM cannot directly upload the data to H4H for us, so you will need to use the tools below to query the data and then upload it to HPC4Health.</p> <p>There are three tools you will need to access the data:</p> <ol> <li>The QIPCM Toolbox (requires UHN Login)</li> <li>MongoDB Compass</li> <li>Horos</li> </ol> <p>Warning</p> <p>You need to be connected to either UHN-wireless-corporate wifi OR the UHN VPN to access the QIPCM Toolbox for this entire process as the setup requires your IP address to be consistent. If you expect to be working from multiple locations, you will need to set up different Horos listeners for each location with your QIPCM team member.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#setting-up-the-tools","title":"Setting up the Tools","text":"<p>The first time you access this database, you will need to work with a member of the QIPCM team to configure the tools.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#mongodb-compass","title":"MongoDB Compass","text":"<p>MongoDB Compass is used to query the QIPCM PACS database to extract metadata for the images you want to download. A QIPCM team member will provide you with details on how to connect to the database.</p> <p>After you have downloaded, installed, and launched MongoDB Compass, under the Connections tab, click the + symbol.</p> <p></p> <p>In the URL field, enter the connection string sent to you by the QIPCM team with the password. Set the name of the connection to QIPCM and click \"Save &amp; Connect\".</p> <p>You should now see your QIPCM database under Connections on the left side:</p> <p></p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#horos","title":"Horos","text":"<p>Horos is a DICOM viewer where the data from QIPCM will be sent to on your local machine.</p> <p>After you have downloaded, installed, and launched Horos, you will need to configure it to connect to QIPCM's database. Navigate to the Preferences menu (on a Mac, this is found in the Horos menu bar, then Settings...). You need to setup Locations and Listener(#horos-listener-menu) configurations.</p> <p></p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#horos-locations-menu","title":"The Locations Menu","text":"<p>Click the \"Add new node\" button to create a node for QIPCM data.</p> <p></p> <p>Set the following values for the new node:</p> <ul> <li>Address: qipcm-pacs.uhn.ca</li> <li>AETitle: QIPCM_OCTANE</li> <li>Port: 11112</li> <li>Q&amp;R: Check box / Yes</li> <li>Retrieve: C-MOVE</li> <li>Send: Uncheck box / No</li> <li>TLS: Uncheck box / No</li> <li>Name: QIPCM {Project or Dataset Name}</li> </ul> <p>Make sure you have checked the lefthand box so the location is active.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#horos-listener-menu","title":"The Listener Menu","text":"<p>From Listener menu, copy the values for:</p> <ul> <li>AETitle</li> <li>Port Number</li> <li>Address</li> <li>Host Name</li> </ul> <p>and send these to the QIPCM team member setting up your access. You will also need the AETitle when you download the images from the QIPCM PACS.</p> <p>Note</p> <p>Ensure the \"Activate DICOM listener when Horos is running\" box is checked.</p> <p>You can now close the Preferences window, but need to leave Horos open and running for the remainder of this process.</p> <p>Note</p> <p>Horos needs to be open for the duration of the data transfer from the QIPCM PACS to your local machine.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#querying-the-image-metadata","title":"Querying the Image Metadata","text":"<p>This next step will get the metadata from MongoDB Compass to identify which images you want to download from QIPCM.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#connecting-to-the-qipcm-database","title":"Connecting to the QIPCM Database","text":"<p>Open MongoDB Compass. Hover over the QIPCM connection on the left side of the window and click on the \"CONNECT\" button to access to your project's database. Navigate to scrapeDb on left hand side panel and select the collection you want to query.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#query-the-metadata","title":"Query the metadata","text":"<p>Now that you're connected, you can query the available image metadata to determine which specific images you want to download.</p> <p>On the right side of the window, next to the page navigation arrow, there is a hamburger menu, curly braces, and a table icon. Click the table icon to view the data in a table. At this point, you can query a subset of the data (e.g. by modality, patient ID, etc.) or you may export the entire collection.</p> <p>Once you have the data you want, click the EXPORT DATA button just above the table. Either export the query results or the full collection.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#exporting-the-metadata","title":"Exporting the metadata","text":"<p>This step will produce the CSV file you need to search the QIPCM database in the Toolbox.</p> <p>In the popup Export menu, under Fields to export, choose \"Select fields in table\". Then click \"Next\".</p> <p>In the next window, select the following fields:</p> <ul> <li>PatientID</li> <li>Modality</li> <li>SeriesInstanceUid</li> <li>StudyInstanceUid</li> </ul> <p>then click \"Next\".</p> <p>Select CSV as the Export File Type, then click \"Export\". Select where in your local file system you want to save the file and click \"Save\".</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#downloading-the-images","title":"Downloading the Images","text":"<p>Now that you have the desired image metadata, you can download the images from the QIPCM PACS using the QIPCM Toolbox. Make sure that Horos is still open and running!</p> <p>Navigate to The QIPCM Dashboard and login with your UHN credentials.</p> <p>On the main page, select the Portal option from the top menu to get to the Image Search page.</p> <p></p> <p>Select the following options:</p> <ul> <li>\"Select a PACS\", set it to QIPCM-PACS.</li> <li>\"Select a Trial\", set it to whichever study you want to download.</li> <li>\"Select a Reason\", set it to Trial Close Out.</li> </ul> <p>In the next section, click the Query with a CSV File tab. Click Load CSV and select the file you exported from MongoDB Compass.</p> <p>Click on the red \"Search PACS\" button and wait for the results to load.</p> <p>Once the results have loaded, select those you wish to download. You can filter by any of the headings in the table and use the Select All button at the bottom of the table.</p> <p>Under \"Select destination Trial\", find your AETitle from the Horos listener setup. Click the red \"Add to queue\" button and the download to Horos should begin.</p> <p>Warning</p> <p>If your download does not appear to be working, check the following:</p> <ol> <li>Make sure Horos is still open and running.</li> <li>Make sure you have the correct AETitle from the Horos listener setup in the \"Select destination Trial\" field.</li> <li> <p>Confirm that your IP address matches the Address field in the Horos listener setup. </p> Linux &amp; macOSWindows <p>In a terminal, run: <pre><code>ipconfig getifaddr en0\n</code></pre></p> <p>Open Command Prompt and run: <pre><code>ipconfig/all \n</code></pre></p> </li> </ol> <p>If all of these are correct, reach out to your QIPCM team member for further help.</p>"},{"location":"disciplines/Imaging/Tools/QIPCM/querying_data/#exporting-the-images","title":"Exporting the Images","text":"<p>To export from Horos, select the patient(s) you wish to export, and click the Export button at the top of the window.</p> <p>Select the directory you want to save the images to and click the \"Choose\" button.</p> <p>Your images should now be in the directory you selected, organized by patient.</p>"},{"location":"disciplines/Machine_Learning/","title":"What is Machine Learning?","text":"<p>Machine Learning (ML) is an approach of using computers to find patterns in data and make predictions or decisions without being explicitly programmed for specific tasks. It uses algorithms to analyze and learn from data to improve performance over time.</p>"},{"location":"disciplines/Machine_Learning/#why-do-we-use-machine-learning","title":"Why Do We Use Machine Learning?","text":"<p>Machine Learning is used because traditional programming methods, which rely on explicitly defining rules for every scenario, are often inefficient or impossible for complex problems. For example, recognizing faces, predicting stock prices, or diagnosing diseases involves patterns too intricate for manual rule-based systems.</p> <p>Before ML, we relied on statistical methods, manual rule-based algorithms, or human judgment, which had limited scalability and adaptability.</p>"},{"location":"disciplines/Machine_Learning/#advantages-of-machine-learning","title":"Advantages of Machine Learning","text":"<ul> <li>Scalability: ML handles large and complex datasets better than manual methods.</li> <li>Adaptability: Models improve automatically as they process more data.</li> <li>Automation: ML can automate repetitive tasks, saving time and resources.</li> </ul>"},{"location":"disciplines/Machine_Learning/#limitations-of-machine-learning","title":"Limitations of Machine Learning","text":"<ul> <li>Data Dependency: Requires large amounts of quality data to perform well.</li> <li>Black Box Models: Some ML models lack interpretability, making them hard to trust.</li> <li>Bias and Errors: ML can amplify biases present in the data.</li> <li>High Costs: Requires computational resources and expertise.</li> </ul>"},{"location":"disciplines/Machine_Learning/#where-does-machine-learning-fit-in-science","title":"Where Does Machine Learning Fit in Science?","text":"<p>Machine Learning is an interdisciplinary field at the intersection of:</p> <ul> <li>Computer Science: Provides algorithms and computational power.</li> <li>Statistics: Forms the mathematical foundation for analyzing and interpreting data.</li> <li>Artificial Intelligence (AI): ML is a subset of AI focused on learning from data.</li> <li>Deep Learning (DL): A specialized branch of ML that uses neural networks to process large amounts of data.</li> </ul> <p>ML integrates concepts from these fields to solve diverse problems in areas like biology, finance, healthcare, and engineering.</p>"},{"location":"disciplines/Machine_Learning/#when-should-we-use-machine-learning","title":"When Should We Use Machine Learning?","text":""},{"location":"disciplines/Machine_Learning/#appropriate-use-cases","title":"Appropriate Use Cases","text":"<ul> <li>When there's a need to analyze large datasets for complex patterns.</li> <li>When the problem requires automation or decision-making without explicit programming.</li> </ul>"},{"location":"disciplines/Machine_Learning/#when-we-should-avoid-machine-learning","title":"When We Should Avoid Machine Learning","text":"<ul> <li>When data is insufficient or of poor quality.</li> <li>When simpler, rule-based systems can solve the problem more effectively.</li> <li>When interpretability is critical, and black-box methods aren't acceptable.</li> </ul>"},{"location":"disciplines/Machine_Learning/#categories-and-branches-of-machine-learning","title":"Categories and Branches of Machine Learning","text":"<p>ML is broadly divided into three main categories:</p> <ol> <li> <p>Supervised Learning:</p> <ul> <li>Models are trained on labeled data (input-output pairs).</li> <li>Examples: Linear regression, support vector machines (SVMs).</li> </ul> </li> <li> <p>Unsupervised Learning:</p> <ul> <li>Models learn patterns from unlabeled data.</li> <li>Examples: Clustering (e.g., k-means), dimensionality reduction (e.g., PCA).</li> </ul> </li> <li> <p>Reinforcement Learning:</p> <ul> <li>Models learn by interacting with an environment and receiving rewards or penalties.</li> <li>Examples: Markov decision process (MDP), Deep Q Networks (DQN).</li> </ul> </li> </ol>"},{"location":"disciplines/Machine_Learning/learning_resources/","title":"Learning Resources","text":"<p>Luis Serrano - Serrano.Academy - YouTube</p> <p>Supervised Machine Learning: Regression and Classification | Coursera</p> <p>Practical Machine Learning | Coursera</p> <p>Elements of Statistical Learning: data mining, inference, and prediction - 2nd Edition - Hastie, Tibshirani, and Friedman </p> <p>An Introduction to Statistical Learning, more of an introduction than ESL, by James, Witten, Hastie, and Tibshirani.</p>"},{"location":"disciplines/Machine_Learning/tools/","title":"Tools","text":""},{"location":"disciplines/Machine_Learning/tools/#python-libraries","title":"Python Libraries","text":""},{"location":"disciplines/Machine_Learning/tools/#deep-learning-accelerator-enabled-computing","title":"Deep Learning &amp; Accelerator-Enabled Computing","text":"<ul> <li>PyTorch</li> <li>Lightning</li> </ul>"},{"location":"disciplines/Machine_Learning/tools/#r-libraries","title":"R Libraries","text":"<ul> <li>glmnet</li> </ul>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/","title":"Introduction","text":"<p>Unsupervised Learning (UL) is a branch of machine learning where the inputs are not labeled. Generally speaking, the goal of UL is to find some sort of \"hidden\" (sometimes called \"latent\") structure in the dataset. </p> <p>The two most common tasks in UL are dimensionality reduction and clustering. Parts of the field known as generative modeling also fit under the umbrella of unspervised learning. </p> <ul> <li> <p>Dimensionality Reduction consists of techniques to, as the name suggests, reduce the dimension of the data. Concretely, suppose your data consists of vectors that are in \\(\\mathbb{R}^d\\). Dimensionality reduction procedures will, for each data point \\(x_i\\in\\mathbb{R}^d\\) in your dataset, create a corresponding vector \\(\\tilde{x}_i\\in \\mathbb{R}^k\\) where \\(k\\) is much smaller than \\(d\\). </p> </li> <li> <p>Clustering consists of methods to partition the data into subgroups that are disjoint (i.e., do not overlap). After clustering has been performed the discovered groups can be analyzed and compared.</p> </li> <li> <p>Generative Modeling is a subfield of machine learning that aims to learn the probability distribution that generated the dataset. This can be useful for trying to generate synthetic examples of data. In addition some techniques for generative modeling, such as variational autoencoders (VAEs), can also be used for dimensionality reduction. </p> </li> </ul>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/","title":"Clustering","text":"<p>The idea of clustering algorithms is to discover \"hidden\" groups in a dataset. Specifically it's about taking a collection of \\(d\\)-dimensional observations and partitioning the set into \\(K\\geq 1\\) groups such that the groups \"make sense.\"</p> <p>Several algorithms exist. Some of the most popular ones are</p> <ol> <li>K-means</li> <li>Spectral Clustering</li> <li>Gaussian Mixture Models</li> </ol>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/K_Means/","title":"K-Means Clustering","text":"<p>The idea behind k-means clustering is to choose a number of clusters, \\(k\\), and centroids for the clusters, and then iteratively refine the cluster assignments and centroids until a number of iterations is reached or the cluster assignments don't change. Detailed mathematical developments can be found in [1]. </p> <p>Specifically given a dataset of points \\(\\mathcal{D}= \\{x_1, \\dots, x_n\\}\\) where \\(x_i\\in\\mathbb{R}^n\\) and an integer parameter \\(k\\geq2\\) we pick \\(k\\) random vectors \\(c_1, \\dots, c_k\\in \\mathbb{R}^d\\) which serve as candidate centroids and iteratively perform the following steps:</p> <ul> <li>E Step This step updates the cluster assignments.</li> <li>M Step This step updates the centroids. </li> </ul> <p>Adopting the convention that \\(d(x_j, c_k) = ||x_j-c_k||_2^2\\) and letting \\(C(j)\\in\\{1,\\dots, k\\}\\)  denote the cluster assignment of data point \\(x_j\\), adding up over all of the clusters we penalize a given assignment \\(C\\) via the following objective function:</p> \\[ J = \\sum_{k}\\sum_{j:C(k)=k} = \\sum_{j=1}^N\\sum_{\\ell=1}^k||x_j-c_\\ell||A_{j\\ell} \\] <p>where \\(A_{j\\ell}\\) is an indicator (0/1 variable) indicating if observation \\(j\\) assigned to cluster \\(l\\). Concretely \\(A_{j\\ell} = 1\\) if \\(C(j)=\\ell\\) and is 0 otherwise. </p> <p>Looking at the objective function we can see that it separates over clusters, thus for fixed clusters we can minimize \\(J\\) by assigning each \\(x_j\\) to its closest centroid. Using this insight the algorithm fixes clusters and penalizes them, and then updates the centroids by computing the means. </p> <p>Thus the algorithm to minimize \\(J\\) proceeds in two rounds:</p> <p>1) E step This step is done first and assigns clusters according to which closest  $$ C(j)=\\underset{r=1,\\dots, k}{\\text{argmin }} d(x_j,c_r)^2 $$</p> <p>2) M step We update the centroids by computing the average of points in each cluster: $$ c_\\ell =\\frac{\\sum_{j=1}^n A_{jl}x_j}{N_\\ell} $$ where \\(N_\\ell = \\sum_{j=1}^nA_{j\\ell}\\), that is, the number of points in the \\(\\ell\\)'th cluster. </p> <p>The \\(M\\) step is derived by taken the gradient of \\(J\\) with respect to each centroid,  \\(\\nabla_{c_k}J\\),  and setting it to zero. According to [1] the algorithm is guaranteed to converge but not to a global minimum. </p> <p>\\(K\\)-means works as long as the metric \\(d\\) is differentiable with respect to \\(c_k\\).  That is if we replace \\(||x_j-c_\\ell||^2\\) with any differentiable \\(d(x_j, c_\\ell)\\) we can still use \\(k\\) means. Of particular interest is the case where the metric as an inner-product norm derived from a positive definite matrix \\(M\\), that is: $$ d(x_j,c_\\ell) = (x_j-c_\\ell)^TM(x_j-c_\\ell) $$ as in the case of metric learning (see [2] for inner product norms and [3] for metric learning). </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/K_Means/#worked-example","title":"Worked Example","text":"<p>We follow the same set up as in the worked example for spectral clustering. Specifically we simulate expression data by generating two groups from negative binomial distributions and subsequently applying a log transform.  <pre><code>from sklearn.cluster import KMeans\nfrom scipy.stats import nbinom\nimport numpy as np\n\nseed = 1234 # seed to get reproducible behavior\nn1, p1 = 10, 0.3\nn2, p2 = 15, 0.5\n\ng1 = nbinom.rvs(n1, p1, size = 50,random_state = seed).reshape(10,5) # simulated expression for distribution 1\ng2 = nbinom.rvs(n2, p2, size = 50,random_state = seed).reshape(10,5) # simulated expression for distribution 2\n\n\nX = np.log2(np.vstack((g1,g2))+0.001)\nn_nbrs = int(np.ceil(np.log(X.shape[0])))+1\ntrue_labels = 10*[0]+10*[1]\n</code></pre></p> <p>Running the algorithm and checking the predicted clusters is simple: <pre><code>clustering_algo = KMeans(n_clusters = 2)\nclustering_algo.fit(X)\n\n\nprint(true_labels)\nprint(clustering_algo.labels_)\n</code></pre></p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/K_Means/#references","title":"References","text":"<ul> <li> <p>[1] Bishop, Christopher M. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006.</p> </li> <li> <p>[2] Lax, Peter D. Linear Algebra and Its Applications, 2nd Edition</p> </li> <li> <p>[3] Bellet, Aur\u00e9lien, Amaury Habrard, and Marc Sebban. Metric learning. Morgan &amp; Claypool Publishers, 2015.</p> </li> </ul>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/Spectral_Clustering/","title":"Spectral Clustering","text":"<p>Spectral clustering is a graph-theoretic method used for clustering data points. The paper [1]. </p> <p>There are many versions of this algorithm which leverage different graph Laplacians derived from similarity scores. Thus Spectral Clustering will work on any collection of objects \\(\\{x_1,\\dots, x_n\\}\\) as long as one can compute similarity scores \\(s_{ij}\\geq 0\\) for all pairs of points. </p> <p>The basic sketch for the algorithm is:</p> <ol> <li>Construct a weighted graph adjacency matrix \\(W\\) from \\(s_{ij}\\). Construct a Laplacian \\(L\\) (normalized, regular, random walk, etc.) from \\(W\\). </li> <li>Compute first \\(R\\) eigenvectors of \\(L\\), call them \\(u_1, \\dots, u_R\\) </li> <li>Construct the \\(n\\times R\\) matrix \\(U=\\left[u_1, u_2, \\dots, u_R\\right]\\)  4 . Use K-means or other clustering algorithms to cluster the rows and return the clusters. This requires tuning the number of clusters \\(k\\) </li> </ol> <p>Different flavors and interpretations of the algorithms can be given, depending on the Laplacian used. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/Spectral_Clustering/#rules-of-thumb","title":"Rules of Thumb","text":"<p>The general rule of thumb is to use a \\(k\\) nearest neighbor graph with \\(k\\) tuned to make the graph connected. For large graphs try \\(k\\approx \\mathcal{O}(\\log(n))\\), else try it manually. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/Spectral_Clustering/#worked-example","title":"Worked Example","text":"<p>This code snippet gives an example of how to run spectral clustering in python. We begin by importing the needed libraries:</p> <pre><code>from sklearn.cluster import SpectralClustering\nfrom scipy.stats import nbinom\nimport numpy as np\n</code></pre> <p>The next step involves simulating some data. We generate 10 samples each from 2 separate negative binomial distributions. The choice of negative binomial distribution is due to its common use in modeling the raw counts of bulk RNA-seq. </p> <pre><code>seed = 1234 # seed to get reproducible behavior\nn1, p1 = 10, 0.3\nn2, p2 = 15, 0.5\n\ng1 = nbinom.rvs(n1, p1, size = 50,random_state = seed).reshape(10,5) # simulated expression for distribution 1\ng2 = nbinom.rvs(n2, p2, size = 50,random_state = seed).reshape(10,5) # simulated expression for distribution 2\n</code></pre> <p>Next we stack this data into a matrix and perform a pointwise \\(\\log_2(x+0.001)\\) transformation like those done on ORCESTRA. We also follow the rule of thumb above and pick the number of neighbors as the natural log of the number of observations. Lastly, we write down the labels.</p> <pre><code>X = np.log2(np.vstack((g1,g2))+0.001)\nn_nbrs = int(np.ceil(np.log(X.shape[0])))+1 # add an offset to make the graph slightly more dense\ntrue_labels = 10*[0]+10*[1]\n</code></pre> <pre><code>clustering_algo = SpectralClustering(n_clusters = 2,affinity = 'nearest_neighbors',n_neighbors=n_nbrs)\nclustering_algo.fit(X)\n</code></pre> <p>After fitting we can access the predicted labels and compare them to the ground truth using the following: <pre><code>print(clustering_algo.labels_)\nprint(true_labels)\n</code></pre></p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Clustering/Spectral_Clustering/#references","title":"References","text":"<ul> <li>[1]  Von Luxburg, Ulrike. \"A tutorial on spectral clustering.\" Statistics and computing 17 (2007): 395-416.</li> </ul>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/","title":"Introduction","text":"<p>In general we assume we have a dataset \\(X\\) which consists of \\(n\\) observations in \\(\\mathbb{R}^d\\). The dimensionality of the dataset is \\(d\\). </p> <p>Dimensionality reduction is often used as pre-processing step for subsequent analyses, including clustering, supervised learning, or visualization. Common techniques for dimensionality reduction include:</p> <ul> <li>Principal Component Analysis (PCA) </li> <li>Random Projections</li> <li>t-distributed Stochastic Neighbor Embeding (t-SNE)</li> <li>Uniform Manifold Approximation and Projection (UMAP)</li> <li>Locally Linear Embeddings (LLE)</li> <li>Laplacian Eigenmaps</li> </ul> <p>Dimensionality reduction is needed due to a phenomenon known as the curse of dimensionality, which states that there is a pessimism-inducing relationship between the dimension of your data. Most dimensionality reduction techniques are based on something called the manifold hypothesis, which assumes that the data, while living in some high dimensional space, are actually on some much lower dimensional surface. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/#the-curse-of-dimensionality","title":"The Curse of Dimensionality","text":"<p>The curse of dimensionality (CoD), roughly speaking, states that as \\(d\\) grows, the number of samples \\(n\\) needed to be able to develop a good statical model increases exponentially on the order of \\(2^d\\). For context, \\(2^265\\) is roughly the number of atoms in the universe and, by modern standards, 265 is relatively small dimension. In pharmacogenomics we often work with datasets where \\(d\\approx 1000\\) after dimensionality reduction! </p> <p>It might be helpful to understand a bit of the why behind the CoD. Suppose we  start with \\(d=1\\) and have data that consists of \\(\\{-1,+1\\}\\) chosen uniformly at random (probability of \\(\\frac{1}{2}\\)). How many samples would we need to draw in order to get a representative data set? On average, we'd expect the answer to be 2. </p> <p>Suppose we now set \\(d=2\\) and consider a similar set up where the data is sampled uniformly at random from the four corners of the square \\(\\{(+1,+1), (+1,-1), (-1,-1),(-1,+1)\\}\\). Similar reasoning suggests that we need 4 samples, average, to get a data set that overs the sampling domain (i.e. each point is represented at least once). </p> <p>If \\(d=3\\) we then have a hypercube with 8 points for each possible combination of \\((\\pm1, \\pm 1, \\pm1)\\), which suggests we need \\(8\\) samples to get coverage. </p> <p>At each step the number of samples needed to cover the data set domain is approximately \\(2^d\\) and this holds in all dimensions. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/#the-manifold-hypothesis","title":"The Manifold Hypothesis","text":"<p>While the curse of dimensionlity paints a bleak picture, the manifold hypothesis offers some help. The manifold hypothesis supposes that the observations \\(x_i\\in \\mathbb{R}^d\\) live on some unknown lower-dimensional surface (a manifold).</p> <p>As a visual example, suppose you have a bunch of points in \\(\\mathbb{R}^3\\). After some data analysis you discover that they all have the same Euclidean norm, which means they lie on a sphere, a two-dimensional surface that lies in a three-dimensional space. </p> <p>In a higher dimension, if you have data consisting of vectors \\(x_i\\in\\mathbb{R}^d\\) but discover that there a vector \\(w\\in \\mathbb{R}^d\\) such that \\(w^Tx_i=0\\) for all \\(i\\), then the data actually lie on a hyperplane, which must be of smaller dimension. Even simpler, if each \\(x_i\\) is a multiple of some given vector, \\(\\tilde{x}\\), that is \\(x_i=c_i\\tilde{x}\\) then all the data points lie on a line, which is one-dimensional. </p> <p>The dimensionality reduction methods listed at the top of the page differ, in part, in the nature of the manifold they assume and how they try to approximate/learn it. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/PCA/","title":"Principal Component Analysis (PCA)","text":""},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/PCA/#intuition","title":"Intuition","text":"<p>The rough idea behind PCA is to find the directions in which the data varies. The PCA algorithm proceeds iteratively, beginning by finding the direction in which the data set \"points most.\" The algorithm then finds the direction that is orthogonal (that is, perpendicular) to the first direction and captures the most variation, and so on. Recall that two vectors \\(x,y\\) are orthogonal if \\(x^Ty=0\\) and we write this as \\(x\\perp y\\).</p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/PCA/#details","title":"Details","text":"<p>Suppose we have a collection of \\(n\\) observations in \\(\\mathbb{R}^d\\), that is we have \\(x_1, \\dots, x_n\\) where \\(x_i\\in \\mathbb{R}^d\\). Since we are looking for directions we only focus on vectors \\(v\\) with unit norm: </p> <p>\\(||v||_2 = \\sqrt{\\sum_i v_i^2} = 1\\)</p> <p>Suppose we have such a \\(v\\). We can then compute how much each \\(x_i\\) points in the direction of \\(v\\) by computing \\(\\tilde{x}_i=x_i^Tv\\). We can do this for all of our data points to get a collection of \\(n\\) real numbers \\(\\{\\tilde{x}_1, \\dots, \\tilde{x}_n\\}\\). To compute how much the data varies in the direction \\(v\\) we can just compute the variance of this set of real numbers, that is we compute Var\\((\\tilde{x}_1, \\dots, \\tilde{x}_n)\\). </p> <p>Now that we have a way to compute how much the data varies in a given direction, we need to find the optimal directions as outlined at the top of this page. PCA computes:</p> \\[ u_1 = \\underset{v\\in \\mathbb{R}^d, ||v||_2=1}{\\text{argmax}}\\text{Var}(x_1^Tv, \\dots, x_n^Tv)\\\\ u_2 = \\underset{v\\in \\mathbb{R}^d, ||v||_2=1, v\\perp u_1}{\\text{argmax}}\\text{Var}(x_1^Tv, \\dots, x_n^Tv)\\\\ u_3 = \\underset{v\\in \\mathbb{R}^d, ||v||_2=1, v\\perp u_1, v\\perp u_2}{\\text{argmax}}\\text{Var}(x_1^Tv, \\dots, x_n^Tv)\\\\ \\vdots \\\\ u_d = \\underset{v\\in \\mathbb{R}^d, ||v||_2=1, v\\perp u_1, \\dots, u_{d-1}}{\\text{argmax}}\\text{Var}(x_1^Tv, \\dots, x_n^Tv)\\\\ \\] <p>Note that \\(u_1, \\dots, u_d\\) are \\(d\\) orthogonal vectors with unit length in \\(\\mathbb{R}^d\\), and thus form an orthonormal basis. </p> <p>But how do we compute these vectors? The answer comes via matrix algebra, in particular we use the singular value decomposition (SVD) to find the directions (which will be the singular vectors) and the amounts of variation (the singular values). To keep this page relatively self-contained we recall a few facts about the SVD.</p> <p>SVD Reminders</p> <ul> <li>The SVD is a decomposition of a matrix. Meaning it re-writes a matrix \\(M\\in\\mathbb{R}^{n\\times d}\\) as a product of other matrices. </li> <li>The SVD rewrites \\(M\\) as the product of three matrices, \\(U, \\Sigma, W\\). Explicitly we have $$ M=U\\Sigma W^T $$ </li> <li>\\(U\\in \\mathbb{R}^{n \\times n}\\) where the columns are unit vectors that are orthogonal. The columns of \\(U\\) are called the right singular vectors.</li> <li>\\(\\Sigma\\in \\mathbb{R}^{n\\times d}\\) and is diagonal with non-negative real numbers along the diagonal. The diagonal entries are called the singular values. We assume that the columns of \\(\\Sigma\\) are listed in decreasing order of the singular values. These are written as \\(\\sigma_i\\). </li> <li>\\(W\\in \\mathbb{R}^{d\\times d}\\) where the the columns are unit vectors that are orthogonal. The columns are called the right singular vectors. </li> </ul>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/PCA/#the-pca-procedure","title":"The PCA Procedure","text":"<p>The full PCA procedure is as follows. </p> <ol> <li>Compute the mean of the data \\(\\bar{x}=\\frac{1}{n}\\sum_{i}x_i\\)</li> <li>Center the data, that is subtract \\(\\bar{x}\\) from each data point to create a new collection \\(a_i=x_i-\\bar{x}\\)</li> <li>Stack the \\(a_i\\) into a matrix \\(A = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n\\end{bmatrix}\\in\\mathbb{R}^{n\\times d}\\)</li> <li>Compute the SVD of the matrix \\(A\\), that is \\(A=U\\Sigma W^T\\). </li> </ol> Warning <p>Centering the data is required for PCA to succeed. If you do fail to do this the first principal component will be just the mean and thus, a waste of computation. </p> Tip <p>It is wise to make sure that your data is, roughly, on the same scale. This is due to the construction of PCA. Suppose one of the dimensions contains values around 100 and the other features are in between 0 and 1. Then the dot product will be distorted by that dimension even if the variation in one of the other dimensions is larger. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/PCA/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>So far we've only just computed an SVD which hasn't actually reduced the dimensionality. Note that each column \\(w_j\\) of \\(W\\) is in \\(\\mathbb{R}^d\\) and so we can compute \\(a_i^Tw_j\\) for all data points. Doing this for all the \\(w_j\\) could be written as the matrix product \\(AW\\). However, doing this doesn't change the dimension, as this is a product of an \\(n\\times d\\) matrices and a \\(d\\times d\\) matrix. </p> <p>The way to reduce dimensionality is to only pick a subset of the \\(w_j\\). Suppose we pick the first \\(L\\) columns of \\(W\\), write this \\(W_L\\), then \\(AW_L\\in \\mathbb{R}^{n\\times L}\\) which is dimensionally reduced.  The question then is how to choose \\(L\\). There are two methods:</p> <ol> <li>Since the singular values are assumed to be listed in decreasing order, we can choose the first \\(L\\) columns to pick the largest one where \\(L\\) is a user-determined design point. </li> <li>The second method aims to capture a percentage of the variance. Specifically let \\(B\\) be the sum of the singular values and suppose we want to capture \\(p\\) percentage of the variance. Pick \\(L\\) such that: \\(\\frac{\\sum_{i=1}^L\\sigma_i}{B}\\geq p\\)</li> </ol> <p>Mathematical Note</p> <p>Looking at the SVD again we can note that \\(AW=U\\Sigma W^TW = U\\Sigma\\) and so the principal components are the left singular vectors and the principal values the corresponding singular values. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/PCA/#worked-example","title":"Worked Example","text":"<p>First we generate some data. We perform a simulation that generates data that should look a little like gene expression data in log-transformed transcripts per million (TPM).</p> <pre><code>from sklearn.decomposition import PCA\nfrom scipy.stats import nbinom\nimport numpy as np\n\nseed = 1234 # seed to get reproducible behavior\nn1, p1 = 10, 0.3\nn2, p2 = 15, 0.5\n\ng1 = nbinom.rvs(n1, p1, size = 50,random_state = seed).reshape(10,5) # simulated expression for distribution 1\ng2 = nbinom.rvs(n2, p2, size = 50,random_state = seed).reshape(10,5) # simulated expression for distribution 2\n\n\nX = np.log2(np.vstack((g1,g2))+0.001)\n</code></pre> <p>The centering of <code>X</code> can be done manually or using <code>scikit-learn</code> built in functionality. The latter is recommended but we give both for completeness. <pre><code># manual\nX = X-np.mean(X,axis=0,keepdims=True)\n\n# scikit-learn\nfrom sklearn.preprocessing import StandardScaler\nsclr = StandardScaler(with_std=False) # only want to center, not scale in this instance. \nX = sclr.fit_transform(X)\n</code></pre></p> <p>To see how to get explained variance, let's try with the maximum number of principle components which is the dimensionality of the data, which in this instance is 5.  <pre><code>dim_red = PCA(n_components=5)\ndim_red.fit(X)\n</code></pre></p> <p>Luckily <code>sciki-learn</code> has functions for immediately giving percentage of variance. Suppose we want to explain 70% of the variance.  The following block will find the minimum number of principal components to compute to above this value. If the data was much higher dimension, say over 500 features, this approach will be slow and it is better to start with a guess at number of principal components needed. </p> <pre><code>explain_variance = dim_red.explained_variance_ratio_\ncumulative_explained_variance = np.cumsum(dim_red.explained_variance_ratio_)\ntol = 0.7\nlocations_above_tol = [k for k in range(len(cumulative_explained_variance)) if cumulative_explained_variance[k]&gt;=tol]\nnc = min(locations_above_tol)+1 # add 1 because python uses 0 indexing. \n</code></pre> <p>Now that we have the minimum required number of components, we can compute the PCA and reduce down to desired number of dimensions.: <pre><code>dim_red = PCA(n_components=nc)\ndim_red.fit(X)\nX_reduced = dim_red.transform(X)\n\nprint(X.shape) # (20,5)\nprint(X_reduced.shape) #(20,2)\n</code></pre></p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/TSNE/","title":"T-SNE","text":"<p>t-SNE is a common to</p> <p>....TODO: add in the description of how t-SNE works and how it can be used for visualization. </p>"},{"location":"disciplines/Machine_Learning/Unsupervised_Learning/Dimensionality_Reduction/UMAP/","title":"UMAP","text":"<p>TODO...</p>"},{"location":"disciplines/Pharmacogenomics/","title":"Introduction","text":"<p>Pharmacogenomics is the discipline of associating molecular patient measurements with clinical outcomes. </p>"},{"location":"disciplines/Pharmacogenomics/Cell_Viability_Screens/","title":"Cell Line Viability Screens","text":"<p>TODO</p>"},{"location":"disciplines/Pharmacogenomics/Navigating_PSets/","title":"Navigating PSets","text":"<p>When you first join the BHKLab Pharmacogenomics (PGx) team, you will likely need to familiarize yourself with our R package <code>PharmacoGx</code>.  The fundamental object of <code>PharmacoGx</code> is the PSet, which is a data structured specifically designed to handle the inputs, results, and meta-data surrounding a cell line screen. </p> <p>PSets can be a bit counter-intuitive at first and so this page provides a rough guide on how to manipulate them. At present we only have tools in <code>R</code> to navigate them. </p> <p>Before we get into the basics, it is probably worth addressing a natural question: why use PSets?  The answer is that pharmacogenomics data is messy.  Research into links between small molecules and compound effect is conducted in many instutitions, each of which has their own idiosyncratic approach to handling and annotating data. PSets allow for standardized nomenclature across datasets, meaning each PSet will ahve the same column names inside its dataframes, a common set of identifiers for drugs and for cell lines, and quality-controlled response measurements. </p> <p>This page only covers the basics. Once you've reviewed these snippets you can look at the detailed vignettes here.</p>"},{"location":"disciplines/Pharmacogenomics/Navigating_PSets/#finding-psets","title":"Finding PSets","text":"<p>In order to find PSets of interest, you can look at PharmacoDB or ORCESTRA and manually download them. In addition PharmacoGx has a the functions <code>availablePSets()</code> and <code>downloadPSet()</code> functions which can be used to download data. An example call is:</p> <pre><code>downloadPSet(\n  name = \"CCLE_2015\",\n  saveDir = \"../psets\", # change this directory as you see fit\n  timeout = 3600,\n  verbose = TRUE\n)\n</code></pre>"},{"location":"disciplines/Pharmacogenomics/Navigating_PSets/#loading-psets","title":"Loading PSets","text":"<p>The PSets will be downloaded as a <code>.RDS</code> file and so will need to be loaded via a call to <code>readRDS</code>. If your PSet is stored in <code>PSet.file.path</code> then the following script will load it:</p> <pre><code>library(PharmacoGx)\nmy.PSet &lt;- readRDS(PSet.file.path)\n\nmy.PSet &lt;- updateObject(ps) # update to the latest version\n</code></pre>"},{"location":"disciplines/Pharmacogenomics/Navigating_PSets/#accessing-drug-info","title":"Accessing Drug Info","text":"<p>To get information about the drugs used in the PSet you can use the <code>drugInfo</code> function to access information about the compounds used in the screen. </p> <pre><code>drugInfo(my.PSet)\n</code></pre> <p>If you wanted to get just the compound names and their representations as molecular SMILES strings, the following will do the trick: <pre><code>drug.data &lt;- as.data.frame(drugInfo(my.PSet))%&gt;% select(treatmentid,smiles)\n</code></pre></p>"},{"location":"disciplines/Pharmacogenomics/Navigating_PSets/#accessing-cell-line-info","title":"Accessing Cell Line Info","text":"<p>This is done via the <code>sampleInfo</code> function:</p> <pre><code>sample.Data &lt;- sampleInfo(my.PSet)\n</code></pre>"},{"location":"disciplines/Pharmacogenomics/Navigating_PSets/#accessing-treatment-response-info","title":"Accessing Treatment Response Info","text":"<p>To get treatment response information we need the <code>treatmentResponse</code> function. This returns a list with several items. To get the data of treament response the following will work: <pre><code>treatmentResponse(my.PSet)$info\n</code></pre></p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/","title":"Accessing Data in a PharmacoSet","text":"<p>Quick Reference</p> <p>For a more detailed introduction to PharmacoSet, see the Vignettes and Reference Manual in the PharmacoGx package.</p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/#quick-start-tutorial-load-a-tiny-demo-pset","title":"Quick\u00a0start tutorial: Load a tiny demo PSet","text":"<p>Accessor functions give you a clean, read\u2011only (or read\u2011write) gateway to each slot. Use them instead of poking the S4 object directly.</p> <pre><code># Get started by setting up PharmacoGx\nlibrary(PharmacoGx)\ndata(CCLEsmall)             # ships with the package\ntreatmentInfo(CCLEsmall)    # peek at treatment metadata\n</code></pre> What is an accessor? <p>Accessors are S4 methods whose names read like plain English:</p> <ul> <li><code>treatmentInfo()</code> returns (or sets) the <code>TreatmentResponse</code></li> <li><code>molecularProfiles()</code> returns (or sets) the <code>MultiAssayExperiment</code></li> </ul> <p>They shield you from slot names, so your code keeps working even if the class internals change.</p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/#treatments","title":"Treatments","text":"<p>Retrieve or update treatment metadata in a single call.</p> Migration to Pharmacoset v2.0 <p>The <code>drugInfo</code> and <code>drugNames</code> slots are deprecated in Pharmacoset v2.0. Use the new accessors below instead. - <code>treatmentInfo()</code> replaces <code>drugInfo()</code> - <code>treatmentNames()</code> replaces <code>drugNames()</code></p> <p>Similarly, <code>sampleInfo</code> and <code>sampleNames</code> are now used for sample metadata, instead of <code>cellInfo</code> and <code>cellNames</code>.</p> treatmentInfotreatmentNames <p>One\u2011row\u2011per\u2011treatment <code>data\u2011frame</code> holding IDs, names, targets, etc. <pre><code># Getter\ninfo &lt;- treatmentInfo(pSet)\n# Setter\ntreatmentInfo(pSet) &lt;- info\n</code></pre></p> <p>Character vector of treatment IDs. <pre><code>names &lt;- treatmentNames(pSet)\ntreatmentNames(pSet) &lt;- names\n</code></pre></p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/#samples","title":"Samples","text":"<p>Keep track of cell\u2011line annotations without wrangling rows manually.</p> sampleInfosampleNames <p>Per\u2011sample metadata table (cell\u2011line, tissue, etc.). <pre><code>sampleInfo(pSet)\n</code></pre></p> <p>Character vector of sample IDs. <pre><code>sampleNames(pSet)\n</code></pre></p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/#molecular-profiles","title":"Molecular profiles","text":"<p>Treat multi\u2011omics data like a tidy list of matrices plus metadata.</p> <p>First, we need to know what data types are available, then we can dive into the actual data.</p> Migration to Pharmacoset v2.0 <p>The <code>molecularProfiles</code> slot is now a <code>MultiAssayExperiment</code> (MAE) object from SummarizedExperiment. It holds multiple assays (e.g. RNA, CNV) with their own metadata.</p> <p>The old approach was just a list of <code>SummarizedExperiment</code> objects, which is less flexible and harder to subset/filter.</p> Get available data types <p>List available omics layers (<code>\"rna\"</code>, <code>\"cnv\"</code>, \u2026). <pre><code>mDataNames(pSet)\n</code></pre></p> Why two steps? <p>First ask what data types exist (<code>mDataNames</code>), then dive into a matrix with <code>molecularProfiles</code>.</p> molecularProfilesfeatureInfophenoInfo <p>Grab an assay matrix for a chosen layer. <pre><code>expr &lt;- molecularProfiles(pSet, \"rna\", \"exprs\")\n</code></pre></p> <p>Row\u2011level feature metadata (<code>rowData</code>). <pre><code>featureInfo(pSet, \"rna\")\n</code></pre></p> <p>Column\u2011level sample metadata (<code>colData</code>). <pre><code>phenoInfo(pSet, \"rna\")\n</code></pre></p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/#treatment-response","title":"Treatment Response","text":"Migration to PharmacoSet v2.0 <p>The <code>treatmentResponse(pSet)</code> accessor now returns a TreatmentResponseExperiment (TRE) object from CoreGx\u2014a <code>SummarizedExperiment</code>\u2011derived container with assays (e.g. viability) plus <code>rowData</code> (samples) and <code>colData</code> (treatments).</p> <p>A TRE supports multi\u2011drug screens and high\u2011dimensional synergy analyses introduced in PharmacoGx 3.0.</p> <p>It replaces the old list\u2011based sensitivity slot, though legacy getters still work for stability.</p> <code>TreatmentResponseExperiment</code> accessorslegacy accessors <p>Modern workflow using a <code>TreatmentResponseExperiment</code> (TRE). <pre><code>tre &lt;- treatmentResponse(pSet)\n\n# Get the names of the assays\nassayNames(tre)\n\n# Access any assay (i.e 'viability', 'computed_profiles', 'published_profiles')\nhead(assay(tre, \"viability\"))\nrowData(tre)    # sample metadata\ncolData(tre)    # treatment metadata\n</code></pre></p> <p>Old functions remain for backward compatibility. <pre><code>sensitivityInfo(pSet)\nsensitivityProfiles(pSet)\nsensitivityRaw(pSet)\n</code></pre></p>"},{"location":"disciplines/Pharmacogenomics/pharmacoset_accessors/#faq","title":"FAQ","text":"<ul> <li>Missing assay name? If you skip the <code>assay</code> arg, <code>molecularProfiles</code>   defaults to the first assay in that SummarizedExperiment.</li> <li>Deprecated slots? <code>datasetType</code> and <code>perturbation</code> remain for   backward compatibility; lean on accessors instead.</li> </ul>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/","title":"TreatmentResponseExperiment Tutorial","text":""},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Context</li> <li>Summarized Features</li> <li>Structure</li> <li>Diagram of Structure</li> <li>Key Components</li> <li>Example Pipelines that create a TreatmentResponseExperiment</li> <li>Example Repositories</li> <li>Extra Resources</li> </ul>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#context","title":"Context","text":"<ul> <li>Developed to address shortcomings of the old <code>@treatmentResponse</code> data structures</li> <li>Designed to be highly flexible, extendable, and performant for high-dimensional data</li> <li>Inspired by the design of the SummarizedExperiment and MultiAssayExperiment classes</li> </ul>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#summarized-features","title":"Summarized Features","text":"<p>The <code>TreatmentResponseExperiment(TRE)</code> is a specialized data class from the CoreGx package, organize and store treatment response data in a structured way.</p> <ul> <li>holds information about how biological samples respond to various treatments</li> <li><code>treatment</code> usually means applying one or more drugs to a biological sample (e.g., cell lines, tissues)</li> <li>the observed viability (survival) of the sample after treatment and   any computed metrics/profiles (e.g., IC50, AUC) are stored in the <code>assays</code> slot</li> </ul>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#structure","title":"Structure","text":""},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#diagram-of-structure","title":"Diagram of Structure","text":"<p>This structure makes it easy to work with drug response data. It's designed to be flexible, allowing easy access and manipulation while being performant when computing over millions of data points.</p> <p></p> <p>Note</p> <p>For a more detailed explanation of the structure, see the TreatmentResponseExperiment Class Vignette.</p>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#key-components","title":"Key Components","text":"<p>To best understand the <code>TreatmentResponseExperiment</code>, we will conceptualize it as a data structure with four main components.</p> <p>Tip</p> <p>You can access these respective components using the <code>rowData</code>, <code>colData</code>, <code>assays</code>, and <code>metadata</code> functions, e.g.:</p> <pre><code>rowData(tre)\nassays(tre)\n</code></pre> <ol> <li><code>rowData</code> - The treatments you're testing<ul> <li><code>rowIDs</code>: the columns that identify unique treatment experiment (make up the <code>rowKey</code>)</li> <li><code>rowKey</code>: the unique identifier for each treatment experiment</li> <li>Examples of identifiers:<ul> <li>\"Doxorubicin at 10 \u00b5M\" (a single drug at a specific concentration)</li> <li>\"Combination of Drugs A+B at 10 \u00b5M and 5 \u00b5M respectively, for 24 hours\" (a combination treatment, specifying both drugs and their concentrations, and duration)</li> </ul> </li> </ul> </li> <li><code>colData</code> - The cell lines or patient samples<ul> <li><code>colIDs</code>: the columns that identify unique biological samples (make up the <code>colKey</code>)</li> <li><code>colKey</code>: the unique identifier for each biological sample</li> <li>Examples of identifiers:<ul> <li>\"MCF7, technical replicate B2\"</li> <li>\"Primary tumor cells from patient ID-456, isolation batch 2\"</li> </ul> </li> </ul> </li> <li><code>assays</code> - The actual results of your experiments<ul> <li>Example:<ul> <li><code>assay(\"viability\")</code>: a table where each cell shows how well a specific sample survived after a specific treatment</li> <li><code>Doxorubicin::10\u00b5M</code> for <code>MCF7::B2</code> might show a viability of <code>0.75</code> (meaning 75% of the cells survived)</li> <li><code>assay(\"profiles\")</code>: a table where each unique treatment experiment has a computed metric like IC50 or AUC</li> <li><code>Doxorubicin</code> for <code>MCF7</code> might show an IC50 value of <code>0.5 \u00b5M</code></li> </ul> </li> </ul> </li> <li><code>metadata</code> - General notes about your experiment<ul> <li>\"Experiment run by Dr. Smith on January 15, 2025\"</li> <li>\"All samples were treated for 24 hours at 37\u00b0C\"</li> <li>\"Date of data collection: January 20, 2025\"</li> <li>\"Data source: XYZ Biobank\"</li> </ul> </li> </ol>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#example-pipelines-that-create-a-treatmentresponseexperiment","title":"Example Pipelines that create a <code>TreatmentResponseExperiment</code>","text":"<p>We have a handful of examples that demonstrate the various approaches, given different data sources and formats, to create a <code>TreatmentResponseExperiment</code>.</p> <p>Within each of these repositories, you will find a:</p> <ul> <li>the <code>workflow/Snakefile</code> file that defines the steps to create the <code>TreatmentResponseExperiment</code></li> <li>the <code>docs/data_sources.md</code> page on the github website that describes the data sources and their formats</li> <li>the <code>TreatmentResponseExperiment</code> object created by the pipeline in Github Actions and uploaded to the <code>releases</code> section of the repository</li> </ul> <p>These examples are meant to show how the <code>TreatmentResponseExperiment</code> becomes the standardized data structure for treatment response data, regardless of the source or format of the data.</p> <p>They can then be easily used in downstream analyses, such as creating a <code>PharmacoSet</code>, performing drug response analysis, or integrating with other data types.</p>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#example-repositories","title":"Example Repositories","text":"<ul> <li>GDSC Treatment Response</li> <li>CTRPv2 Treatment Response</li> <li>GCSI Treatment Response</li> <li>CCLE Treatment Response</li> </ul>"},{"location":"disciplines/Pharmacogenomics/treatment-response-experiment/#extra-resources","title":"Extra Resources","text":"<ul> <li>TreatmentResponseExperiment Class Vignette</li> <li>Bioconductor CoreGx Package</li> <li>CBW Workshop 2024: Computing over a <code>TreatmentResponseExperiment</code></li> <li>CBW Workshop 2024: Subsetting and plotting a <code>TreatmentResponseExperiment</code></li> </ul>"},{"location":"grants/","title":"Grant Submission Handbook: Tools, Guides &amp; References","text":"<p>This handbook is a guide to navigating the grant submission process efficiently. It includes a curated list of grants to consider, step-by-step instructions on how to apply, pre-submission checklists, and reusable templates to streamline the process.</p> <p>Please note that each grant agency is unique in certain areas. We suggest verifying the information directly from grant websites.</p> <p>Point of contact for BHK Lab grant applications - Sisira Kadambat Nair @ sisira.nair@uhn.ca</p>"},{"location":"grants/Identifying_Suitable_Grants/","title":"Identifying Suitable Grants","text":""},{"location":"grants/Identifying_Suitable_Grants/#grant-databases-websites","title":"Grant Databases &amp; WebsitesCanadian funding opportunities","text":""},{"location":"grants/Identifying_Suitable_Grants/#canadian-institutes-of-health-research-cihr","title":"Canadian Institutes of Health Research (CIHR)","text":"<p>CIHR has several funding programs.  Read more </p> <ul> <li>New Frontiers in Research Fund</li> <li>Training award programs</li> <li>Project Grant Program</li> <li>Foundation Grant Program</li> <li>Initiatives</li> <li>Prizes</li> </ul> <p>Note: Check with your PI if you are unsure of which program to apply for. If you have access to UHN email, you can also track emails from research-community-news-bounces@uhnresearch.ca.</p> <p>Applying for funding - List of current funding opportunities</p> <p>Note</p> <p>Benjamin will have an account on ResearchNet. Your PI can add you as delegate on the application. Final submission can be done only by the NPI. Please reach out directly to Benjamin or contact Sisira at sisira.nair@uhn.ca for questions regarding submission portal for the lab.</p>"},{"location":"grants/Identifying_Suitable_Grants/#genome-canada-gapp","title":"Genome Canada (GAPP)","text":"<p>Funding guidelines and policies</p> <p>Funding opportunities</p>"},{"location":"grants/Identifying_Suitable_Grants/#terry-fox-research-institute-tfri","title":"Terry Fox Research Institute (TFRI)","text":"<p>Programs for Funding Research</p>"},{"location":"grants/Identifying_Suitable_Grants/#cancer-research-society-crs","title":"Cancer Research Society (CRS)","text":"<p>Funding Programs</p>"},{"location":"grants/Identifying_Suitable_Grants/#natural-sciences-and-engineering-research-council-of-canada-nserc","title":"Natural Sciences and Engineering Research Council of Canada (NSERC)","text":"<p>NSERC has several programs for both Professors and Students/Fellows. Please check sub-sections within the below links.</p> <p>Discover grants Innovate grants</p>"},{"location":"grants/Identifying_Suitable_Grants/#canadian-cancer-society-ccs","title":"Canadian Cancer Society (CCS)","text":"<p>Open funding opportunities for Researchers</p> <p>Application portal (EGRaMS)</p> <p>Note</p> <p>Benjamin will have an account on EGrAMS. Please reach out directly or contact Sisira at sisira.nair@uhn.ca for questions regarding submission portal for the lab.</p>"},{"location":"grants/Identifying_Suitable_Grants/#university-of-toronto-data-sciences-institute-dsi","title":"University of Toronto Data Sciences Institute (DSI)","text":"<p>Funding Opportunities</p> <p>Programs include, not limited to: Catalyst Grant program, competitive seed funding program Data Access Grant Research Software Development Support Program</p>"},{"location":"grants/Identifying_Suitable_Grants/#digital-research-alliance-of-canada-dra","title":"Digital Research Alliance of Canada (DRA)","text":"<p>Funding Opportunities</p>"},{"location":"grants/Identifying_Suitable_Grants/#ontario-institute-for-cancer-research-oicr","title":"Ontario Institute for Cancer Research (OICR)","text":"<p>Funding Opportunities</p>"},{"location":"grants/Identifying_Suitable_Grants/#princess-margaret-cancer-centre-pmpmcc","title":"Princess Margaret Cancer Centre (PM/PMCC)","text":"<p>PM offers a variety of seed funding to support researchers. Please check UHN emails for funding announcement.</p>"},{"location":"grants/Identifying_Suitable_Grants/#the-temerty-centre-for-artificial-intelligence-research-and-education-in-medicine-t-cairem-at-the-university-of-toronto","title":"The Temerty Centre for Artificial Intelligence Research and Education in Medicine (T-CAIREM) at the University of Toronto","text":"<p>Past TCAIREM grants include T-CAIREM/DSI Catalyst Grants, T-CAIREM Health Data Nexus Dataset Grants, AI for Population Health and Health Systems Implementation Grant, Vector Institute-Temerty Clinical AI Integration Grant etc.</p> <p>Grant Opportunities</p>"},{"location":"grants/Identifying_Suitable_Grants/#canadian-institute-for-advanced-research-cifar","title":"Canadian Institute for Advanced Research (CIFAR)","text":"<p>Current research programs</p> Corporate funding opportunities"},{"location":"grants/Identifying_Suitable_Grants/#roche","title":"Roche","text":"<p>Roche Canada Funding Request Tool</p> International funding opportunities"},{"location":"grants/Identifying_Suitable_Grants/#us-department-of-defense-dod","title":"U.S. DEPARTMENT OF DEFENSE (DOD)","text":"<p>This is an extensive process. Please request the help of Research Grants months ahead for advice and document requirements.</p> <p>Grant Programs</p>"},{"location":"grants/Identifying_Suitable_Grants/#the-dataworks-prize-faseb-nih","title":"The DataWorks! Prize (FASEB &amp; NIH)","text":"<p>Federation of American Societies for Experimental Biology (FASEB) and the National Institutes of Health (NIH) hosts an annual challenge that showcases the benefits of research data management while recognizing and rewarding teams whose research demonstrates the power of data sharing or reuse practices. Webpage only</p>"},{"location":"grants/Identifying_Suitable_Grants/#eu-funding","title":"EU funding","text":"<p>European Commission: Horizon Europe has Pillar I and II funding programs. </p> <ul> <li> <p>Pillar I - Investigator-driven initiatives in all research areas that give the scientific community a strong role in determining the avenues of research to be pursued:</p> </li> <li> <p>Pillar II - Collaborative research and innovation projects responding to thematically specific calls for proposals within the following six clusters that address key societal challenges.</p> </li> </ul> <p>Read more</p> <p>Check here for active funding calls using filter view</p> Cloud credits opportunities"},{"location":"grants/Identifying_Suitable_Grants/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<p>Apply for Google Cloud research credits</p>"},{"location":"grants/Identifying_Suitable_Grants/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<p>AWS Cloud Credit for Research</p> <p>You can also check Azure Research credits</p>"},{"location":"grants/Submission_Process/","title":"Submission Process","text":"<p>Each funding agency has different submission portals and requirements. </p> <p>Ensure you:</p> <ul> <li> <p>Create an account on the required submission portal (e.g., ResearchNet, EGrAMS)</p> </li> <li> <p>Format the application according to guidelines (PDF, Word, online forms)</p> </li> <li> <p>IMPORTANT -Check for word limits, font, and spacing requirements</p> </li> <li> <p>Validate the submission using any built-in tools provided by the portal</p> </li> <li> <p>Ensure all attachments are correctly labeled and uploaded in the correct sections</p> </li> <li> <p>Confirm that all required fields are completed and sections marked as \"complete\"</p> </li> <li> <p>Print or save a copy of the submission confirmation/receipt for your records. Add it to the bhklab manage drive folder</p> </li> <li> <p>If the system allows, generate and download a compiled copy of the full application as submitted</p> </li> </ul>"},{"location":"grants/Templates_%26_Resources/","title":"Templates &amp; Resources","text":"<ol> <li> <p>UHN Sharepoint for  finding Research Grants - Dashboard</p> </li> <li> <p>Submit here for grant budget review  by UHN Research Grants team BusinessHub</p> </li> <li> <p>Use this for creating figures - BHK lab Miro</p> </li> <li> <p>General Gantt chart, created on Feb 2025 CIHR template</p> </li> <li> <p>Budget calculations made easy, created on Feb 2025 CIHR template</p> </li> <li> <p>Research Data management template RDM</p> </li> </ol>"},{"location":"grants/Writing_a_Strong_Grant_Proposal/","title":"Writing a Strong Grant Proposal","text":"<p>This is intended as a guide to the general sections of a full grant proposal. Please refer to the funding guidelines for specific requirements.</p> <p>Pro Tip</p> <p>Export your proposal to Microsoft Word to check the spelling and grammar errors, you can choose to display information about the reading level of the document. This will include readability scores according to the Flesch-Kincaid Grade Level test and Flesch Reading Ease test. </p> <p>Understand readability scores</p>"},{"location":"grants/Writing_a_Strong_Grant_Proposal/#scientific-part-of-the-proposal","title":"Scientific part of the proposal","text":"<p>Lay title and Summary/Abstract</p> <p>Write a concise, jargon-free summary that clearly explains the project's purpose, significance, and expected impact. This is often the first (and sometimes only) section reviewers read, so clarity and accessibility are key. Very relevant for non-scientific stakeholders such as patient, patient partners etc.</p> <p></p> <p>Background/Problem Statement</p> <p>Present the context and significance of the research. What gap does it address? Why is it important now? Support with current literature and statistics to make a compelling case.</p> <p>Pro Tip</p> <p>Articulate better by dividing into sub-sections with titles instead on one big section</p> <p>Preliminary work</p> <p>Demonstrate that the project builds on solid groundwork. Include a table of data, previous publications, or plots from proof-of-concept studies to show feasibility and credibility. These should be indexed in the text as well. </p> <p>Pro Tip</p> <p>Include an Overiew of the research idea, highlight preliminary sections vs. Aims</p> <p>Hypothesis</p> <p>State the central hypothesis or research question. It should be specific, testable, and grounded in the background presented. A well-defined hypothesis guides the aims and methodology.</p> <p>Specific Aims</p> <p>Clearly defined research objectives or aims. Brief summary of the hypothesis or core research question. What the project seeks to accomplish and why it's significant.</p> <p>Methodology</p> <p>Detailed description of the research design, methods, and techniques to be used. Justification for chosen approaches. </p> <p>Feasibility, Risk &amp; Mitigation Strategies</p> <p>Outline potential risks or challenges (technical, ethical, logistical) and explain how they will be mitigated. Highlight available expertise, access to necessary infrastructure, and alternative approaches. For example, mention statistical model limitations, data privacy, clinical data acquisition challenges etc.</p> <p>Project Outcomes and future directions</p> <p>Describe the expected scientific outcomes and broader impact. Indicate how the findings will be disseminated and how they may inform future research or policy. Include any plans for scaling, follow-up studies, or knowledge translation. Include Gantt chart for Timeline and milestones.</p> <p>Pro Tip</p> <p>For Gantt chart, you can enhance visualization by coding dark colors to initial deliverables and light colours for improvements or updates following early studies.</p> <p>Research Data Management &amp; Open Science</p> <p>Detail how data will be collected, stored, shared, and preserved. Include plans for open access publication, data repositories, and compliance with FAIR data principles (Findable, Accessible, Interoperable, Reusable).</p> <p>Note</p> <p>If your proposal includes private data access, include secure storage plans via H4H or GCP. Mention the size of data if known. Adhere to the regulations included in the Personal Information Protection and Electronic Documents Act (PIPEDA), Health Insurance Portability and Accountability Act (HIPAA) and the General Data Protection Regulation (GDPR) for Canada, USA, and Europe, respectively. Also refer to First Nations principles of Ownership, Control, Access, and Possession (OCAP) in Canada and mention if applicable.</p> <p>Expertise, Experience &amp; Resources</p> <p>Most grants include multiple labs. Highlight the strengths of each teams. Include relevant expertise, past accomplishments, and institutional support (labs, equipment, collaborations) that ensure project success.</p>"},{"location":"grants/Writing_a_Strong_Grant_Proposal/#other-sections","title":"Other sections","text":"<p>Sex and/or Gender considerations (SGBA)</p> <p>Discuss how sex and/or gender will be considered in the design, data collection, analysis, and interpretation of results. If SGBA is not applicable, provide a rationale for its exclusion</p> <p>Summary of Progress</p> <p>Often refers to NPI's research progress so far. Please reach out to your NPI as they might have a draft already.</p> <p>Budget</p> <p>Please check grant agency specific requriements for budget. Breakdown the costs, cross the t's dot the i's</p> <ul> <li> <p>Personnel: Salaries, benefits, and stipends for staff and researchers</p> </li> <li> <p>Consumables: Supplies and materials needed for experiments. </p> </li> <li> <p>Non-Consumables: Equipment purchases or rentals</p> </li> <li> <p>Knowledge Translation: Costs for dissemination activities, open access publishing, stakeholder engagement</p> </li> </ul> <p>Peer review information</p> <p>To be discussed with NPI. The lab has a list of PI names and credentials required for this section.</p> <p>Suggested Peer Review Committees: List preferred committees or panels for reviewing your application</p> <p>Reviewers to Exclude for this Application: List any individuals or groups that should not be involved in the review process, with justifications if required</p>"},{"location":"grants/Preparing_for_Submission/","title":"Preparing for Submission","text":"<p>While there are common sections, each grant agency has specific requirements. Before submitting a grant application, ensure you have all necessary documents and have addressed key requirements.</p> <p>Use this guide to navigate each stage of the grant application process, from initial registration to final submission. </p>"},{"location":"grants/Preparing_for_Submission/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Create a folder under BHKLab Manage Google drive. General access is restricted to Benjamin, Soleil and Sisira, hence reach out to Sisira to create folders.</p> </li> <li> <p>Sub folders SHOULD be arranged in the following hierarchy.</p> <pre><code>    Parent folder --&gt; Application, Submitted, Reports\n</code></pre> </li> <li> <p>It is recommended to schedule a meeting with all PIs and lab members involved to outline the proposal.</p> </li> <li> <p>Login to submission portal to check sections and create a google doc to use as a working proposal .</p> </li> </ol>"},{"location":"grants/Preparing_for_Submission/#step-1-registration","title":"Step 1: Registration","text":"<ul> <li> <p>Register on the funding agency's submission portal listed in Identifying_Suitable_Grants.md </p> <ul> <li>In most cases, this has to be done from the PI's account. Ask Benjamin or Sisira</li> </ul> </li> <li> <p>Create or update your profile with current contact information and institutional details</p> <ul> <li>Ask Soleil for Benjamin's updated CV</li> </ul> </li> <li> <p>Obtain necessary identifiers of participants (e.g., CCV ID, ORCID)</p> </li> </ul> <p>Pro Tip</p> <p>Attend webinars detailing the grant submission conducted by grant agencies.</p>"},{"location":"grants/Preparing_for_Submission/#step-2-letter-of-intent-loi-or-registration-if-required","title":"Step 2: Letter of Intent (LOI) or Registration (if required)","text":"<p>Check if the grant agency requires an Expression of Interest (EOI) prior to LOI submission.</p> <ul> <li> <p>Review specific grant requirements to determine if an LOI or preliminary proposal is needed</p> </li> <li> <p>Draft and submit the LOI by the stated deadline</p> </li> <li> <p>NPI review is required before submission</p> </li> <li> <p>Wait for approval or invitation to submit a full proposal</p> </li> </ul>"},{"location":"grants/Preparing_for_Submission/#step-3-internal-coordination-and-approvals","title":"Step 3: Internal Coordination and Approvals","text":"<p>Pro Tip</p> <p>Start this as early as possible to provide enough time for approvals, review etc.</p> <ul> <li>Inform relevant institutional offices about your grant submission (e.g., UHN Research Grants, RFS)</li> </ul> <p>Budgets should be added to BusinessHub  see the UHN Budget Process page</p> <ul> <li> <p>Obtain internal approvals, budgets, and documentation</p> </li> <li> <p>Coordinate with collaborators or co-investigators</p> </li> <li> <p>If your project involves biospecimens and animal models, you will need to obtain certificates from labs that generate the data. For patient data, REB is required</p> </li> <li> <p>Obtain Letters of Support and letters of collaboration </p> </li> </ul>"},{"location":"grants/Preparing_for_Submission/#step-4-develop-full-proposal","title":"Step 4: Develop Full Proposal","text":"<ul> <li> <p>Follow the structure outlined in the Proposal Template section</p> </li> <li> <p>Use BHK lab Miro for figures (Point of contact - Sisira)</p> </li> <li> <p>Gantt charts can be easily created with google sheets CIHR template</p> </li> <li> <p>This template can be adapted to each grants CIHR Budget template</p> </li> <li> <p>Research Data management is a crucial part of all grants, access a template here RDM</p> </li> <li> <p>Some grant websites use specific templates for each sections, please stick to those templates</p> </li> <li> <p>Draft, review, and refine each sections</p> </li> </ul>"},{"location":"grants/Preparing_for_Submission/#step-5-assemble-supporting-documents","title":"Step 5: Assemble Supporting Documents","text":"<ul> <li> <p>Budget and budget justification</p> </li> <li> <p>CVs or biosketches for key personnel</p> </li> <li> <p>Letters of support or commitment</p> </li> <li> <p>Certificates</p> </li> <li> <p>Institutional endorsement or authorization</p> </li> </ul>"},{"location":"grants/Preparing_for_Submission/#step-6-peer-review-final-checks","title":"Step 6: Peer Review &amp; Final Checks","text":"<ul> <li> <p>Request feedback from colleagues, mentors, or grant officers</p> </li> <li> <p>Share with PIs for a final review</p> </li> <li> <p>Check for compliance with guidelines, formatting, and completeness</p> </li> </ul>"},{"location":"grants/Preparing_for_Submission/#step-7-submit-the-proposal","title":"Step 7: Submit the Proposal","text":"<ul> <li> <p>Upload the proposal and all required documents to the submission portal</p> </li> <li> <p>Run checks on the portal if available</p> </li> <li> <p>Submit before the deadline and save confirmation docs to Submitted folder on drive</p> </li> </ul>"},{"location":"grants/Preparing_for_Submission/UHN_budget_process/","title":"UHN Budget Process","text":"<p>Budgets for grants need to be submitted to the UHN Grants Office at least one week before the application deadline.</p>"},{"location":"grants/Preparing_for_Submission/UHN_budget_process/#uhn-grant-application-module-info","title":"UHN Grant Application Module Info","text":"<p>Budgets must be submitted to UHN through their Business Hub application portal. In case you don\u2019t already have access to Business Hub, please see the Delegation section on the SharePoint module, which includes a form to be filled out so Business.Hub@uhn.ca can provide you with access.</p> <p>Once you have access to Business Hub, navigate to the Grant Applications page and click New Grant Application.</p> <ol> <li>Select Full Application</li> <li>Fill out the form with your budget information.     </li> <li>Once you've submitted, you'll receive an email with feedback from the grants office and whether you're approved for submission.</li> </ol>"},{"location":"onboarding_offboarding/Offboarding/","title":"Offboarding Policy","text":"<p>Thank you for your time in the lab. To ensure a smooth transition and back up of your work, please follow the policy detailed below. This policy applies to all individuals leaving BHK lab\u2019s employ. It is meant to ensure that all staff who leave the employ of BHK lab ensure that all relevant intellectual and physical assets purchased with BHK lab funds remain in BHK lab. </p>"},{"location":"onboarding_offboarding/Offboarding/#offboarding-meetings","title":"Offboarding Meetings","text":"<p>Individuals under the employ of BHK lab who either received or provided a notice of termination shall abide by the following offboarding processes. The process shall include the following meetings: </p> <ol> <li> <p>Offboarding Initialization - at least two (2) weeks prior to the last official working day</p> </li> <li> <p>Exit Interview - a final meeting on the day prior to the employee's last official working day or based on availability</p> </li> </ol>"},{"location":"onboarding_offboarding/Offboarding/#offboarding-initialization","title":"Offboarding Initialization","text":"<p>An assigned lab staff member will contact the employee to set this meeting. This meeting will be use to summarize the off-boarding requirements.</p> <ul> <li>A folder will be shared with the employee to add documents discussed during the initial meeting. This includes but is not restricted to a detailed back up of project details such as links to code, data folders and documentation.</li> <li>The Lab Off-boarding Information template will be shared with the employee. Please make a copy of this document in your offboarding report folder.</li> <li> <p>The Exit Interview Form will be shared with the employee. This should be completed just prior to the exit interview.</p> <p>Warning</p> <p>If you expect continued work with the lab for a publication or similar activity, permissions to any of the lab services MUST be requested and marked in this Exit Interview form.</p> </li> </ul>"},{"location":"onboarding_offboarding/Offboarding/#exit-interview","title":"Exit Interview","text":"<p>This meeting will be used to ensure all work items have been appropriately turned over and an exit interview will be conducted.</p> <p>The final documents will be reviewed for completeness and accuracy by the lab member taking over the project or mentor (students). The members will identify any missing items, or items that require clarification by the employee. The terminated employee will develop a progress plan to address any issues that arise in this meeting.</p>"},{"location":"onboarding_offboarding/Offboarding/#offboarding-intellectual-assets","title":"Offboarding Intellectual Assets","text":""},{"location":"onboarding_offboarding/Offboarding/#offboarding-report","title":"Offboarding Report","text":"<p>Members leaving the lab will prepare a single digital parent folder containing all ongoing and completed project work. This folder shall be renamed to the first and last name of the employee leaving BHK lab. The parent folder's contents shall contain: </p> <ol> <li> <p>The Lab Off-boarding Information document listing the following information:</p> <ul> <li> The name and last date of employment of the employee;</li> <li> All ongoing and completed projects that the terminated employee has worked on;</li> <li> Links to the location of all items saved to Github organized by project;</li> <li> Links to the location of all items saved to HPC4Health (H4H) organized by project, with all items saved to H4H adhering to the H4H Data Management Plan;</li> <li> Details about data ownership for each project and name of the lab personnel in charge who takes over your project. All ownership should be transferred to this person-in-charge.</li> </ul> </li> <li> <p>If applicable to individual projects, a single readme file that includes the following:</p> <ul> <li> Data download sources;</li> <li> Comments on data curation or any notable challenges;</li> <li> Any other relevant information.</li> </ul> </li> <li> <p>Any other relevant documents that are not caputred in (1) or (2).</p> </li> </ol> <p>This prepared folder will be shared by the outgoing lab member with the assigned lab staff member, in addition to at least one (1) topic expert (e.g. their mentor/supervisor) involved in each project included in the prepared folder. For short-term interns or volunteers, the parent folder can be shared with respective mentor(s). </p> <p>On receipt of the folder, each recipient shall confirm with the terminated employee the folder\u2019s contents, including clarification for the contents and location of any items not included in the folder. </p> <p>Link this document in your final SOW.</p> <p>Note</p> <p>All files related to your project that are linked in the above documents should be present in your BHKLab Google Drive account and shared with bhklab-admin@googlegroups.com and/or bhklab-members@googlegroups.com with edit access. The same applies for any relevant work documents. This is to ensure uninterrupted access without permission issues later.</p>"},{"location":"onboarding_offboarding/Offboarding/#bhklab-google-drive","title":"BHKLab Google Drive","text":"<p>To ensure continued access to all lab work related documents, we ask that the employee:</p> <ol> <li> <p>Upload all lab work related documents to your BHKLab Google Drive account. This includes any project related documents, presentations, etc. on your local machine. </p> <p>Warning</p> <p>This should not include code or internal datasets. Please store these in the appropriate locations (e.g. GitHub, H4H, etc.).</p> </li> <li> <p>Transfer ownership of all contents of your BHKLab Google Drive to bhklab.archive@gmail.com. This can be done from the top level of the drive by selecting all contents and clicking the share button.</p> </li> </ol> <p>If there are private documents you wish to keep private, please move these to your personal storage. </p>"},{"location":"onboarding_offboarding/Offboarding/#github","title":"GitHub","text":""},{"location":"onboarding_offboarding/Offboarding/#project-repositories","title":"Project Repositories","text":"<ol> <li>Transfer any project repositories from your personal GitHub account to the BHKLab GitHub organization(https://github.com/bhklab).</li> <li>Include a <code>README</code> describing the project and any project documentation explaining how to run the code and/or reproduce the results.</li> </ol>"},{"location":"onboarding_offboarding/Offboarding/#open-pull-requests-branches","title":"Open Pull Requests / Branches","text":"<ol> <li>If you have any open pull requests or working branches on any BHKLab project repositories, push the latest local commit you have. <ol> <li>If the branch does not already have an open pull request, create a new pull request.</li> </ol> </li> <li>Leave a comment on the PR describing what stage the branch is at and what next steps need to be taken.</li> <li>Assign the PR to the repo owner, whoever is taking over the project, or your mentor.</li> </ol>"},{"location":"onboarding_offboarding/Offboarding/#project-data","title":"Project Data","text":"<p>For any project data listed in your Offboarding Report, please ensure that:</p> <ol> <li>ALL data paths are included (e.g. srcdata, rawdata, procdata, should all be listed separately).</li> <li>Indicate for each dataset if it is required to replicate the project. This will help us determine if we can keep the data or if it needs to be removed.</li> <li> <p>Ensure that group access is enabled for all data. Run the following command for the dataset directory:</p> <p><pre><code>chmod -R 775 &lt;DIRECTORY_PATH&gt;\n</code></pre> You can confirm that the data is accessible by running the following command: <pre><code>ls -l \ntotal 2\ndrwxrwxr-x  2 &lt;USERNAME&gt; &lt;USERNAME&gt; 4096 Jan  1 00:00 &lt;DIRECTORY_PATH&gt;\n</code></pre></p> </li> </ol>"},{"location":"onboarding_offboarding/Offboarding/#physical-assets-purchased-by-bhklab","title":"Physical Assets Purchased by BHKLab","text":"<p>Please ensure you return the assets to the lab coordinator in person.</p> <p>If an employee wishes to have possession of any BHK Lab asset purchased with operating funds and/or research funds, they may request from the employer permission to purchase such asset at fair market value.  The employer will determine if it is in the best interests of BHK Lab to sell the asset.</p> <p>If the employer agrees to sell the asset, the employer shall determine fair market value through processes deemed appropriate by the employer. </p>"},{"location":"onboarding_offboarding/Offboarding/#on-your-last-day","title":"On Your Last Day","text":"<p>On your official last day, please ensure that you return your Photo ID card and access card to the PMCRT Security office, located on the first floor of the MARS Discovery District.</p>"},{"location":"onboarding_offboarding/Offboarding/#letters-of-recommendation","title":"Letters of Recommendation","text":"<p>If an employee requires a letter of recommendation, s/he can send an email to Dr. Benjamin Haibe-Kains (benjamin.haibe-kains@uhn.ca) from their non-UHN email with mentors or other topic experts (optional) in cc. If the letter is required in the future, this email chain can be used. Please make sure the purpose of the letter is specified in your email. This is particularly relevant for short term students.</p>"},{"location":"onboarding_offboarding/Offboarding/#consequences-for-non-compliance","title":"Consequences for Non-Compliance","text":"<p>An individual whose employment with BHK Lab ends but who does not return BHK Lab assets in accordance with this policy may be subject to collection agency pursuit and/or legal action. Letters of recommendation or any similar styled letters will not be made out to any employee who fails to return lab assets in compliance with the stated offboarding processes and procedures. </p>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/","title":"Admin Offboarding Instructions","text":"<p>This checklist is for the assigned lab staff running the offboarding process.</p>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#before-the-offboarding-initialization-meeting","title":"Before the Offboarding Initialization Meeting","text":"<ol> <li> <p>Create a folder in the bhklab.archive drive under Offboarding Reports with the name of the lab member.</p> <ol> <li>Confirm this folder is owned by bhklab.archive@gmail.com. Transfer ownership if necessary.</li> <li>Share this folder to the lab member's BHKLab Gmail (e.g. bhklab.johndoe@gmail.com).</li> </ol> </li> <li> <p>Create a folder in the bhklab.archive drive under Personal Folders with the name of the lab member.</p> <ol> <li>All contents of the lab member's BHKLab Google Drive will be transferred to this folder.</li> </ol> </li> </ol>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#offboarding-initialization-email-template","title":"Offboarding Initialization Email Template","text":"<p>Send the following email to the lab member to schedule the two meetings: Email Template</p> <p>CC the lab member's mentor/supervisor on this email. They are not required to attend the meetings, but should be aware of the offboarding process, including their requirement to review the Lab Off-boarding Information document. This review is mandatory to ensure the work is backed up properly and with the correct access permissions.</p> <p>The email must includes links to:</p> <ul> <li>The Offboarding Policy</li> <li>The Lab Off-boarding Information template</li> <li>The lab member's Offboarding Report folder</li> <li>The Exit Interview Form</li> </ul>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#during-the-offboarding-initialization-meeting","title":"During the Offboarding Initialization Meeting","text":"<ol> <li>Walk through the Offboarding Policy with the lab member.</li> <li>Make sure they have access to their Offboarding Report folder and Information template.</li> <li>Remind the lab member of the Exit Interview Form sent in the initial email.<ol> <li>Explain that the Exit Interview form should be completed just prior to the exit interview.</li> </ol> </li> <li>Request that the lab member change the password to their BHKLab Gmail account and share this with the lab coordinator.</li> </ol>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#during-the-exit-interview","title":"During the Exit Interview","text":""},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#uhn-account","title":"UHN Account","text":"<p>The Office coordinator (Soleil Miron) has to make sure that the employee\u2019s UHN email/inbox is inactivated. </p>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#off-boarding-information-document","title":"Off-boarding Information Document","text":"<p>Confirm this has been filled out thoroughly and reviewed by the lab member's mentor/supervisor.</p>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#lab-service-access","title":"Lab Service Access","text":"<p>Make sure that access to lab resources are inactivated.</p> <p>Note</p> <p>Check the Exit Interview form to see if any of the following lab resources have been requested to be kept active for the lab member.</p> <p>Any requested services will be kept active for </p> <ul> <li>Six months after the exit interview OR </li> <li>If working on a manuscript, when that manuscript is submitted</li> </ul> <p>Service list:</p> <ul> <li> Slack</li> <li> GitHub</li> <li> Google Groups</li> <li> H4H</li> <li> GPU Server</li> <li> Cloud services (GCP, AWS, Azure)</li> <li> Miro</li> <li> Paperpile</li> </ul>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#return-physical-id-cards","title":"Return Physical ID Cards","text":"<p>Remind the lab member to return their ID cards to the PMCRT Security office, located on the first floor of the MARS Discovery District.</p>"},{"location":"onboarding_offboarding/Offboarding/admin_checklist/#after-the-exit-interview","title":"After the Exit Interview","text":"<p>To be done by assigned lab staff in charge of offboarding:</p> <ol> <li>Update member status in the BHKLab Member Tracking sheet to 'Alumni'<ol> <li>Update Offboarding Data table in 'Transit Member Data' with Resource Access Expiry date and what tools have been deactivated.</li> </ol> </li> <li>Update member status on the BHKLab Website</li> <li>Remove the member from the Lab Member Expertise page</li> <li>In bhklab.archive@gmail.com Drive, accept ownership of the lab member's BHKLab Google Drive contents and put them in their Personal Folders.</li> <li>Email Ben, Sisira, and Soleil (admin team) when off-boarding is complete and lab member has completed their last working day.</li> </ol>"},{"location":"onboarding_offboarding/Onboarding/","title":"Onboarding","text":"<p>Congratulations on your successful interview and welcome to BHK Lab! </p> <p>We\u2019re excited to have you join the team. Before you begin, there are a few important items that we wanted you to be aware of. This document will provide you with an overview of BHK Lab\u2019s processes, procedures, and our standards for maintaining digital files. </p>"},{"location":"onboarding_offboarding/Onboarding/#lab-overview","title":"Lab Overview","text":"<p>Review the Lab Mission Statement to get a sense of what we do and what we are trying to accomplish.</p>"},{"location":"onboarding_offboarding/Onboarding/#uhn-onboarding-package","title":"UHN Onboarding Package","text":"<p>You will be contacted by BHK Lab\u2019s Office Coordinator, Soleil Miron, via email with the instructions for completing your UHN onboarding. Your UHN onboarding must be completed before your start date. The instructions for completing the UHN onboarding will be provided to you in Soleil\u2019s email. Any changes to your work schedule must be approved by Dr. Haibe-Kains and reported by email to Soleil for payroll purposes. Please look for a biweekly Payroll Reporting reminder email from Soleil. </p> <p>For further instruction on how to complete your UHN onboarding, please see the UHN Onboarding page.</p>"},{"location":"onboarding_offboarding/Onboarding/#bhklab-onboarding-package","title":"BHKLab Onboarding Package","text":"<p>Once your UHN onboarding is complete, new employees can begin the BHKLab specific onboarding process. Steps are outlined on the BHKLab Onboarding page.</p>"},{"location":"onboarding_offboarding/Onboarding/#meetings-to-attend","title":"Meetings to Attend","text":"<p>See the Meetings page for an overview of mandatory meetings to attend.</p>"},{"location":"onboarding_offboarding/Onboarding/#mbp-rotation-students","title":"MBP Rotation Students","text":"<p>If you are an MBP student completing a rotation in the BHKLab, please see the MBP Rotation Students page.</p>"},{"location":"onboarding_offboarding/Onboarding/#best-practices-and-templates","title":"Best Practices and Templates","text":"<p>BHK Lab has developed a series of data storage guidelines to ensure that files can be found with ease by all members of the lab. All members, including new employees, are expected to maintain files in agreement with the current lab standards. You can find these around the BHKLab Handbook. We'll highlight a few below:</p> <ul> <li>Summary of Work Instructions</li> <li>Lab Meeting best practices</li> <li>Journal club best practices</li> <li>HPC4Health help</li> <li>R Coding</li> <li>Scientific Software Best Practices</li> </ul>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/","title":"Admin Onboarding Instructions","text":"<p>These instructions are for the assigned lab staff running the offboarding process.</p> Access Requirements <p>To complete these instructions, you will need </p> <ul> <li>Access to the <code>bhklab.research@gmail.com</code> account </li> <li>Your personal BHKLab Gmail account mus be a member of the <code>BHKLab Admin</code> Google Group. If you are not a member of this group, please contact the lab coordinator to be added.</li> <li>An owner in the BHKLAB GitHub organization</li> </ul> Pages to Have Open <ul> <li>BHKLab Member Tracking sheet</li> <li>BHKLab Project Ideas sheet</li> <li>Lab workspace</li> <li>Email Template</li> <li>Google Groups page</li> </ul> <p>BHKLab Onboarding starts after UHN onboarding from Soleil\u2019s end. Hiring is usually project-dependent, hence new hires will have a project plan from the beginning. Once the final decision is made, projects and mentors will be emailed to you along with other details required. </p> <p>Exceptions are rotation students as we will be informed a few days before they join. In this case, the lab-coordinator (Sisira) will find a project and share it with you.</p>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#upon-hiring","title":"Upon Hiring","text":"<p>Once a new lab member has been hired, the following steps should be taken:</p> <ol> <li>Add the new lab member to the Incoming Member Data sheet in the BHKLab Member Tracking sheet.</li> <li>Determine what workspace the new lab member will be assigned to in the Lab workspace document. You may need to create a new tab to rearrange the seating. Consult with the lab coordinator about this.</li> </ol>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#welcome-email","title":"Welcome Email","text":"<p>Upon successful completion of UHN onboarding, Soleil will send an email (Subject: UHN Photo ID and PMCRT access card) to the new employee with details including access card and photo ID pick-up, directions etc. Following this, the lab onboarding by the assigned lab member can commence.</p> <p>Respond to the email with the following template: Email Template</p>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#onboarding-form-completion","title":"Onboarding Form Completion","text":"<p>Once they have completed the onboarding form their information will show up in the BHKLab Member Tracking sheet in the <code>BHKLab_Onboaring_Form_Response</code> tab.</p> <p>You can start setting up their lab access now.</p>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#1-bhklab-member-tracking-sheet","title":"1. BHKLab Member Tracking sheet","text":"<ol> <li>Set their <code>ACTIVE STATUS</code> to <code>Onboarding</code></li> <li>Add their bhklab gmail to the BHKLab Members Google Group</li> <li>Check off the boxes for lab resource access as they have been set up following the instructions below.</li> </ol>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#2-bhklab-slack","title":"2. BHKLab Slack","text":"<p>Invite them to the BHKLab Slack with their provided Slack email address 1. On Slack, click on the <code>BHKLab</code> workspace title. 1. Click <code>Invite people to BHKLab</code> 1. Enter the provided email address 1. Select if this user is a member or guest 1. Click <code>Send Request</code></p> <p>Note</p> <p>Slackbot will send a request in the <code>#admin</code> channel that must be approved by the lab coordinator for the user to be added to the <code>BHKLab</code> workspace.</p>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#3-bhklab-github-organization","title":"3. BHKLAB GitHub organization","text":"<p>Invite them to the BHKLAB GitHub organization with their provided GitHub email address.</p> <p>GitHub Admin Requirement</p> <p>To add users to the BHKLAB GitHub organization, you must be an owner in the BHKLAB GitHub organization. If you are not an owner, check the People tab of the organization to see who to contact to add this user.</p> <ol> <li>Go to the BHKLAB GitHub organization</li> <li>Navigate to the <code>People</code> tab</li> <li>Click the 'Invite member' button</li> <li>Enter the provided email address</li> <li>Click <code>Invite</code></li> <li>The new user will receive an email invitation to join the BHKLAB GitHub organization. They will need to accept the invitation within a week of receiving it.</li> </ol>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#4-optional-h4h-access","title":"4. (Optional) H4H Access","text":"<p>If the new lab member requires access to H4H for their project, they need an account to be created for them and granted access to the lab project directories.</p> <ol> <li>Ask the new member to send an email to Zhibin Liu (zhibin.liu@uhn.ca) and CC Ben, Sisira, and you. Include the following information:<ul> <li>Their UHN email address or TID</li> <li>What directories they need access to: <code>bhklab</code> if project is in PGx and <code>radiomics</code> if project is in Radiomics</li> </ul> </li> <li>Ben will reply to approve the request.</li> <li>Zhibin will reply once the account has been created.</li> </ol>"},{"location":"onboarding_offboarding/Onboarding/admin_checklist/#orientation-meeting","title":"Orientation Meeting","text":"<ul> <li>Go through the BHKLab Onboarding Guide with the new lab member (virtually or in person).</li> <li>Brief about working mode (onsite vs. virtual days)</li> <li>Lab meeting and JC scheduling<ul> <li>Mention Julia is in charge of scheduling</li> </ul> </li> <li>Confirm they can see the Google Calendar</li> <li>Confirm they are in Slack</li> <li>Confirm they accepted the GitHub invite</li> <li>When meeting is complete, get them to set up their Summary of Work document</li> <li>Connect them with their mentors</li> </ul>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/","title":"BHKLab Onboarding Policy","text":""},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#bhklab-onboarding-form","title":"BHKLab Onboarding Form","text":"<p>Once you have completed UHN onboarding, you will be directed to the Lab Coordinator responsible for onboarding. You should have received an email with instructions for completing the BHKLab Onboarding Form. This form will help guide you through the onboarding process. When complete, please notify the Lab Coordinator so they can begin setting up your access to lab resources.</p>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#pmcrt-lab-tour","title":"PMCRT Lab Tour","text":""},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#lab-location","title":"Lab Location","text":"<p>The BHKLab is located in room 11-401 of the Princess Margaret Cancer Research Tower (PMCRT) in the MARS Discovery District.</p>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#directions-to-the-lab","title":"Directions to the lab","text":"<ol> <li> <p>Find the elevators nearest to Mercatto on the MARS ground floor . If you are exiting the PMCRT security office, walk up the hallway ramp, through the frosted glass doors, and go straight across to the elevators.</p> </li> <li> <p>Go to the 11th floor of PMCRT. If the button does not work, tap your security card on the black glass below the floor buttons on the left side of the elevator.</p> </li> <li> <p>On the 11th floor, when you cross the security doors, make a right and go down the hallway. Turn left when you reach the next hallway with a sign that reads \"11-401 Main Laboratory, South\". </p> <p></p> </li> <li> <p>Go through security doors and make a left.</p> </li> </ol>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#lab-coordinator-onboarding","title":"Lab Coordinator Onboarding","text":"<p>On the first day in the lab, all new employees will be scheduled for a research onboarding by the Lab Coordinator. During the research onboarding, you will be provided an office tour of the physical and/or digital workplace. The digital office tour will include an overview of our internal research systems including: </p> <ul> <li>BHKLab Google Account</li> <li>Summary of Work (SOW)</li> <li>BHKLab Calendar, and the dates of lab meetings and journal club</li> <li>BHK Lab Slack</li> <li>BHK Lab GitHub (added by supervisor/mentor as this is project specific)</li> <li>Any additional programs/systems specific to the employee\u2019s work duties</li> </ul>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#in-the-physical-labspace-confirm-that-the-following-items-are-set-up","title":"In the physical labspace, confirm that the following items are set up:","text":"<ul> <li> Employee has picked up their UHN Photo ID card from Toronto General Photo ID office (Department is \u201cPrincess Margaret - Research\u201d)</li> <li> Employee has picked up their PMCRT Access card from the PMCRT security office</li> <li> Assigned to a workspace in the lab</li> <li> Ensure the employee has access to a computer/monitor, keyboard, and mouse if available in the Lab Equipment Inventory</li> </ul>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#digitally-confirm-that-the-following-items-are-set-up","title":"Digitally, confirm that the following items are set up:","text":"<ul> <li> Creation of a BHKLab Google account (long-term employees only)</li> <li> Addition of BHKLab Gmail to the \"BHKLab Members\" Google Group - this will be used to grant access to BHKLab Google Drive documents and lab calendar</li> <li> Invitation to BHKLab Slack</li> <li> Invitation to BHKLab GitHub Team</li> <li> Addition of prefered email address to the \"BHKLab\" Google Group - this will be used for lab-wide communication by Soleil</li> <li> Any other resources deemed necessary</li> </ul>"},{"location":"onboarding_offboarding/Onboarding/bhklab_onboarding/#lab-mentorsupervisor-onboarding","title":"Lab Mentor/Supervisor Onboarding","text":"<p>The employees will then be directed to their supervisor/mentor to discuss your project and any other resources you may need. </p> <p>These may include:</p> <ul> <li> Set up for HPC4Health access (if applicable)</li> <li> Any project specific resources deemed necessary</li> </ul>"},{"location":"onboarding_offboarding/Onboarding/lab_member_expertise/","title":"Lab Member Expertise","text":"<p>This document outlines various expertise of lab members. Helpful for new recruits, writing personnel skills for funding, and clear communication of individual roles.</p>"},{"location":"onboarding_offboarding/Onboarding/lab_member_expertise/#expertise-table","title":"Expertise Table","text":"Expertise Names Statistical modeling &amp; analysis Farnoosh Abbas Aghababazadeh Biomarker analysis Farnoosh Abbas Aghababazadeh, Julia Nguyen, Sisira Kadambat Nair, Nikta Feizi, Nasim BondarSahebi RNAseq processing Julia Nguyen, Nasim BondarSahebi Microarray processing Sisira Kadambat Nair ATACseq processing Julia Nguyen circRNA analysis Julia Nguyen, Peter Her Exploratory Data Analysis in R Julia Nguyen, Farnoosh Abbas Aghababazadeh, Kevin Wang, Sisira Kadambat Nair, Nikta Feizi, Michael Tran, Yash Patel, Nasim BondarSahebi Single cell processing TBA Single cell analysis TBA Quantitative imaging Caryn Geady, Katy Scott, Ruiyan Ni, Sejin Kim ML models Katy Scott, Caryn Geady, Ruiyan Ni, James Bannon, Farnoosh Abbas Aghababazadeh, Nabin Bagale, Shaghayegh Reza, Kewei Ni ML analysis Katy Scott, Caryn Geady, James Bannon, Ruiyan Ni, Sejin Kim, Nabin Bagale, Shaghayegh Reza, Kewei Ni Deep learning James Bannon, Katy Scott, Sejin Kim, Ruiyan Ni, Joshua Siraj, Nabin Bagale, Shaghayegh Reza Cancer subtype analysis Foram Vyas, Nikta Feizi R packages Jermiah Joseph Clinical oncology Kevin Wang Full stack web development Matthew Boccalon Immunotherapy Farnoosh Abbas Aghababazadeh, Kevin Wang Python packages Jermiah Joseph, Katy Scott, Sejin Kim Cloud services (GCP) Jermiah Joseph, Matthew Boccalon, Nabin Bagale, Shaghayegh Reza Grant &amp; Award proposals Sisira Kadambat Nair LabOps Sisira Kadambat Nair, Katy Scott Hiring Sisira Kadambat Nair Payroll, grant budgets, meeting room scheduling, purchases Soleil Miron Pipelines (Snakemake) Jermiah Joseph High Performance Computing (HPC) on H4H Jermiah Joseph Version Control (Git &amp; GitHub) Jermiah Joseph"},{"location":"onboarding_offboarding/Onboarding/mbp_rotation/","title":"MBP Rotation Students","text":"<p>If you are a student from the University of Toronto Medical Biophysics program completing a rotation in the BHKLab, there are a few more steps to make your rotation experience as smooth as possible.</p>"},{"location":"onboarding_offboarding/Onboarding/mbp_rotation/#your-project","title":"Your Project","text":"<p>Work with your assigned lab mentor to determine what project you will be working on and what the expectations are for your time in the lab.</p> <p>Focus on elements such as:</p> <ul> <li>Is there existing literature to review?</li> <li>What datasets will you be working with?</li> <li>Are you writing code from scratch or expanding an existing code base?</li> <li>What are the deliverables for your project? Code, documentation, analysis results, etc.</li> </ul> <p>We know your rotation time will fly by, so make sure to keep that timeline in mind when planning out your work.</p>"},{"location":"onboarding_offboarding/Onboarding/mbp_rotation/#presentation-to-the-lab","title":"Presentation to the Lab","text":"<p>In your final week with the lab, you will be presenting the work you have completed during the weekly lab meeting OR at a time that Ben is available that week. If you are not able to attend the lab meeting, you will need to present your work at a time that Ben is available that week.</p> <p>Warning</p> <p>Schedule your presentation with your lab mentor and the lab coordinator in the first week of your rotation so we can ensure that Ben is available and a room may be booked if the presentation is scheduled outside of lab meeting.  For mentors: if the presentation needs to be scheduled outside of lab meeting, please contact the lab coordinator to ensure that Ben is available and a room may be booked.</p> <p>You can review the lab meeting page for guidance on how to present your work, but the best way to prepare is to attend the lab meetings during your rotation to get a sense of what is expected of you.</p>"},{"location":"onboarding_offboarding/Onboarding/uhn_onboarding/","title":"UHN Onboarding Policy","text":""},{"location":"onboarding_offboarding/Onboarding/vpn/","title":"Configuring UHN VPN","text":""},{"location":"onboarding_offboarding/Onboarding/vpn/#quick-introduction-why-do-we-need-a-vpn","title":"Quick introduction: why do we need a VPN?","text":"<p>A Virtual Private Network (VPN) is essential in the workplace to ensure secure and private access to the company network, especially when working remotely or on public Wi-Fi. </p> <p>A VPN encrypts internet traffic, protecting sensitive data from potential cyber threats and unauthorized access. This helps maintain confidentiality, ensures data integrity, and supports secure access to internal resources, safeguarding the organization's digital environment.</p> <p>GlobalProtect is UHN's VPN service for workers. GlobalProtect provides secure, encrypted access to the corporate network, ensuring users can safely connect to company resources from any location, while maintaining data security and compliance with organizational policies.</p>"},{"location":"onboarding_offboarding/Onboarding/vpn/#steps-for-configuring-vpn-on-personal-devices","title":"Steps for configuring VPN on personal devices","text":"<p>Most likely you will need to configure the VPN on your personal device. The person onboarding you or your supervisor will probably tell you so and either request VPN access to IT for you or help you to do it.</p> <p>IT will generally accept the request quickly, and send instructions to your corporate email on the steps to follow to install and configure the VPN.</p> <p>If you are a Windows or Mac user, that's it. The manual is pretty good, simple and straightforward. Follow the steps and you've got this. Easy.</p> <p>If you are on Ubuntu... There's no downloading link or steps. You get stuck on the very first step. Don't worry, you don't have to open a ticket on Helpdesk and wait for them to respond... The next section will make things easier for you.</p>"},{"location":"onboarding_offboarding/Onboarding/vpn/#downloading-globalprotect-software-on-ubuntu","title":"Downloading GlobalProtect software on Ubuntu","text":"<p>After contacting Helpdesk a few times, I finally got an email with the downloading instructions for Ubuntu. Here I leave the email, I think it may be useful.</p> <p>Download the installation file from the link or via the curl command: </p> <pre><code>curl https://roseshare.rose-hulman.edu/portal/s/162637781701644125980.tgz --output PanGPLinux-5.3.0-c32.tgz --ciphers 'DEFAULT:!DH'\n</code></pre> <p>Unzip tar file, by running: tar -xvf PanGPLinux-5.3.0-c32.tgz</p> <p>Install the program:</p> <p>On Ubuntu/Debian, this is done through the command:</p> <pre><code>sudo dpkg \u2013i GlobalProtect_deb-5.3.0.0-32.deb\n</code></pre> <p>On Redhat/CentOS, this is done through the command:</p> <pre><code>sudo yum localinstall GlobalProtect_rpm-5.3.0.0-32.rpm\n</code></pre> <p>To start the program, simply enter in a shell</p> <pre><code>globalprotect\n</code></pre> <p>and then a prompt should display.</p> <p>From the prompt, run </p> <pre><code>connect -portal connect2.uhn.ca\n</code></pre> <p>Login with your email address (username@uhn.ca) as your username and password.</p> <p>Type quit to exit the prompt.</p>"},{"location":"onboarding_offboarding/Onboarding/vpn/#thats-it-just-remember-connecting-and-disconnecting-the-vpn-every-time-you-need-it","title":"That's it - just remember connecting and disconnecting the VPN every time you need it","text":"<p>Now that you have your laptop configured for the VPN, don't forget connecting every time you want to use it, running the following command:</p> <pre><code>globalprotect\n</code></pre> <p>And then running in the prompt:</p> <pre><code>connect -portal connect2.uhn.ca\n</code></pre> <p>When you want to disconnect from the VPN, you have to run</p> <pre><code>globalprotect\n</code></pre> <p>And then in the prompt:</p> <pre><code>disconnect\n</code></pre>"},{"location":"resources/","title":"Essential Lab Resources: Tools, Guides &amp; References","text":"<p>A comprehensive collection of tools, guides, and references to support your lab work. Access learning resources, explore training programs and award opportunities to enhance your skills and advance your career.</p>"},{"location":"resources/Guide_to_mentors/","title":"Guide for mentors and supervisors","text":"<p>This guide is for:</p> <ul> <li> <p>Postdocs and senior trainees mentoring undergraduate, Master's, or PhD students</p> </li> <li> <p>Lab staff coordinating with interns or junior hires</p> </li> <li> <p>Anyone supervising a research trainee or contributor in the lab</p> </li> </ul>"},{"location":"resources/Guide_to_mentors/#core-responsibilities-of-a-mentor","title":"Core Responsibilities of a Mentor","text":"<ul> <li> <p>Your mentee will have finished lab onboarding as per lab onboarding. Help them get oriented with research questions, project goals, specific tools, documentation, and workflows.</p> </li> <li> </li> </ul> <p>Internship Project Template</p> <p>Here's a template we highly recommend to adapt to your projects - (Year) Summer Internship (Project name)-(Student name ) TEMPLATE DO NOT EDIT</p> <ul> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"resources/Guide_to_mentors/#set-expectations-early","title":"Set expectations early","text":"<p>First week - Discuss timelines, communication preferences, availability, and evaluation criteria.</p>"},{"location":"resources/Guide_to_mentors/#provide-consistent-feedback","title":"Provide consistent feedback","text":"<p>Hold regular 1:1 meetings. Use constructive, kind, and clear communication to help mentees grow.</p>"},{"location":"resources/Guide_to_mentors/#document-and-share-progress","title":"Document and share progress","text":"<p>Encourage your mentee to keep a record of work using the tracker. Code should be reviewed via Pull requests on GitHub.</p>"},{"location":"resources/Guide_to_mentors/#advocate-and-support","title":"Advocate and support","text":"<p>Encourage them to present, apply for awards, or explore professional development opportunities.</p>"},{"location":"resources/Undergraduate_award_opportunities/","title":"Trainee funding opportunities","text":""},{"location":"resources/Undergraduate_award_opportunities/#undergraduate-award-websites-for-summer-fall-and-winter-internships","title":"Undergraduate award websites for Summer, Fall and Winter internships","text":"<ol> <li> <p>U of T Medical Biophysics Summer Student Program</p> </li> <li> <p>Natural Sciences and Engineering Research Council of Canada NSERC USRA Undergraduate Student Research Awards</p> </li> <li> <p>MITACS Discover Programs for Students and Postdocs</p> </li> <li> <p>Amgen Scholars Program at the University of Toronto Amgens Scholars Program</p> </li> <li> <p>Summer Undergraduate Data Science (SUDS) Opportunities Program SUDS</p> </li> <li> <p>The Temerty Centre for Artificial Intelligence Research and Education in Medicine (T-CAIREM) at the University of Toronto TCAIREM</p> </li> </ol>"},{"location":"resources/Undergraduate_award_opportunities/#graduate-students-and-post-docs","title":"Graduate students and Post docs","text":"<p>The ORT has compiled a list of awards, conference grants and fellowship opportunities for graduate students and postdoctoral researchers. ORT link</p> <p>Vector Institute An entrance award for top students pursuing AI master\u2019s in Ontario</p>"},{"location":"resources/literature_review/","title":"Literature Review Tools","text":"<p>This section is intended as a collection of useful tools and tips to help with conducting literature reviews. Visit the subpages for more information about specific resources.</p> <p>Often literature review is important for starting up new projects, especially for understanding current and previous work done in the field. It is also great for discovering ideas, tools, frameworks, and datasets that can be used in your own work. Near completion of your project, literature review is also useful for drawing citations for your manuscript.</p>"},{"location":"resources/literature_review/covidence/","title":"Covidence","text":"<p>Covidence is an online tool designed to simplify and streamline the systematic review process. It is particularly helpful for managing and screening large numbers of studies in a collaborative, step-by-step workflow.</p>"},{"location":"resources/literature_review/covidence/#overview","title":"Overview","text":"<p>Covidence enables researchers and teams to:</p> <ul> <li>Import large sets of references from literature searches (e.g., PubMed keyword or MeSH queries).</li> <li>Automatically fetch abstracts and full-text PDFs when available.</li> <li>Conduct reviews in structured phases: abstract screening, full-text review, and data extraction.</li> </ul>"},{"location":"resources/literature_review/covidence/#workflow","title":"Workflow","text":""},{"location":"resources/literature_review/covidence/#0-setup","title":"0. Setup","text":"<ul> <li>To use Covidence, you should first prepare a list of studies you want to include in your systematic review.</li> <li>You can either manually add a list of studies you want to review, or bulk-add a list from a query.</li> <li>One way to collect many studies programmatically is to use the PubMed API and export the results in a format that Covidence can import such as EndNote XML.</li> <li>Some example code of doing so using Biopython and MeSH queries is seen in PredictRx (Note: You must be logged into GitHub and have access to BHKLAB organization).</li> </ul>"},{"location":"resources/literature_review/covidence/#1-abstract-screening","title":"1. Abstract Screening","text":"<ul> <li>Covidence first presents all study abstracts for initial screening.</li> <li>Reviewers can approve or exclude papers, with the option to annotate reasons for exclusion.</li> <li>Highlight terms of interest in abstracts to help with decision-making.</li> <li>Reviews can be configured to require single or multiple reviewers per paper.</li> </ul>"},{"location":"resources/literature_review/covidence/#2-full-text-review","title":"2. Full-Text Review","text":"<ul> <li>For studies that pass abstract screening, the full text can be reviewed.</li> <li>Covidence attempts to retrieve the full PDFs automatically.</li> <li>Reviewers again decide whether to include or exclude, with optional comments or rationale.</li> </ul>"},{"location":"resources/literature_review/covidence/#3-data-extraction-export","title":"3. Data Extraction / Export","text":"<ul> <li>Once the final set of studies is selected, users can extract relevant data within Covidence.</li> <li>Alternatively, the finalized list can be exported for use in other tools or reporting.</li> <li>Covidence also saves the review history, which is helpful in producing a consort diagram to show how many studies were included from the beginning, how many were excluded at each stage, and the reasons for exclusion.</li> </ul>"},{"location":"resources/literature_review/covidence/#access","title":"Access","text":"<ul> <li>You can access one free systematic review with a UofT email address.</li> <li>Only one person needs to create the review; other team members (with free accounts) can be added as collaborators.</li> <li>If you need to restart or delete a review, contact Covidence support and they will handle the deletion on your behalf.</li> </ul> <p>For more details, see their official site: https://www.covidence.org/.</p>"},{"location":"resources/literature_review/paperpile/","title":"Paperpile","text":"<p>Paperpile is the lab's citation manager of choice. It has seamless integration with Google Apps and Google Drive, which is the lab's primary document management system. Paperpile simplifies managing references, PDFs, and citations directly within the browser.</p>"},{"location":"resources/literature_review/paperpile/#getting-started","title":"Getting Started","text":"<p>The BHK Lab has a paid Paperpile license. If you need access credentials, you can reach out to the Lab Coordinator via Slack or Email.</p> <p>Required</p> <p>When collecting citations for your project, you must create a subdirectory within the Paperpile library with the name of your project to help organize citations.</p>"},{"location":"resources/literature_review/paperpile/#using-paperpile","title":"Using Paperpile","text":""},{"location":"resources/literature_review/paperpile/#extension","title":"Extension","text":"<ul> <li>Install the Paperpile Chrome extension to streamline your citation workflow.</li> <li>As of writing, there is no support for Firefox or Safari, but you can use the web app in any browser.</li> </ul>"},{"location":"resources/literature_review/paperpile/#basic-workflow","title":"Basic Workflow","text":"<ul> <li>Save articles easily: When viewing articles online, click the Paperpile icon in the browser toolbar to add references directly to your library.</li> <li>Manage PDFs: Paperpile automatically organizes PDFs and integrates seamlessly with Google Drive, ensuring your references and files are synced and accessible.</li> <li>Cite while writing: Quickly insert formatted citations and bibliographies into your Google Docs manuscripts using the integrated citation tool.</li> </ul>"},{"location":"resources/literature_review/paperpile/#additional-resources","title":"Additional Resources","text":"<ul> <li>Paperpile help center.</li> </ul>"},{"location":"software_development/","title":"Software Development","text":"<p>All things related to software development.</p>"},{"location":"software_development/Development_Environment/","title":"Development Environment","text":"<p>Think of the development environment as EVERYTHING that you need to develop your project.</p> <p>This includes:</p> <ul> <li> <p>What Shell are you using?</p> <ul> <li><code>Bash</code></li> <li><code>PowerShell</code> for most Windows users</li> <li><code>Windows Subsystem for Linux</code> (WSL)</li> <li><code>Zsh</code> (commonly installed for macOS users) but can be used on Linux!</li> </ul> </li> <li> <p>Integrated Development Environments (IDEs)</p> <ul> <li>VS Code</li> <li>PyCharm</li> <li>Jupyter Notebook</li> <li>etc.</li> </ul> </li> <li>Package Managers<ul> <li>Conda/Mamba</li> <li>Pixi</li> <li>Pip</li> <li>npm</li> </ul> </li> <li>Containerization<ul> <li>Docker</li> <li>Singularity</li> </ul> </li> <li>Version Control</li> <li>etc.</li> </ul>"},{"location":"software_development/Development_Environment/CLI_Tools/","title":"Command Line Interface (CLI) Tools","text":""},{"location":"software_development/Development_Environment/Containerization/","title":"Containerization","text":""},{"location":"software_development/Development_Environment/Integrated_Development_Environments/","title":"Integrated Development Environments (IDEs)","text":""},{"location":"software_development/Development_Environment/Integrated_Development_Environments/r_studio/","title":"RStudio","text":"<p>RStudio is an integrated development environment (IDE) for R and Python. It includes a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging, and workspace management. RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux). (Source)</p> <p>Download RStudio</p> <p>User Guides</p>"},{"location":"software_development/Development_Environment/Integrated_Development_Environments/visual_studio_code/","title":"Visual Studio Code","text":"<p>Download Visual Studio Code</p> <p>Visual Studio Code documentation</p>"},{"location":"software_development/Development_Environment/Package_Managers/","title":"Package Managers","text":""},{"location":"software_development/Development_Environment/Package_Managers/pixi/","title":"Pixi Package Manager","text":"TL;DR <p>Pixi is a modern, performant package manager designed as an alternative to Conda, Mamba, and even PyPI. It simplifies dependency management across operating systems, making project reproducibility easier across systems.</p>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#what-is-pixi","title":"What is Pixi?","text":"<p>Pixi is a package manager built to improve upon and provide an alternative to Conda and Mamba (or environments in general). Specifically, Pixi aids projects that require managing complex dependencies and ensuring reproducibility across different operating systems.</p> <p>Pixi excels in ease of use, speed, and robustness, positioning itself as a compelling replacement for Conda when managing project-level dependencies, especially in multi-language (e.g., Python and R) data science projects.</p> <p>Python-only projects</p> <p>For projects focused exclusively on Python, you might also consider using <code>uv</code> instead, a modern alternative to <code>pip</code> and/or <code>pyenv</code>.</p> <p>For extra context on why the developers built Pixi in the first place, see their Vision page.</p>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#why-consider-pixi","title":"Why Consider Pixi?","text":""},{"location":"software_development/Development_Environment/Package_Managers/pixi/#reproducibility-and-cross-platform-compatibility","title":"Reproducibility and Cross-platform Compatibility","text":"<p>Pixi excels in dependency management and reproducibility, particularly across different operating systems. With Pixi, sharing your environment setup and ensuring your project runs identically on other machines is straightforward:</p> <ul> <li>Simply include a <code>pixi.toml</code> file in your project repository.</li> <li>Another user runs <code>pixi install</code> and instantly replicates your environment.</li> </ul>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#tasks","title":"Tasks","text":"<p>Instead of describing ways to run pipelines or commands to do repeated actions, you can define tasks in the <code>pixi.toml</code> file. This way, instead of running long commands with convoluted arguments and flags, you can alias it with a simple name. You can find more details on tasks on their task documentation.</p>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#enhanced-performance-and-usability","title":"Enhanced Performance and Usability","text":"<p>Pixi enhances the performance and usability experience compared to traditional tools like Conda or Anaconda, providing:</p> <ul> <li>Faster dependency resolution</li> <li>Simplified, intuitive command-line interface (CLI)</li> <li>Robust management of dependencies for multiple languages within a single environment</li> </ul>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#getting-started","title":"Getting Started","text":"<p>Installation instructions can be found directly on the Pixi installation guide.</p> <p>Once installed, a great starting point and introduction can be found on their dedicated Getting Started page.</p> <p>Here's a cheatsheet on how you might use Pixi for basic project management:</p> Command Description <code>pixi init</code> Start a pixi project in the current working dir <code>pixi install</code> Install all dependencies from <code>pixi.toml</code> <code>pixi add &lt;pkg&gt;</code> Add a new dependency and update environment <code>pixi run &lt;cmd&gt;</code> Run a command (or task) within Pixi environment <code>pixi shell</code> Activate a shell with Pixi-managed environment"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#example-usage","title":"Example Usage","text":""},{"location":"software_development/Development_Environment/Package_Managers/pixi/#working-in-python-with-pixi","title":"Working in Python with Pixi","text":"<p>Instead of pip installing Python packages globally, or using a Conda environment, you can define per-project dependencies like so:</p> <pre><code>pixi add python@3.10 # specify an exact package version!\npixi add pandas\n\n# or add multiple at once:\npixi add python@3.10 pandas\n</code></pre>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#working-in-r-with-pixi","title":"Working in R with Pixi","text":"<p>Because Pixi utilizes the Conda package ecosystem, packages you can install with Conda are also available in Pixi. This includes R packages, which can be installed easily:</p> <pre><code>pixi add r-base\npixi add r-ggplot2\npixi add bioconductor-limma -c bioconda\n</code></pre> <p>Installing R and Bioconductor Packages</p> <p>R packages are often prepended with <code>r-</code> in the package name, and depend on the base R installation <code>r-base</code>. Bioconductor packages are also available if you add <code>bioconda</code> as a channel, and are often prepended with <code>bioconductor-</code>. The easiest way to find the exact package names is to search for them on the Anaconda Cloud.</p> Bioconductor packages on ARM-based Macs <p>Some Bioconductor packages have not yet been built for ARM-based Macs (Apple Silicon) and thus might raise an error:</p> <p></p> <p>If you encounter issues, you can try falling back to osx-64 which uses rosetta to run x86_64 binaries on ARM-based Macs:</p> <pre><code>[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"my_project\"\n- platforms = [\"osx-arm64\"]\n+ platforms = [\"osx-64\"]\n</code></pre>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#vs-code-integration","title":"VS Code Integration","text":"<p>You can install the VS Code extension to help manage Pixi environments within the IDE. You can find it on the VS Code Marketplace:</p> <ul> <li>Pixi VS Code Extension</li> </ul>"},{"location":"software_development/Development_Environment/Package_Managers/pixi/#additional-resources","title":"Additional Resources","text":"<ul> <li>Pixi Documentation</li> <li>GitHub Repository</li> </ul>"},{"location":"software_development/Development_Environment/Shells/","title":"Shells","text":""},{"location":"software_development/Development_Environment/Shells/#what-is-a-shell","title":"What is a Shell?","text":"<p>A shell is a user interface that provides access to the operating system's services. Typically, it functions as a command-line interface (CLI), where users can enter commands to perform tasks such as navigating directories, executing programs, and automating processes with scripts.</p>"},{"location":"software_development/Development_Environment/Shells/#types-of-shells","title":"Types of Shells","text":"<p>There are various shells, each offering unique features and catering to different needs. Some of the most commonly used shells include:</p> Shell Name Description Platforms Bash The Bourne Again Shell, commonly used on Unix-based systems. Unix, Linux, macOS Zsh An enhanced shell with additional features, extending the functionality of the Bourne Shell. Unix, Linux, macOS PowerShell Task automation and configuration management framework by Microsoft, cross-platform support. Windows, Linux, macOS"},{"location":"software_development/Development_Environment/Shells/#tldr","title":"TL;DR","text":"<p>Here's a quick comparison of the most popular shells:</p> Shell Best For Key Strengths Bash Simplicity, basic scripting, Unix environments Widely available, easy to use, ideal for general tasks Zsh Customization, advanced features, plugin ecosystem Highly customizable, rich plugin and theme support, advanced completion PowerShell Cross-platform automation, Windows environments Object-oriented, powerful scripting, deep Windows integration"},{"location":"software_development/Development_Environment/Shells/#comparison-table","title":"Comparison Table","text":"<p>Here's a comparison table that highlights the key differences between Bash, PowerShell, and Zsh:</p> Feature Category Bash PowerShell Zsh Features Simplicity Simple and beginner-friendly - - Portability Widely available on Unix-like systems Cross-platform (Windows, Linux, macOS) - Scripting Basic scripting support for automation Advanced scripting with cmdlets and modules Enhanced scripting over Bash Object-Oriented - Works with objects, unlike other text-based shells - Integration - Deep integration with Windows for administrative tasks - Customizability - - Highly customizable with themes and plugins Completion - - Advanced, programmable completion features Plugins - - Extensive plugin support for extended functionality User Experience Default Shell Default on many Linux distributions and macOS - - Prompt Simple, functional, customizable Customizable prompt with rich information display Highly customizable with frameworks like Oh My Zsh Integration with IDEs - Integrated with Visual Studio Code - Themes and Plugins - - Extensive theme and plugin support Ecosystem and Community Documentation Extensive official documentation and community tutorials Comprehensive official docs by Microsoft Good official docs and community resources Community Large and active community with many resources available Growing community among Windows admins and developers Active community with strong focus on customization Modules/Plugins - Rich set of modules for various administrative tasks Vibrant plugin and theme ecosystem (e.g., Oh My Zsh)"},{"location":"software_development/Development_Environment/Shells/wsl/","title":"Windows Subsystem for Linux (WSL)","text":"<p>Windows Subsystem for Linux (WSL) lets developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a traditional virtual machine or dual-boot setup. (Source)</p> <p>Installation</p>"},{"location":"software_development/Languages/","title":"Languages","text":"<p>Ideas for a breakdown:</p>"},{"location":"software_development/Languages/#programming-languages","title":"Programming Languages","text":"<ul> <li>R</li> <li>Python</li> <li>JavaScript/TypeScript</li> <li>Rust</li> </ul>"},{"location":"software_development/Languages/#scripting-languages","title":"Scripting Languages","text":"<ul> <li>Purpose and Usage</li> <li>Examples<ul> <li>Bash</li> <li>Perl</li> <li>Ruby</li> </ul> </li> </ul>"},{"location":"software_development/Languages/#domain-specific-languages","title":"Domain-Specific Languages","text":"<ul> <li>Definition</li> <li>Examples<ul> <li>SQL</li> <li>HTML</li> </ul> </li> </ul>"},{"location":"software_development/Languages/#compiled-vs-interpreted-languages","title":"Compiled vs. Interpreted Languages","text":"<ul> <li>Definitions</li> <li>Key Differences</li> </ul>"},{"location":"software_development/Languages/#programming-paradigms","title":"Programming Paradigms","text":"<ul> <li>Procedural Programming</li> <li>Object-Oriented Programming</li> <li>Functional Programming</li> </ul>"},{"location":"software_development/Languages/#choosing-a-programming-language","title":"Choosing a Programming Language","text":"<ul> <li>Considerations</li> <li>Language Popularity</li> <li>Community and Support</li> </ul>"},{"location":"software_development/Languages/Javascript_Typescript/","title":"JavaScript/TypeScript","text":""},{"location":"software_development/Languages/Javascript_Typescript/#what-is-javascript","title":"What is JavaScript","text":"<p>JavaScript is primarily known as a high-level (abstracted from complexity) programming language, most notably used in creating dynamic front-end web content. Due to it's versatility it has also been adopted to create APIs and run server side operations in full stack applications. </p>"},{"location":"software_development/Languages/Javascript_Typescript/#why-we-use-javascript","title":"Why We Use JavaScript","text":"<p>JavaScript is the most common choice for the front-end and backend in full stack applications in the lab because of its ease of use, portability, improving performance, and how common it is in the industry. When learning web development you typically will end up using vanilla JavaScript or JavaScript libraries such as React, Vue, or Angular to start, making it a safe option for newcomers and seasoned developers.</p>"},{"location":"software_development/Languages/Javascript_Typescript/#how-to-run-javascript","title":"How to run JavaScript","text":"<p>For the front-end/client portions of JavaScript applications, there are no additional engines/compilers needed to execute code because browsers have built-in JavaScript engines (utilizing JIT) that will execute the code for you. However, when using JavaScript code server side (code execution by your machine) you will need a Node.js runtime environment to be installed on your computer because operating systems don't include them natively. The best way to install Node.js on your system and manage versions is using nvm (Node Version Manager). Every project in the lab was likely developed with a different version of node, so making sure you are utilizing the ones compatible with the code/packages is important to ensure expected functionality.</p>"},{"location":"software_development/Languages/Javascript_Typescript/#what-is-typescript","title":"What is TypeScript","text":"<p>TypeScript is simply a superset language of JavaScript that provides additional functionality such as static typing, interfaces/types, and enums to name a few. The additional functionalities of TypeScript help you avoid errors before they happen (saves you debugging time), makes your code more predictable, and help others understand it easier. The best part about TypeScript is the fact that all JavaScript code is also valid TypeScript code (because TypeScript is a superset of JavaScript). This makes adding TypeScript to your arsenal easier for your next project and also makes incrementally adopting TypeScript into already existing JavaScript projects quite seamless.</p>"},{"location":"software_development/Languages/Javascript_Typescript/learning_resources/","title":"Learning Resources","text":"<p>For ReactJS: Official docs</p> <p>For JavaScript: MDN Guide for JS</p> <p>For NodeJS Express: Official Documentation</p> <p>Our NodeJS + React web app architecture: Tutorial to create a React web app with NodeJS Express API</p> <p>For Flask: Official Documentation</p> <p>MongoDB + Mongoose: Introduction to MongoDB and Mongoose</p> <p>For Knex.js (a SQL query builder) : Knex.js Documentation </p> <p>For GraphQL: Official Documentation</p> <p>D3.js (JavaScript Graph Gallery): The D3 Graph Gallery</p> <p>Plotly (JavaScript data visualization library): Official documentation and examples</p> <p>JS Docs: Official page</p>"},{"location":"software_development/Languages/Markdown/","title":"Markdown","text":"<p>Markdown is extensively used in all kinds of software development projects for creating documentation, blog posts, and other content.</p>"},{"location":"software_development/Languages/Markdown/#why-markdown","title":"Why Markdown?","text":"<p>Markdown is a simple and easy-to-use markup language that only takes 10 minutes to learn the basics of.</p> <p>Its syntax is simple and easy to read, making it a great choice for writing documentation.</p>"},{"location":"software_development/Languages/Markdown/#markdown-tutorials","title":"Markdown Tutorials","text":"<p>Visit this 10 minute Markdown tutorial to learn the basics of Markdown.</p> <p>The Markdown Basic Syntax page is a great reference when you're getting started with Markdown.</p> <p>Keeping the Markdown Guide handy is also recommended, as it can provide quick answers to common questions.</p>"},{"location":"software_development/Languages/Markdown/mkdocs/","title":"MkDocs","text":"<p>MkDocs is a static site generator that uses Markdown files (and many other formats) to create a website.</p> <p>This handbook is built using MkDocs and Material for MkDocs. These tools also have extensive documentation and guides for contributing to a Mkdocs project.</p> <p>Note</p> <p>For a basic tutorial on Markdown, see the Markdown page.</p> <p>Funamentally, any standard Markdown file is valid for MkDocs, however, there are some additional features that can be used to enhance the look and feel of your documentation.</p>"},{"location":"software_development/Languages/Markdown/mkdocs/#material-for-mkdocs","title":"Material for MkDocs","text":"<p>Material for MkDocs is a theme for MkDocs that provides a modern and customizable look and feel.</p> <p>It provides an easy-to-use interface for creating documentation.</p> <p>Visit the Material for MkDocs Formatting page for more information on adding extra flavor to your Markdown files.</p>"},{"location":"software_development/Languages/Markdown/mkdocs/examples/","title":"Tabbed Code/Content Blocks","text":"<p>The <code>tabbed</code> extension allows you to create tabbed code/content blocks.</p> <p>Note</p> <p>This extension is already enabled, you just have to format your markdown to use it.</p> <p>See reference here.</p> Tabbed Code BlocksTabbed Content <p>Click on either <code>C</code> or <code>C++</code> to view the code specific to that language.</p> CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <p>The markdown for the above code blocks: <pre><code>=== \"C\"\n\n    ``` c\n    #include &lt;stdio.h&gt;\n\n    int main(void) {\n      printf(\"Hello world!\\n\");\n      return 0;\n    }\n    ```\n\n=== \"C++\"\n\n    ``` c++\n    #include &lt;iostream&gt;\n\n    int main(void) {\n      std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n      return 0;\n    }\n    ```\n</code></pre></p> <p>Click on either <code>Unordered list</code> or <code>Ordered list</code> to view the content specific to that list type.</p> Unordered listOrdered list <ul> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ul> <ol> <li>Sed sagittis eleifend rutrum</li> <li>Donec vitae suscipit est</li> <li>Nulla tempor lobortis orci</li> </ol> <p>The markdown for the above content blocks:</p> <pre><code>=== \"Unordered list\"\n\n    * Sed sagittis eleifend rutrum\n    * Donec vitae suscipit est\n    * Nulla tempor lobortis orci\n\n=== \"Ordered list\"\n\n    1. Sed sagittis eleifend rutrum\n    2. Donec vitae suscipit est\n    3. Nulla tempor lobortis orci\n</code></pre>"},{"location":"software_development/Languages/Python/","title":"Python","text":""},{"location":"software_development/Languages/R/","title":"R","text":""},{"location":"software_development/Languages/R/#what-is-r","title":"What is R?","text":"<p>R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.</p>"},{"location":"software_development/Languages/R/#basic-resources","title":"Basic Resources","text":"<p>Deep Dive into R Advanced R from Chapman &amp; Hall\u2019s R Series. \"R users who want to improve their programming skills and understanding of the language. It should also be useful for programmers coming to R from other languages, as help you to understand why R works the way it does.\"</p> <p>Holy Grail of Building R-Packages \u201cR Packages\u201d by Hadley Wickham and Jennifer Bryan Very in-depth, discusses all the fundamentals of building a package and introduces useful tools (devtools, usethis)</p> <p>Brief Overview of building R-packages Mastering Software Development in R: Chapter 3 Building R Packages Part of a book on software development in R (highly recommend the entire book to anyone working in R)</p> <p>Bioconductor Developer Book Bioconductor Packages: Development, Maintenance, and Peer Review If you are looking to build a package for Bioconductor, I heavily recommend going through these guidelines before starting as they have a rigorous approval process</p> <p>Highlighted chapters: Coding Style</p> <p>Guide to data.tables data.table in R  supplemented with External Guide to data.table and 101 R data.table exercises</p>"},{"location":"software_development/Languages/R/#advanced-resources","title":"Advanced Resources","text":"<p>R-Polars for efficiency and speed</p> <p>Tidyverse: R Packages for Data Science supplemented with R for Data Science Book </p>"},{"location":"software_development/Languages/R/MultiAssayExperiment/","title":"MultiAssayExperiment","text":""},{"location":"software_development/Languages/R/MultiAssayExperiment/#multiassayexperiment","title":"MultiAssayExperiment","text":"<p>MultiAssayExperiment (MAE) is an R package used to harmonize data management of multiple experimental assays performed on an overlapping set of specimens. It is designed to manage and integrate multiple types of omics or experimental data (e.g., RNA-seq, mutation data, methylation, proteomics) into a single, structured object. The object stores different data modalities (assays) together, keep track of which samples have which data types, with a goal to facilitate joint analysis, visualization, and subsetting across multiple experiments.</p> <p>You can create a MultiAssayExperiment object if you have a multiomics dataset (RNA-seq, mutation, proteomics data) for the same or overlapping sets of samples, and if you want to apply multi-omics integration methods, ML models, or visualizations that require synchronized datasets.</p> <p>\"It provides a familiar Bioconductor user experience by extending concepts from SummarizedExperiment, supporting an open-ended mix of standard data classes for individual assays, and allowing subsetting by genomic ranges or rownames.  Facilities are provided for reshaping data into wide and long formats for adaptability to graphing and downstream analysis.\"</p> <p>Citation</p> <p>Ramos M, Schiffer L, Re A, Azhar R, Basunia A, Rodriguez Cabrera C, Chan T, Chapman P, Davis S, Gomez-Cabrero D, Culhane A, Haibe-Kains B, Hansen K, Kodali H, Louis M, Mer A, Reister M, Morgan M, Carey V, Waldron L (2017). \u201cSoftware For The Integration Of Multi-Omics Experiments In Bioconductor.\u201d Cancer Research, 77(21), e39-42. doi:10.1158/0008-5472.CAN-17-0344, https://cancerres.aacrjournals.org/content/77/21/e39. </p>"},{"location":"software_development/Languages/R/MultiAssayExperiment/#structure-of-a-mae-object","title":"Structure of a MAE Object","text":"<ul> <li><code>ExperimentList</code>: A list of assays (i.e. SummarizedExperiment, matrix).</li> <li><code>colData</code>: Metadata about the primary samples (patients/cell lines), such as age, treatment, outcome.</li> <li><code>sampleMap</code>: A table mapping how each assay's sample IDs relate to the primary colData sample IDs (handles cases where IDs differ across experiments).<pre><code>MultiAssayExperiment\n|-- ExperimentList\n        |-- RNA-seq matrix\n        |-- Mutation calls (binary matrix)\n        |-- Proteomics data\n|-- colData (patient-level metadata)\n|-- sampleMap (which patient \u2192 which sample in each assay)\n</code></pre> </li> </ul>"},{"location":"software_development/Languages/R/MultiAssayExperiment/#installation-requirement","title":"Installation requirement","text":"<p>R (version \"4.5\") </p> <pre><code>    if (!require(\"BiocManager\", quietly = TRUE))\n        install.packages(\"BiocManager\")\n\n    BiocManager::install(\"MultiAssayExperiment\")\n</code></pre>"},{"location":"software_development/Languages/R/MultiAssayExperiment/#build-a-mae-object","title":"Build a MAE Object","text":"<pre><code>    library(MultiAssayExperiment)\n\n    # Suppose you have rna, mutation data and patient information\n    rna &lt;- SummarizedExperiment(assays = list(counts = matrix(rnorm(1000), ncol = 10)))\n    snv &lt;- matrix(sample(0:1, 100, replace = TRUE), ncol = 10)\n    patient_info &lt;- as.data.frame(\n    PatientID = paste0(\"Patient\", 1:10),\n    Age = sample(40:80, 10)\n    )\n\n    # Create the MultiAssayExperiment object with function\n    mae &lt;- MultiAssayExperiment(\n    experiments = ExperimentList(rna = rna, mutations = mutations),\n    colData = patient_info\n    )\n</code></pre>"},{"location":"software_development/Languages/R/MultiAssayExperiment/#work-with-a-mae","title":"Work with a MAE","text":"<p>We will give an example Using Immune Checkpoint Blockade Dataset. Suppose you have downloaded a Immune Checkpoint Blockade Dataset from ORCESTRA</p> <pre><code>    library(MultiAssayExperiment)\n\n    # Load dataset\n    icb_mae &lt;- readRDS(icb_file)\n\n    # Extract clinical data (patient information, treatment, disease etc.)\n    clinical_data &lt;- colData(icb_mae) %&gt;% as.data.frame()\n\n    # view RNA\n    rna_se &lt;- experiments(icb_mae)[[\"expr\"]]\n    rna_sample &lt;- colnames(rna_se) # Sample names inside RNA assay\n    rna_expr &lt;- assay(rna_se) # save the rna expression with sample to a matrix\n\n    # view mutation data\n    snv_se &lt;- experiments(icb_mae)[[\"snv\"]]\n    snv_matrix &lt;- assay(snv_se)\n\n    # Mapping between assays and primary samples\n    sample_map &lt;- sampleMap(icb_mae)\n\n    # View metadata if available, this include any extra information about the experiment (i.e. batch info, project details)\n    meta_data &lt;- metadata(icb_mae)\n</code></pre>"},{"location":"software_development/Languages/R/MultiAssayExperiment/#additional-references","title":"Additional References","text":"<ul> <li>Bioconductor MultiAssayExperiment</li> <li>User reference manual PDF</li> </ul>"},{"location":"software_development/Languages/R/Tidyverse/","title":"Tidyverse","text":"TL;DR <p>The Tidyverse is a suite of R packages that mesh together with a goal of improving common data science pipeline steps, namely data import, tidying, manipulation, visualisation, and programming.</p> <p>Of the eight core packages, some notable ones include:</p> <ul> <li><code>readr</code>, for data import and export</li> <li><code>dplyr</code>, for data manipulation</li> <li><code>ggplot2</code>, for data visualization</li> </ul> <p>The packages also emphasize the use of the pipe, <code>|&gt;</code> or <code>%&gt;%</code>. When you pipe an object forward, it is treated as the first parameter in the next function. For example, the two lines below are equivalent to one another:</p> <p><pre><code>colnames(read.csv(filename, header = TRUE))\n</code></pre> <pre><code>filename |&gt; read.csv(header = TRUE) |&gt; colnames()\n</code></pre></p>"},{"location":"software_development/Languages/R/Tidyverse/#what-is-the-tidyverse","title":"What is the Tidyverse?","text":"<p>From the Tidyverse website:</p> <p>\"The Tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\"</p> <p>The origins of the Tidyverse began from a paper written by Hadley Wickham in 2014, titled \"Tidy Data\", in which he describes the aforementioned design philosophy. (For those interested, you can read the paper here: 10.18637/jss.v059.i10.) More recent documentation of the underlying principles guiding Tidyverse packages can be found in the tidy tools manifesto as well as the Tidy design principles book.</p> <p>For an introduction on the Tidyverse, take a look at their package page.</p> <p>You can see a list of the eight core packages on their site here, and further details within their respective documentation pages. For a quick visual overview of a specific package, you can also find its respective cheatsheet on this page.</p>"},{"location":"software_development/Languages/R/Tidyverse/#why-use-the-tidyverse","title":"Why use the Tidyverse?","text":"<p>The Tidyverse is an opinionated collection of packages, meaning that while the packages all cohesively follow the same design philosophy, they may not work well with how you prefer to program or for your specific use case. You may choose to use only certain packages from the Tidyverse, or you may choose to only use base R and other alternatives.</p> <p>Some advantages of using the Tidyverse are:</p> <ul> <li>Consistency: All packages in the Tidyverse follow the same design     philosophy, making it easier to learn and use multiple packages. They also     mesh well with one another, making it easier to use multiple packages     together.</li> <li>Piping: Most of the Tidyverse is designed to be used together with pipes.     The <code>|&gt;</code> operator from base R &gt;=4.1 works well for this, or you can use the     <code>%&gt;%</code> pipe from the <code>magrittr</code> package for more advanced piping options.</li> <li>Readability: Often Tidyverse code is more human-readable than more complex     R alternatives, especially when using long pipe chains.</li> </ul> <p>Some disadvantages of using the Tidyverse are:</p> <ul> <li>Learning curve: The Tidyverse has a learning curve, especially if you are     new to R or programming in general. If switching over from base R, there     might also be a learning curve to relearn how to do things you are already     familiar with because of how differently things are designed.</li> <li>Stability: The Tidyverse is constantly evolving, and packages may be     updated or deprecated. While base R tries to emphasize stability across     updates, the Tidyverse packages are actively developed, and updates may     introduce changes that improve functionality but could impact existing code.</li> <li>Compatibility: The Tidyverse is not the only way to do things in R, and as     well, there are countless other packages that may not play nicely with     Tidyverse-oriented data structures or tibbles. In these cases you may have     to convert back and forth between tibbles and data frames, which can be     cumbersome.</li> </ul>"},{"location":"software_development/Languages/R/Tidyverse/#introductory-example","title":"Introductory Example","text":"<p>Say we have a CSV file containing a table, and we want to:</p> <ol> <li>Read the file into R;</li> <li>Perform some manipulations on the data; and</li> <li>Write out the new table to a different CSV file.</li> </ol> <p>First, let's define some common paths for input and output:</p> <pre><code>infile &lt;- \"rawdata/my_df.csv\"\noutfile &lt;- \"procdata/my_df.csv\"\n</code></pre> <p>Using these paths, let's compare the code for what this whole process might look like.</p> Base RTidyverse <pre><code># Read the CSV file\nmy_df &lt;- read.csv(infile)\n\n# Filter to only rows where value &gt; 10\nmy_df_filtered &lt;- my_df[my_df$value &gt; 10, ]\n\n# Select only specific columns\nmy_df_selected &lt;- my_df_filtered[, c(\"id\", \"value\")]\n\n# Write the output out to a new CSV file\nwrite.csv(my_df_selected, outfile, row.names = FALSE)\n</code></pre> <ul> <li>Note: You could also overwrite the <code>my_df</code> variable each time you filter rows and select columns instead of creating new intermediary data frames, especially if you have no need to keep the original or intermediary data.</li> </ul> <pre><code>my_df &lt;- infile |&gt;      # Start with the path, piping forward into the \"pipeline\"\n  read_csv() |&gt;         # From `readr`, reads a csv at the path into a tibble\n  filter(value &gt; 10) |&gt; # From `dplyr`, filters to only rows that match condition\n  select(id, value) |&gt;  # From `dplyr`, selects only specified columns by name\n  write_csv(outfile)    # From `readr`, writes the tibble to a csv file path\n</code></pre> <ul> <li>Note: While technically not required, setting <code>my_df</code> to the pipeline will save the final pipeline result to that variable name. The function <code>write_csv()</code> invisibly returns the same tibble piped into it, thus it is saved as <code>my_df</code>.</li> </ul> <p>Arguably, the human-readability of the code became a bit better, even if you're not necessarily familiar with the Tidyverse. Note that the <code>read.csv()</code> function is different from the <code>read_csv()</code> function, as well as <code>write.csv()</code> and <code>write_csv()</code>. Often base R uses dot notation for their variables and functions, e.g. <code>as.data.frame()</code>, whilst Tidyverse convention is to use snakecase, e.g. <code>as_tibble()</code>.</p>"},{"location":"software_development/Languages/R/Tidyverse/#tidyverse-vs-base-r","title":"Tidyverse vs. Base R","text":"<p>It's important to differentiate the R Tidyverse packages and design from the base R language itself. The R programming language is developed and maintained by the R Core team and R Foundation, whilst the Tidyverse (as well as a multitude of other R programming tools and packages) are developed and maintained by Posit, PBC (formerly RStudio, Inc) on top of the R language.</p> <p>If you're familiar with R, you're almost certainly familiar with the RStudio application, the IDE developed by Posit. Hadley Wickham, the author of the Tidy Data paper, also happens to be the Chief Scientist at Posit. If you're a fan of the Tidyverse, also check out Posit's other useful tools like Positron, Quarto, Shiny, and pak.</p>"},{"location":"software_development/Languages/R/Tidyverse/#additional-resources","title":"Additional Resources","text":""},{"location":"software_development/Languages/R/Tidyverse/#articles","title":"Articles","text":"<ul> <li>Welcome to the Tidyverse     \u2014 A brief paper introducing the Tidyverse, from the package vignette.</li> <li>Tidyverse Wikipedia page.</li> <li>Writing performant code with tidy tools     \u2014 An interesting read on analysing code performance and how to     consider alternatives; especially relevant to package development.</li> </ul>"},{"location":"software_development/Languages/R/Tidyverse/#learning","title":"Learning","text":"<ul> <li>R for Data Science Textbook \u2014 The go-to     textbook for learning R and the Tidyverse, authored by Hadley Wickham, Mine     \u00c7etinkaya-Rundel, and Garrett Grolemund.</li> <li>Swirl \u2014 An interactive learning platform for     R.<ul> <li>Specifically, check out this course     for a quick (and interactive!) taste of working with <code>dplyr</code> and <code>tidyr</code>.</li> </ul> </li> <li>Tidyverse style guide \u2014 A guide to consistent code writing in R.</li> </ul>"},{"location":"software_development/Languages/R/Tidyverse/#packages","title":"Packages","text":"<ul> <li>Tidymodels \u2014 A collection of packages     for modeling and machine learning in R.</li> <li>Pharmaverse \u2014 A connected network of     companies and individuals working to promote collaborative development of     curated open source R packages for clinical reporting usage in pharma.</li> </ul>"},{"location":"software_development/Languages/R/Tidyverse/ggplot2/","title":"Plotting with <code>ggplot2</code>","text":"<p>Page Under Construction</p>"},{"location":"software_development/Languages/R/Tidyverse/ggplot2/#see-also","title":"See Also","text":"<ul> <li>The ggplot2: Elegant Graphics for Data Analysis Textbook - A great resource to understand the underlying framework that <code>ggplot2</code> follows. Authored by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen.</li> </ul>"},{"location":"software_development/Languages/R/networkD3/","title":"networkD3","text":"<p>Creates 'D3' 'JavaScript' network, tree, dendrogram, and Sankey graphs from 'R'.</p>"},{"location":"software_development/Languages/R/networkD3/#documentation","title":"Documentation","text":"<ul> <li>CRAN Page</li> <li>GitHub</li> </ul>"},{"location":"software_development/Languages/R/networkD3/sankey/","title":"Sankey Plot","text":"<p>A Sankey plot is a flow diagram in which the width of the arrows is proportional to the magnitude of the flow. It\u2019s ideal for visualizing how quantities split, merge, or move between stages or categories. Sankey plots can also be variants like alluvial and bump charts, which emphasize different aspects of the data.</p> <p>Aside from the example below, a more comprehensive overview of Sankey plots can be found in the R Graph Gallery.</p>"},{"location":"software_development/Languages/R/networkD3/sankey/#basic-example-with-networkd3","title":"Basic Example with networkD3","text":"<p>In this example, a super simple Sankey plot visualizes how patient sample might be split into training, testing, and validation sets.</p> <pre><code>library(tidyverse)\nlibrary(networkD3)\n\n# Define sample dataset: patient counts for each split\ndf_split &lt;- tibble(\n  set       = c(\"Total Patients\", \"Total Patients\", \"Total Patients\"),\n  subset    = c(\"Training\",       \"Testing\",        \"Validation\"),\n  count     = c(60,               20,               20)\n)\n\n# Create nodes: data.frame\nnodes &lt;- data.frame(name = unique(c(df_split$set, df_split$subset)))\n\n# Create links: map factor levels to node indices (zero-based)\nlinks &lt;- df_split |&gt;\n  mutate(\n    source = match(set,    nodes$name) - 1,\n    target = match(subset, nodes$name) - 1,\n    value  = count\n  ) |&gt;\n  select(source, target, value)\n\n# Draw the Sankey diagram\nsankeyNetwork(\n  Links     = links,\n  Nodes     = nodes,\n  Source    = \"source\",\n  Target    = \"target\",\n  Value     = \"value\",\n  NodeID    = \"name\",\n  fontSize  = 12,\n  nodeWidth = 30,\n  units     = \"patients\"\n)\n</code></pre> <p>Tip</p> <p>Use <code>?sankeyNetwork</code> to get specific details about the parameters and additional options for customizing the Sankey plot, such as how <code>links</code> and <code>nodes</code> should be defined.</p>"},{"location":"software_development/Languages/R/networkD3/sankey/#references","title":"References","text":"<p>R Packages:</p> <ul> <li><code>networkD3</code></li> <li><code>ggalluvial</code></li> <li><code>ggsankey</code></li> </ul>"},{"location":"software_development/Languages/Rust/","title":"Rust","text":""},{"location":"software_development/Languages/SQL/","title":"SQL","text":"<p>Structured Query Language (SQL) is a domain-specific programming language designed for managing data held in a relational database management system.</p> <p>What is SQL?</p>"},{"location":"software_development/Remote_Development/Compute_Canada_Cloud/","title":"Introduction To Compute Canada Cloud","text":""},{"location":"software_development/Remote_Development/Compute_Canada_Cloud/#what-is-the-compute-canada-cloud","title":"What is the Compute Canada Cloud?","text":"<ul> <li>Access to a cloud computing environment with a variety of computing resources<ul> <li>CPUs</li> <li>GPUs</li> <li>Memory</li> <li>Storage</li> <li>Networking</li> </ul> </li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/","title":"Google Cloud Artifact Registry","text":"<p>Google Cloud Artifact Registry is a fully-managed service for storing and managing container images, as well as other software artifacts like Maven, npm, and Python packages. It is designed to integrate seamlessly with GCP, providing enhanced security, authentication, and efficiency over external services like Docker Hub.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#why-use-artifact-registry-instead-of-docker-hub","title":"Why Use Artifact Registry Instead of Docker Hub?","text":"Feature Docker Hub Artifact Registry Security Publicly accessible by default. Limited security features unless on paid tiers. Private by default, with IAM-based fine-grained access control and integration with GCP security features. Authentication Separate login credentials required. Uses GCP-managed identities (IAM roles and service accounts). Network Proximity External to GCP, introducing latency. Hosted within GCP, reducing latency and egress costs. Cost Free tier has pull limits. Paid plans for more. Pay only for what you store and access. Integration Limited GCP integration. Full integration with GCP services like Cloud Build, Compute Engine, and Kubernetes Engine."},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#setting-up-and-using-artifact-registry","title":"Setting Up and Using Artifact Registry","text":"<p>Ensure that you have completed How to Use GCP before starting this process.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#enabling-the-artifact-registry-api","title":"Enabling the Artifact Registry API","text":"<pre><code>gcloud services enable artifactregistry.googleapis.com\n</code></pre>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#creating-an-artifact-repository","title":"Creating an Artifact Repository","text":"<pre><code>gcloud artifacts repositories create [REPOSITORY_NAME] \\\n    --repository-format=docker \\\n    --location=[REGION] \\\n    --description=\"Repository for storing Docker images\"\n</code></pre>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#authenticating-docker-with-artifact-registry","title":"Authenticating Docker with Artifact Registry","text":"<p>Run the following command to configure Docker to authenticate with your Artifact Registry:</p> <pre><code>gcloud auth configure-docker [REGION]-docker.pkg.dev\n</code></pre>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#pushing-images-to-artifact-registry","title":"Pushing Images to Artifact Registry","text":"<ol> <li> <p>Tag your Docker image for Artifact Registry:</p> <pre><code>docker tag [IMAGE_NAME] [REGION]-docker.pkg.dev/[PROJECT_ID]/[REPOSITORY_NAME]/[IMAGE_NAME]:[TAG]\n</code></pre> </li> <li> <p>Push the image:</p> <pre><code>docker push [REGION]-docker.pkg.dev/[PROJECT_ID]/[REPOSITORY_NAME]/[IMAGE_NAME]:[TAG]\n</code></pre> </li> </ol>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/artifact_registry/#pulling-images-from-artifact-registry","title":"Pulling Images from Artifact Registry","text":"<pre><code>docker pull [REGION]-docker.pkg.dev/[PROJECT_ID]/[REPOSITORY_NAME]/[IMAGE_NAME]:[TAG]\n</code></pre>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/bigquery/","title":"BigQuery","text":"<p>BigQuery is a powerful SQL-based data warehouse that allows you to process, load, and analyze large datasets efficiently using SQL queries.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/bigquery/#why-use-bigquery","title":"Why Use BigQuery?","text":"<ul> <li>Quickly preprocess and explore large datasets using SQL-like query.</li> <li>Simplifies aggregation, feature extraction, and preparation for ML models.</li> <li>You can directly load data from a GCS bucket into an SQL-based data     warehouse (BigQuery). It supports all types of data \u2014 structured,     semi-structured, and unstructured; including tsv, csv, parquet, avro, xlsx,     and many more.</li> <li>To use BigQuery with a client library, please follow this link     for detailed guide.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/bigquery/#setting-up-and-using-bigquery","title":"Setting Up and Using BigQuery","text":"<p>Ensure that you have completed How to Use GCP before starting this process.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/bigquery/#loading-data-into-bigquery","title":"Loading Data into BigQuery","text":"<ul> <li> <p>From GCS:</p> <pre><code>bq load --source_format=CSV &lt;DATASET_NAME&gt;.&lt;TABLE_NAME&gt; gs://&lt;BUCKET_NAME&gt;/&lt;FILE_NAME&gt;\n</code></pre> </li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/bigquery/#querying-data","title":"Querying Data","text":"<ul> <li>Use BigQuery's web interface or CLI to run SQL queries for data cleaning,     feature engineering, and exploratory analysis.</li> <li> <p>Example:</p> <pre><code>SELECT * FROM `project_id.dataset_name.table_name` LIMIT 10;\n</code></pre> </li> </ul> <p>Follow the instructions on this page to learn more about BigQuery.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/cloud_sql/","title":"Cloud SQL for MySQL, PostgreSQL, and Microsoft SQL Server","text":"<p>Google Cloud SQL is a fully-managed relational database service for MySQL, PostgreSQL, and Microsoft SQL Server. It eliminates the need for database maintenance while offering high availability, scalability, and security. Below is a comprehensive guide to using Cloud SQL effectively for your projects.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/cloud_sql/#why-use-cloud-sql","title":"Why Use Cloud SQL?","text":"<ul> <li>Managed Service: Automated backups, updates, and maintenance.</li> <li>Scalability: Seamless scaling for growing workloads.</li> <li>Security: Built-in encryption, IAM-based access, and network security.</li> <li>Integration: Works seamlessly with GCP services like Compute Engine,     Kubernetes Engine, and BigQuery.</li> <li>Flexibility: Supports popular relational databases: MySQL, PostgreSQL,     and Microsoft SQL Server.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/cloud_sql/#setting-up-and-using-cloud-sql","title":"Setting Up and Using Cloud SQL","text":"<p>Ensure that you have completed How to Use GCP before starting this process.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/cloud_sql/#enabling-the-cloud-sql-api","title":"Enabling the Cloud SQL API","text":"<pre><code>gcloud services enable sqladmin.googleapis.com\n</code></pre>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/cloud_sql/#creating-a-cloud-sql-instance","title":"Creating a Cloud SQL Instance","text":"<ul> <li> <p>Example Using MySQL:</p> <pre><code>gcloud sql instances create [INSTANCE_NAME] \\\n    --database-version=MYSQL_8_0 \\\n    --cpu=[CPU_COUNT] \\\n    --memory=[MEMORY_SIZE] \\\n    --region=[REGION]\n</code></pre> </li> <li> <p>Example using PostgreSQL:</p> <pre><code>gcloud sql instances create [INSTANCE_NAME] \\\n    --database-version=POSTGRES_14 \\\n    --cpu=[CPU_COUNT] \\\n    --memory=[MEMORY_SIZE] \\\n    --region=[REGION]\n</code></pre> </li> <li> <p>Parameters:</p> <ul> <li><code>--cpu</code>: Number of vCPUs (e.g., 2).</li> <li><code>--memory</code>: RAM allocation (e.g., 4GB).</li> <li><code>--region</code>: Choose a region (e.g., <code>us-central1</code>).</li> </ul> </li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/cloud_sql/#configuring-users-and-databases","title":"Configuring Users and Databases","text":"<p>Using the same <code>INSTANCE_NAME</code> as configured in the previous step:</p> <ul> <li> <p>Create a Database with the command below:</p> <pre><code>gcloud sql databases create [DATABASE_NAME] --instance=[INSTANCE_NAME]\n</code></pre> </li> <li> <p>Add a User with the command below:</p> <pre><code>gcloud sql users create [USERNAME] --password=[PASSWORD] --instance=[INSTANCE_NAME]\n</code></pre> </li> </ul> <p>For a detailed guide on using client services, please refer to this link</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/compute_engine/","title":"Compute Engine","text":"<p>Compute Engine on GCP is the standard and most basic VM creation workflow (similar to EC2 on AWS or Virtual Machines on Azure). It is the most common way in the lab to deploy web applications and API's.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/compute_engine/#why-compute-engine","title":"Why Compute Engine","text":"<p>Despite how basic Compute Engine is, it is extremely important for the following reasons:</p> <ul> <li>Allows for full control.<ul> <li>Nginx configurations (allowing for many different projects to be routed to)</li> <li>Certificate management for domains or sub domains</li> <li>System services management (Ideal for many APIs)</li> </ul> </li> <li>Available 24/7, no downtime or boot up time unless explicilty enforced<ul> <li>Unlike Cloud Run or Build where when the application becomes dormant, there will be boot up time upon requesting access to the resource (not ideal for API's or web-apps)</li> </ul> </li> <li>Fairly cost effective. <ul> <li>$25-35 CAD/month for a front-end and back-end deployment that will accessible 24/7</li> </ul> </li> <li>Relatively easy to use if you're familiar with Linux/Unix systems</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/compute_engine/#cost","title":"Cost","text":"<p>Compute Engine VMs can really vary in cost. You usually need a basic n1-standard-1 for a web application deployment (front-end+API). This tends to cost $25-35 CAD/month depending on how much SSD is needed on the VM to clone the project and host data if needed. However, with some very light workflows or beta/QA deployments you can get away with using a f1 micro or g1 small which will only cost around 10-15 CAD/month.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/compute_engine/#compute-engine-vm-settings-for-web-applications","title":"Compute Engine VM Settings For Web Applications","text":"<ol> <li>Navigate to 'Compute Engine' --&gt; 'VM instances'.</li> <li>Press 'Create instance' in the top left.</li> <li>Select your machine type and region (restricted to Montreal/Toronto). </li> <li>Then select your OS and base storage size<ul> <li>It is reccommend to use Ubuntu's 24.10 or newer image. Note: Avoid image's tagged as 'minimal' since they don't include many of the base packages that are extremely useful for getting started on a VM.</li> <li>15-20GB is also a good base disk size for typical front-end/back-end combos. This may need to be altered depending on your respository sizes but typically this is a good range. </li> </ul> </li> <li>Under the data protection tab you can turn snapshots off unless this is a high profile project where data must remain persistent and safe under critical failure instances.</li> <li>Under the networking, make sure to turn on HTTP and HTTPS traffic so that the VM is accessible. </li> <li>Under the security tab \"allow full access to all cloud APIs\" if they are to be used for the project to avoid difficulties later on. </li> </ol>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/compute_engine/#compute-engine-configurations-for-web-apps","title":"Compute Engine Configurations for Web Apps","text":"<p>Typically there are a few things that are standard for all VM configurations for web applications in the lab.</p> <ol> <li>Create a master user<ul> <li>This important so that anyone who wants to adjust configurations or processes on the VM can do so <pre><code>sudo adduser admin_uhn\nsudo usermod -aG sudo admin_uhn\n</code></pre></li> </ul> </li> <li>Install the needed version of nodejs/npm via NVM (Node Version Manager) for the project to be cloned<ul> <li>Install NVM: <pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\n</code></pre></li> <li>Make NVM accessible to the logged in user on the VM: <pre><code>export NVM_DIR=\"$([ -z \"${XDG_CONFIG_HOME-}\" ] &amp;&amp; printf %s \"${HOME}/.nvm\" || printf %s \"${XDG_CONFIG_HOME}/nvm\")\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\" # This loads nvm\n</code></pre></li> </ul> </li> <li>Set up Nginx<ul> <li>install Nginx: <pre><code>sudo apt install nginx\n</code></pre></li> <li>Give account write access to Nginx's www folder: <pre><code>sudo chown -R admin_uhn:admin_uhn /var/www/\n</code></pre></li> <li>clone project in /var/www</li> <li>Remove default Nginx file existing at /etc/nginx/sites-available/ and /etc/nginx/sites-enabled/</li> <li>Setup Nginx project configurations in /etc/nginx/sites-available: <pre><code>server {\n    client_max_body_size 20M;\n    listen 80;\n    server_name _; # Can be replaced with a domain name if you have one for your project\n    root /var/www/repo-name/client/build; #Where your built index.html file resides\n    index index.html;\n\n    location / {\n        try_files $uri $uri/ /index.html;\n        add_header Cache-Control \"no-cache\";\n    }\n\n    location /api {\n        include proxy_params;\n        proxy_pass http://localhost:2000;\n    }\n}\n</code></pre></li> <li>Add symlink file to sites-enabled so that files stay synced when changed: <pre><code>ln -s /etc/nginx/sites-available/file-name.nginx /etc/nginx/sites-enabled/file-name.nginx\n</code></pre></li> </ul> </li> </ol>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/gcs/","title":"Google Cloud Storage (GCS)","text":"<p>GCS is a scalable and secure object storage for data files, datasets, and ML-ready data.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/gcs/#why-use-gcs","title":"Why Use GCS?","text":"<ul> <li>Centralized storage for raw and processed datasets.</li> <li>Facilitates data sharing across team members.</li> <li>Integration with other GCP services like BigQuery and AI/ML tools.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/gcs/#setting-up-and-using-gcs","title":"Setting Up and Using GCS","text":"<p>Ensure that you have completed How to Use GCP before starting this process.</p> <p>Use the GCP console, gcloud CLI, or API to create a bucket:</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/gcs/#creating-a-gcs-bucket-via-cloud-console","title":"Creating a GCS Bucket via Cloud Console","text":"<ul> <li>Follow the instructions in this documentation     to create buckets.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/gcs/#creating-a-gcs-bucket-via-terminal","title":"Creating a GCS Bucket via Terminal","text":"<ul> <li> <p>In your development environment, run the <code>gcloud storage buckets create</code>     command:</p> <pre><code>gcloud storage buckets create gs://&lt;BUCKET_NAME&gt; --location=&lt;BUCKET_LOCATION&gt;\n</code></pre> <p>Where: - <code>&lt;BUCKET_NAME&gt;</code> is the name you want to give your bucket, subject to     naming requirement. For example, <code>my-bucket</code>. - <code>&lt;BUCKET_LOCATION&gt;</code> is the location of your bucket. For example,     <code>us-east1</code>. - If the request is successful, the command returns the following message:</p> <pre><code>Creating gs://BUCKET_NAME/...\n</code></pre> </li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/gcs/#transferring-data-across-gcs","title":"Transferring Data Across GCS","text":"<ul> <li> <p>From Local to GCS:</p> <pre><code>gsutil cp &lt;local_file&gt; gs://&lt;bucket_name&gt;\n</code></pre> </li> <li> <p>From GCS to Local:</p> <pre><code>gsutil cp gs://&lt;bucket_name&gt;/&lt;file_name&gt; &lt;local_destination&gt;\n</code></pre> </li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/","title":"Introduction to Google Cloud Platform (GCP)","text":""},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/#what-is-google-cloud-platform","title":"What is Google Cloud Platform?","text":"<p>Google Cloud Platform is a suite of cloud computing services offered by Google, providing a wide range of infrastructure and application services that can be accessed on-demand. It enables users to build, deploy, and scale applications seamlessly while taking advantage of Google\u2019s powerful and reliable infrastructure.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/#why-use-gcp","title":"Why Use GCP?","text":"<ul> <li>Scalability: Easily scale resources up or down based on workload.</li> <li>Pay-as-You-Go: Only pay for what you use.</li> <li>Integration: Connect seamlessly with open-source and enterprise tools.</li> <li>Global Infrastructure: High-speed global network for faster operations.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/#how-to-use-gcp","title":"How to Use GCP","text":"<p>Prerequisites:</p> <ul> <li>Ensure you have an active Google account.</li> <li>Confirm that your account has been added to the relevant GCP project.</li> </ul> <p>To access and use Google Cloud Platform (GCP), follow these steps:</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/#accessing-gcp-via-the-cloud-console","title":"Accessing GCP via the Cloud Console","text":"<ul> <li>Visit the Google Cloud Console.</li> <li>Explore the dashboard to view, manage, and configure services, projects, and     resources.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/#accessing-gcp-via-terminal","title":"Accessing GCP via Terminal","text":"<p>To interact with GCP directly from your terminal:</p> <ol> <li> <p>Initialize Google Cloud SDK</p> <ul> <li> <p>Install the Google Cloud SDK on your machine by following the official   installation guide, then use   the following command:</p> <pre><code>gcloud init\n</code></pre> </li> <li> <p>Follow the prompts to authenticate, select your project, and configure the   settings.</p> </li> </ul> </li> <li> <p>Authenticate Your Terminal</p> <ul> <li> <p>Run the following command to authenticate:</p> <pre><code>gcloud auth login\n</code></pre> </li> <li> <p>This opens a browser window asking you to log in with your Google account.</p> </li> <li>After login, your terminal will be authenticated, and you\u2019ll see a   confirmation message.</li> </ul> </li> <li> <p>Set the Active Project</p> <ul> <li> <p>Ensure the correct project is set as the active one.</p> <pre><code>gcloud config set project &lt;PROJECT_ID&gt;\n</code></pre> </li> <li> <p>Replace <code>&lt;PROJECT_ID&gt;</code> with your GCP project ID (e.g., <code>bhklabproject-123</code>).</p> </li> <li> <p>Verify the active project:</p> <pre><code>gcloud config list project\n</code></pre> </li> </ul> </li> </ol>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/introduction/#commonly-used-gcp-services","title":"Commonly Used GCP Services","text":"<p>Below are some key Google Cloud Platform (GCP) services that can be used for your project:</p> <ul> <li>Google Cloud Storage (GCS) - Scalable and secure object storage for data files, datasets, and ML-ready data</li> <li>BigQuery - SQL-based data warehouse for processing and analyzing large datasets</li> <li>Cloud SQL - Fully-managed relational database service for MySQL, PostgreSQL, and Microsoft SQL Server</li> <li>Virtual Machines (VMs) - Scalable, on-demand virtual machines for running custom ML experiments</li> <li>Artifact Registry - Fully-managed service for storing and managing container images and software artifacts</li> </ul> <p>Each service page provides detailed information about why to use the service and step-by-step setup instructions.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/serving_images_and_videos/","title":"Serving Images and Videos From A Bucket","text":""},{"location":"software_development/Remote_Development/Google_Cloud_Platform/serving_images_and_videos/#general-use","title":"General Use","text":"<p>Occasionally in a web application there are images and/or videos that need to be frequently added or updated. This tends to be the case when the images served in an application are attached to a database with entries that are continously being added to or changing. Typically when serving images in the front-end from a database entry, we would need to add an image to the code repository every time we want to add a new database entry. However, serving images from a GCS Buckets can help speed up the process of adding images for new entries in a database collection to your project by removing the step of pushing new images to your code repository.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/serving_images_and_videos/#example","title":"Example","text":"<p>For the BHK lab website we have database collections for our team members, important publications, presentations, and preprints which need to be frequently updated. If images were stored locally within the project's image folder for these sections, there would be many hurdles in the way of adding or updating entries in our database (containing images) and having the changes reflect in the deployed application. This is because not only would we need to adjust our database with the new update/entry, we would also need to add the image to the project repository and then redeploy our application. </p> <p>For example, in the case where we want to add or update a member from the Caboodle (bhklab) database we only need to execute the following two steps when utilizing GCS for image serving:</p> <ol> <li> <p>Have a new database entry created:     <pre><code>    {\n        \"_id\": {\n            \"$oid\": \"67dc262b7658e91803c89099\"\n        },\n        \"name\": \"Matthew Boccalon\",\n        \"slug\": \"Matthew_Boccalon\",\n        \"preferredName\": \"\",\n        \"position\": \"Software Developer\",\n        \"bio\": \"Matthew completed his undergrad at York University in computer science. He has an adept understanding of various languages, technologies, and frameworks that aid him in his passion for full stack development. Matthew has previously worked at Lymphoma Canada as their Donor Database Manager and has now pivoted into full stack development here in the BHK lab.\",\n        \"preferredEmail\": \"matthew.boccalon@uhn.ca\",\n        \"socials\": {\n            \"twitter\": \"https://twitter.com/MatthewBHKLab\",\n            \"bluesky\": \"\",\n            \"linkedIn\": \"https://www.linkedin.com/in/matthew-boccalon-1b96791a2/\"\n        },\n        \"image\": \"matthew_boccalon.png\"\n    }\n</code></pre></p> </li> <li> <p>Upload image to GCS bucket referenced for displaying members in the code</p> </li> </ol> <p></p> <p>The updated member will immediately render on the lab website in the current members section. The aftermath looks something like the following, without having to redeploy the application:</p> <p></p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/serving_images_and_videos/#something-to-keep-in-mind","title":"Something To Keep In Mind","text":"<p>When referencing a GCS bucket for an image, it's important to invoke the current date function as added below to make sure the most recent images are always referenced. This is because browsers tend to cache GCS images, to avoid this and to ensure displaying the most recent version of a sourced image, you tack on <code>?v=${new Date().getTime()}</code> to the end of the src string.</p> <pre><code>&lt;img src={`https://storage.googleapis.com/caboodle-images/member-photos/${item.image}?v=${new Date().getTime()}`}&gt;\n</code></pre>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/","title":"Google Cloud Vertex AI","text":"<p>Vertex AI is Google Cloud's end-to-end platform for building, training, deploying, and managing machine learning models.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#why-use-vertex-ai","title":"Why Use Vertex AI?","text":"<ul> <li>Unified platform for ML workflows: training, tuning, deploying, and monitoring models.</li> <li>Integrated with other GCP services like GCS and BigQuery.</li> <li>Supports custom models, AutoML, and pre-trained models.</li> <li>Built-in MLOps tools for tracking experiments, model versions, and pipelines.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#main-features-of-vertex-ai","title":"Main Features of Vertex AI","text":""},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#training-train-custom-models-with-vertex-ai","title":"Training: Train custom models with Vertex AI","text":"<p>Vertex AI enables you to train machine learning models using pre-built containers (for frameworks like TensorFlow, PyTorch, XGBoost) or your custom containers. You can configure distributed training, use accelerators (GPUs/TPUs), and tune hyperparameters automatically. To start training:</p> <ul> <li>Go to Vertex AI &gt; Training in the GCP console.</li> <li>Choose between Custom Training, AutoML, or Pre-trained models.</li> <li>Specify your dataset, training script (if custom), and machine type.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#workbench-managed-jupyter-notebooks-integrated-with-gcp-resources","title":"Workbench: Managed Jupyter notebooks integrated with GCP resources.","text":"<p>Setting Up and Using Vertex AI Workbench</p> <ul> <li>Go to the \"Vertex AI Workbench\" in the GCP console.</li> <li>Click New Notebook.</li> <li>Select:</li> <li>Environment (e.g., TensorFlow Enterprise, PyTorch).</li> <li>Machine Type (CPU or GPU).</li> <li>After creation, click Open JupyterLab to start working.</li> </ul> <p>You can also start and stop notebooks as needed to control costs.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#model-monitoring-track-model-performance","title":"Model Monitoring : Track model performance.","text":"<p>Vertex AI includes a built-in experiment tracking tool called Vertex AI Experiments. It automatically or manually tracks ML training runs and logs:</p> <ul> <li>Hyperparameters (e.g., learning rate, batch size)</li> <li>Evaluation metrics (e.g., accuracy, loss)</li> <li>Artifacts (e.g., model files)</li> <li> <p>Environment details (e.g., container, machine type) To use it you can:</p> </li> <li> <p>Automatically tracks runs from Vertex AI Workbench.</p> </li> <li>You can also log metrics programmatically using the Python SDK.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#prediction-and-deployment-serve-models","title":"Prediction and Deployment: Serve models","text":"<p>Vertex AI provides two ways to serve models: online prediction (real-time requests) and batch prediction (large-scale async jobs). You can deploy trained models to an endpoint with autoscaling and version management. To deploy a model:</p> <ul> <li>Go to Vertex AI Models.</li> <li>Upload or select your model and deploy it to an endpoint.</li> <li>Set machine type and scaling options.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#feature-store-manage-and-serve-ml-features","title":"Feature Store: Manage and serve ML features","text":"<p>Vertex AI Feature Store provides a centralized repository to manage, reuse, and serve ML features for training and online inference. It ensures consistency between training and serving data. To use Feature Store:</p> <ul> <li>Create a Featurestore in Vertex AI Feature Store.</li> <li>Define entity types and features.</li> <li>Ingest data manually or set up ingestion pipelines.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/vertexai/#pipelines-automate-and-orchestrate-ml-workflows","title":"Pipelines: Automate and orchestrate ML workflows","text":"<p>Vertex AI Pipelines allow you to build reproducible, scalable ML workflows by chaining steps like data processing, training, evaluation, and deployment. Based on Kubeflow Pipelines, it helps manage experiment runs and simplifies MLOps. To create a pipeline:</p> <ul> <li>Define your steps using Vertex Pipelines SDK.</li> <li>Upload and run the pipeline through Vertex AI Pipelines.</li> <li>Monitor executions directly in the console.</li> </ul> <p>You can find detailed information and tutorials about Vertex AI in the official: https://cloud.google.com/vertex-ai/docs</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/virtual_machines/","title":"GCP Virtual Machines (VMs)","text":"<p>A Cloud VM is a scalable, on-demand virtual machine hosted in the cloud. It functions like a physical computer, providing compute power, memory, storage, and network connectivity. Cloud VMs are versatile and can be used for a variety of tasks, from running applications and hosting websites to managing databases and performing intensive data processing.</p> <p>In summary, VMs offer flexible compute instances to run custom ML experiments, manage pipelines, or host applications.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/virtual_machines/#why-use-cloud-vms","title":"Why Use Cloud VMs?","text":"<ul> <li>Ideal for workloads requiring full control over the environment, OS, and     configurations.</li> <li>Provides isolated environments for training ML models.</li> <li>Supports GPU/TPU acceleration for deep learning tasks.</li> <li>Can host containerized ML workflows using Docker.</li> </ul>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/virtual_machines/#setting-up-and-using-gcp-vms","title":"Setting Up and Using GCP VMs","text":"<p>Ensure that you have completed How to Use GCP before starting this process.</p>"},{"location":"software_development/Remote_Development/Google_Cloud_Platform/virtual_machines/#creating-a-gpu-enabled-vm-for-model-training","title":"Creating a GPU-Enabled VM for Model Training","text":"<pre><code>gcloud compute instances create $INSTANCE_NAME \\\n    --zone=$ZONE \\\n    --image-family=$IMAGE_FAMILY \\\n    --image-project=deeplearning-platform-release \\\n    --maintenance-policy=TERMINATE \\\n    --accelerator=\"type=nvidia-tesla-v100,count=1\" \\\n    --metadata=\"install-nvidia-driver=True\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>--image-family</code> must be one of the GPU-specific image types. For more     information, see Choosing an Image.</li> <li><code>--image-project</code> must be <code>deeplearning-platform-release</code>.</li> <li><code>--maintenance-policy</code> must be <code>TERMINATE</code>. For more information, see     GPU Restrictions.</li> <li><code>--accelerator</code> specifies the GPU type to use. Must be specified in the     format <code>--accelerator=\"type=TYPE,count=COUNT\"</code>. Supported values of     <code>TYPE</code> are:<ul> <li><code>nvidia-tesla-v100</code>, (<code>count=1</code> or <code>8</code>)</li> <li><code>nvidia-tesla-p100</code>, (<code>count=1</code>, <code>2</code>, or <code>4</code>)</li> <li><code>nvidia-tesla-p4</code>, (<code>count=1</code>, <code>2</code>, or <code>4</code>)</li> </ul> </li> </ul>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/","title":"High Performance Computing for Health (HPC4Health / H4H)","text":"<p>Tutorials and resources for HPC4Health can be found here:</p> <p>High Performance Computing for Health (HPC4Health / H4H), Guide by BHKLab</p>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/quick_start/","title":"Quick Start","text":"<p>When connecting to H4H for the first time, here are some recommended action items:</p>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/quick_start/#set-up-your-bashrc-file","title":"Set up your <code>.bashrc</code> file","text":"TLDR: Complete <code>.bashrc</code> <pre><code># .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n\n# User specific environment\nif ! [[ \"$PATH\" =~ \"$HOME/.local/bin:$HOME/bin:\" ]]\nthen\n    PATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nfi\nexport PATH\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\nif [ -d ~/.bashrc.d ]; then\n        for rc in ~/.bashrc.d/*; do\n                if [ -f \"$rc\" ]; then\n                        . \"$rc\"\n                fi\n        done\nfi\n\nunset rc\n\n# BHKLAB additions\n# User specific aliases and functions\nalias bhklab=\"cd /cluster/projects/bhklab\"\nalias radiomics=\"cd /cluster/projects/radiomics\"\n\n# File permission setting\n# Gives rwx for user and group, r-x for other\numask 002\n</code></pre> <p>The <code>.bashrc</code> file is a configuration file for the Bash shell. For a full description of this file, you can check out this article.</p> <p>To view it, start in your home directory on H4H. You can open the file with your favourite text-editor. If you're unfamiliar with those, you can start with nano</p> <pre><code>$ pwd\n/cluster/home/&lt;YOUR_H4H_USERNAME&gt;\n$ nano .bashrc\n</code></pre> <p>By default, it should look something like this:</p> <pre><code># .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n\n# User specific environment\nif ! [[ \"$PATH\" =~ \"$HOME/.local/bin:$HOME/bin:\" ]]\nthen\n    PATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nfi\nexport PATH\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\nif [ -d ~/.bashrc.d ]; then\n        for rc in ~/.bashrc.d/*; do\n                if [ -f \"$rc\" ]; then\n                        . \"$rc\"\n                fi\n        done\nfi\n\nunset rc\n</code></pre> <p>For some convenience, we're going to add some extra settings to this file.</p>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/quick_start/#data-directory-aliases","title":"Data Directory Aliases","text":"<p>First, to make it easy to get to the BHKLab data directories, we'll add two aliases:</p> <pre><code># User specific aliases and functions\nalias bhklab=\"cd /cluster/projects/bhklab\"\nalias radiomics=\"cd /cluster/projects/radiomics\"\n</code></pre> <p>With these aliases, you can access the <code>bhklab</code> and <code>radiomics</code> data directories easily:</p> <pre><code>$ bhklab\n$ pwd\n/cluster/projects/bhklab\n</code></pre> Access requirement <p>To use these aliases, you need to be connected to a data node or compute node. See Remote Access Nodes for instructions on how to access these.</p> <p>Additionally, when you set up your H4H account with Zhibin Lu, he would have granted you access to <code>bhklab</code>, <code>radiomics</code>, or <code>both</code>. This will determine which data directories you have access to.</p>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/quick_start/#default-file-permissions","title":"Default File Permissions","text":"<p>Second, to make sure any files, directories, etc. you make, either interactively or using a script, is accessible by other lab members, add the following:</p> <pre><code># File permission setting\n# Gives rwx for user and group, r-x for other\numask 002\n</code></pre> Internal or Private data <p>If you work with any internal or private datasets, you may want to adjust the file permissions setup or utilize subgroups to restrict access. Contact the lab member listed next to High Performance Computing (HPC) on H4H on the Lab Member Expertise page if you need assistance.</p>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/quick_start/#interactive-jobs-salloc","title":"Interactive Jobs - <code>salloc</code>","text":"<p>Familiarize yourself with <code>salloc</code> by reading our documentation on Interactive Jobs and Debugging and the Slurm <code>salloc</code> documentation.</p>"},{"location":"software_development/Remote_Development/High_Performance_Computing_for_Health/quick_start/#submitting-jobs-sbatch","title":"Submitting Jobs - <code>sbatch</code>","text":"<p>Familiarize yourself with submitting jobs by reading our documentation on Submitting Jobs on H4H and the Slurm <code>sbatch</code> documentation.</p> <p>Slurm header example</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_job_name\n#SBATCH --mem=8G\n#SBATCH --time 1:0:0\n#SBATCH --cpus-per-task 2\n#SBATCH --nodes 1\n#SBATCH --mail-user=bhklab.johnsmith@gmail.com\n#SBATCH --mail-type=BEGIN,FAIL,END\n#SBATCH --output=\"~/slurm_logs/%A-%x.out\"\n</code></pre> <ul> <li>This will submit a job called \"my_job_name\".</li> <li>It requests 2 CPUs with 8G of memory each for 1 hour. </li> <li>An email will be sent to <code>bhklab.johnsmith@gmail.com</code> when the job begins, fails, and/or ends.</li> <li>Logs will be output as a file with the format <code>{job number}-{job name}</code> in a directory called <code>slurm_logs</code> in the user's home directory.</li> </ul>"},{"location":"software_development/Remote_Development/Lab_Server/","title":"Lab Server","text":""},{"location":"software_development/Remote_Development/Lab_Server/#accessing-the-lab-server","title":"Accessing the Lab Server","text":"<p>The lab server is accessible in a manner similar to H4H. </p> <p>Complete SSH instructions can be found here.</p>"},{"location":"software_development/Remote_Development/Lab_Server/#tldr-ssh-without-password","title":"TLDR: SSH without password","text":"<p>In your home directory, create a new key with the following command: <pre><code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n</code></pre> Save the key in the default location <code>(~/.ssh/id_rsa)</code> by pressing Enter. Leave the passphrase empty to enable passwordless login.</p> <p>To automatically add your public key to the <code>~/.ssh/authorized_keys</code> file on the container <pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub -p &lt;PORT&gt; &lt;USERNAME&gt;@&lt;HOSTNAME&gt;\n</code></pre> Enter the password to the container when prompted.</p> <p>To create an alias for the server, add the following to your <code>~/.ssh/config</code> file: <pre><code>Host labserver\n    HostName &lt;HOSTNAME&gt;\n    Port &lt;PORT&gt;\n    User &lt;USERNAME&gt;\n    IdentityFile ~/.ssh/id_rsa\n</code></pre></p> <p>You should now be able to SSH into the server without a password. <pre><code>ssh labserver\n</code></pre></p>"},{"location":"software_development/Remote_Development/Lab_Server/#i-cant-see-any-files-in-the-home-directory","title":"I can't see any files in the home directory","text":"<p>This likely means the server was rebooted and the directory needs to be re-mounted. </p> <p>To re-mount the home directory, run the following command:</p> <pre><code>sudo mount -a\n</code></pre> <p>If you are still having issues, please contact Jermiah Joseph or the lab coordinator.</p>"},{"location":"software_development/Remote_Development/Lab_Server/github_access/","title":"GitHub Access","text":"<p>In order to use GitHub on the lab server, you will need to use an SSH key set up in your GitHub account.</p> <p>First check that you have an SSH key set up on your local machine for your GitHub account. If you don't, follow the guide on generating a new SSH key and adding it to the ssh-agent.</p> <p>You can then follow GitHub Docs' guide on using SSH agent forwarding to forward your SSH key to the lab server.</p> <ul> <li>On step 2 under \"Setting up SSH agent forwarding\", add the <code>ForwardAgent yes</code> line under the lab server alias you set up in your <code>~/.ssh/config</code> file.</li> </ul>"},{"location":"software_development/Remote_Development/Lab_Server/github_access/#mac-specific-step-for-ssh-agent-forwarding","title":"Mac Specific Step for SSH Agent Forwarding","text":"<p>If you are setting up SSH agent forwarding on a Mac, there is an extra step that you need to take for your local <code>ssh-agent</code> to \"remember\" the key.</p> <p>After you've set up your SSH key, import the SSH keys into Keychanin using this command: <pre><code>ssh-add --apple-use-keychain YOUR-KEY\n</code></pre></p> <p>This will add the key to your local <code>ssh-agent</code> and allow you to use it for SSH agent forwarding.</p> <p>Read more</p>"},{"location":"software_development/Version_Control/orcestra_vc/","title":"ORCESTRA Version Controlling","text":"<p>ORCESTRA is a project bound to the principles of reproducibility and transparency. This is because ORCESTRA is a dataset repository that provides key metadata for the pipelines, raw data, and tools needed to create the standardized datasets published to the platform. This ensures users of the datasets on our platform can be confident in the work we produce and even validate them. The transparency and reproducibility also ensures that platform users can take our metadata and create their own versions of our datasets by adding or omitting fields that we can include in our final datasets.</p>"},{"location":"software_development/Version_Control/orcestra_vc/#what-we-version-control","title":"What We Version Control","text":"<p>ORCESTRA is setup to version control several key components:</p> <ol> <li>The Github repository to create the dataset + commitID used</li> <li>The raw data sources and their versions</li> <li>Tools and key packages used in the pipeline + their versions</li> <li>The final data object DOI from Zenodo</li> </ol> <p>Ensuring users have access to the above resources for each dataset on ORCESTRA we can be confident that users will be able to clearly understand how the dataset was created, what it's comprised of, and how one could recreate it.</p>"},{"location":"software_development/Version_Control/orcestra_vc/#where-does-orcestra-source-metadata","title":"Where Does ORCESTRA Source Metadata","text":"<p>ORCESTRA sources the majority of the metadata for the web-app from the config.yaml file that is required in the pipeline Github repository. Upon the dataset being processed the fields for the raw data and tools are extracted directly from the .yaml file, the Github commitID is sourced from the pipeline repository, and the Zenodo DOI is sourced once the API uploads the processed dataset to the lab Zenodo account.</p>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/","title":"Common Challenges and Solutions in Code Reviews","text":"<p>Code reviews are an essential part of software development, but they come with their own set of challenges. Understanding these obstacles and addressing them proactively can make the process more effective and enjoyable for everyone involved.</p> <p>Below are some common challenges teams face during code reviews and proposed solutions to overcome them.</p>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#challenges-and-solutions","title":"Challenges and Solutions","text":""},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#time-constraints","title":"Time Constraints","text":"<p>Problem: Developers often feel overwhelmed with their workload, making it difficult to dedicate enough time for thorough code reviews.  </p> <p>Impact: Reviews may be rushed, resulting in overlooked issues and suboptimal feedback.  </p> <p>Solution:  </p> <ul> <li>Encourage small, focused pull requests to make reviews more manageable.  </li> <li>Allocate dedicated time for reviews during the workday to prioritize them effectively.  </li> <li>Use automation tools to handle repetitive checks (e.g., style or formatting issues).  </li> </ul>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#adversarial-dynamics","title":"Adversarial Dynamics","text":"<p>Problem: Code reviews can sometimes feel confrontational, with reviewers focusing on criticizing rather than improving the code.  </p> <p>Impact: Creates tension within the team and discourages collaboration.  </p> <p>Solution:  </p> <ul> <li>Promote a positive mindset by framing feedback as an opportunity for learning and improvement.  </li> <li>Encourage respectful, constructive comments that focus on the code, not the person.  </li> <li>Provide training on effective communication for both authors and reviewers.  </li> </ul>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#large-or-complex-changes","title":"Large or Complex Changes","text":"<p>Problem: Reviews of extensive or overly complex changes can be overwhelming and time-consuming.  </p> <p>Impact: Leads to fatigue, reduced attention to detail, and delays in the review process.  </p> <p>Solution:</p> <ul> <li>Request authors to split large changes into smaller, self-contained pull requests.  </li> <li>Use feature flags or staged development to integrate large changes incrementally.  </li> </ul>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#lack-of-clear-standards","title":"Lack of Clear Standards","text":"<p>Problem: Without a defined set of coding standards, reviews can become subjective and inconsistent.  </p> <p>Impact: Results in confusion and inefficiency, with different reviewers providing conflicting feedback.  </p> <p>Solution:  </p> <ul> <li>Develop and document coding standards for the team, including style, architecture, and testing.  </li> <li>Leverage linters and automated tools to enforce standards consistently.  </li> <li>Regularly review and update standards to align with team goals and best practices.  </li> </ul>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#inadequate-feedback","title":"Inadequate Feedback","text":"<p>Problem: Reviewers may provide vague or unhelpful comments, leaving the author unsure of how to proceed.  </p> <p>Impact: Reduces the effectiveness of the review and prolongs the process.  </p> <p>Solution:  </p> <ul> <li>Ensure feedback is specific, actionable, and concise.  </li> <li>Use examples or links to documentation to clarify points.  </li> <li>Balance feedback by highlighting both strengths and areas for improvement.  </li> </ul>"},{"location":"software_development/Version_Control/code_reviews/challenges_solutions/#conclusion","title":"Conclusion","text":"<p>By addressing these common challenges with thoughtful solutions, teams can transform code reviews into a constructive and efficient process.</p> <p>This not only improves the quality of the codebase but also fosters a collaborative and supportive team environment.</p>"},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/","title":"Conducting a Review","text":""},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/#terminology","title":"Terminology","text":"<ul> <li>Author: The individual who submitted the contribution.</li> <li>Reviewer: The individual reviewing the contribution.</li> <li>Maintainer: The person responsible for merging the contribution after review.</li> </ul>"},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/#understanding-a-review","title":"Understanding a Review","text":"<p>Reviews are discussions around the changes proposed in a PR. They allow for collaborative feedback, ensuring code quality and alignment with project standards.</p> <p>Tip</p> <p>Anyone can review a PR, including those who are not maintainers! If you see a PR from another author, and have suggestions for improvement, feel free to leave a review.</p> <p>For further reading, refer to the Official GitHub Documentation on PR Reviews.</p>"},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/#review-statuses","title":"Review Statuses","text":"<p>When submitting a review, you can select from three statuses:</p> <ol> <li>Comment: Provide general feedback without explicitly approving or requesting changes.</li> <li>Approve: Indicate that the changes are acceptable, and approve merging the PR.</li> <li>Request Changes: Highlight issues that need to be addressed before the PR can be merged.</li> </ol>"},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/#requesting-a-review-for-a-pull-request","title":"Requesting a Review for a Pull Request","text":"<p>After creating a PR, you can request specific individuals or teams to review it.</p> <ul> <li>Only members of the BHK Lab organization can request reviews from other members.</li> </ul> <p>For more information, check the Official GitHub Documentation on Requesting a Pull Request Review.</p>"},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/#adding-to-an-existing-document","title":"Adding to an Existing Document","text":"<p>If a document already exists, you can add to it. Check the bottom of the page for information on current authors.</p>"},{"location":"software_development/Version_Control/code_reviews/conducting_a_review/#conclusion","title":"Conclusion","text":"<p>Effective reviews help ensure that contributions meet project standards, improve code quality, and facilitate knowledge sharing within the team. Whether you're an author, reviewer, or maintainer, understanding the review process is essential to contributing successfully.</p>"},{"location":"software_development/Version_Control/code_reviews/introduction/","title":"Introduction to Code Reviews","text":"<p>Code reviews are a critical practice in software development where developers collaboratively examine each other\u2019s code to identify issues, ensure adherence  to coding standards and improve overall quality before changes are integrated  into the main codebase.</p>"},{"location":"software_development/Version_Control/code_reviews/introduction/#objectives","title":"Objectives","text":"<p>The primary objectives of code reviews are to:</p> <ul> <li> <p>Ensure Code Quality:</p> <ul> <li>Identify and fix bugs early.</li> <li>Maintain consistent coding standards.</li> <li>Improve code readability and maintainability.</li> </ul> </li> <li> <p>Facilitate Knowledge Sharing:</p> <ul> <li>Expose team members to different parts of the codebase.</li> <li>Share insights on best practices and new techniques.</li> <li>Encourage collaboration and mentoring.</li> </ul> </li> <li> <p>Enhance Team Productivity:</p> <ul> <li>Reduce technical debt through proactive feedback.</li> <li>Enable informed decision-making with diverse perspectives.</li> <li>Prevent knowledge silos by decentralizing expertise.</li> </ul> </li> <li> <p>Support Team Collaboration:</p> <ul> <li>Build trust and positive relationships among team members.</li> <li>Encourage open communication and constructive feedback.</li> <li>Foster a culture of continuous improvement.</li> </ul> </li> </ul>"},{"location":"software_development/Version_Control/code_reviews/mindsets/","title":"Mindsets in Code Reviews","text":"<p>A successful code review process depends not just on technical knowledge but also on the right mindset. The mindset impacts how authors and reviewers approach the process and influences the outcomes of a code review. Let's explore the difference between a negative mindset and a positive mindset and their effects.</p>"},{"location":"software_development/Version_Control/code_reviews/mindsets/#negative-mindset","title":"Negative Mindset","text":"<p>A negative mindset can turn code reviews into adversarial or unproductive experiences. This often leads to unnecessary tension and poor outcomes for the team.</p>"},{"location":"software_development/Version_Control/code_reviews/mindsets/#authors-perspective-self-criticism","title":"Author's Perspective (Self-Criticism)","text":"Thought Impact \"They won\u2019t understand my work.\" Resentment toward reviewers and less clarity in code. \"This is just wasting my time.\" Lack of effort in preparing quality code. \"They\u2019re nitpicking my work.\" Defensive behavior and dismissing constructive feedback."},{"location":"software_development/Version_Control/code_reviews/mindsets/#reviewers-thoughts-criticism","title":"Reviewer's Thoughts (Criticism)","text":"Thought Impact \"Another task on my plate.\" Rushed reviews with limited attention to detail. \"This is my code, and they better not ruin it.\" Resistance to change and collaboration. \"Their work is subpar.\" Criticism without constructive guidance."},{"location":"software_development/Version_Control/code_reviews/mindsets/#positive-mindset","title":"Positive Mindset","text":"<p>A positive mindset transforms code reviews into collaborative and enriching experiences. This creates better outcomes for the team and the codebase.</p>"},{"location":"software_development/Version_Control/code_reviews/mindsets/#authors-perspective-self-reflection","title":"Author's Perspective (Self-Reflection)","text":"Thought Impact \"Feedback will make my code better.\" Encourages learning and improvement. \"I should make this easy to review.\" Leads to well-prepared, clear, and concise code. \"I trust my reviewers to help me improve.\" Fosters collaboration and teamwork."},{"location":"software_development/Version_Control/code_reviews/mindsets/#reviewers-thoughts-mentorship","title":"Reviewer's Thoughts (Mentorship)","text":"Thought Impact \"They trust me to help improve their work.\" Motivates thorough and constructive feedback. \"Let\u2019s identify opportunities for learning.\" Builds a culture of mentorship and shared knowledge. \"I can learn from their approach too.\" Promotes an open exchange of ideas and perspectives. <p>By fostering a positive mindset in both authors and reviewers, teams can ensure that code reviews are efficient, productive, and an essential tool for growth and collaboration.</p>"},{"location":"software_development/Version_Control/code_reviews/presentation/","title":"Code Review Presentation","text":"<p>This presentation was presented on November 26th, 2024, and summarizes the docs on code reviews in a slide format.</p>"},{"location":"software_development/Version_Control/git/introduction/","title":"Introduction","text":"<p>Below are the slides presented at the first workshop on Git. Full screen at this link</p> <p>These slides can also be accessed through this link (you may need to request access).</p> <p>The slides cover the following topics:</p> <ul> <li>What is Git?</li> <li>Why use Git?</li> <li>What is a Git repository?<ul> <li>What's inside the <code>.git</code> directory?</li> </ul> </li> <li>Stages of the git workflow<ul> <li>Working directory</li> <li>Staging area</li> <li>Local repository</li> <li>Remote repository (GitHub)</li> </ul> </li> <li>Basic Git commands<ul> <li><code>git add</code></li> <li><code>git commit</code></li> <li><code>git push</code></li> <li><code>git fetch</code></li> <li><code>git merge</code></li> <li><code>git pull</code></li> </ul> </li> </ul> <p>The second workshop on Code Reviews can be found this handbook page.</p>"},{"location":"software_development/Version_Control/git/quick_git/","title":"Quick Git Tips","text":""},{"location":"software_development/Version_Control/git/quick_git/#1-switch-branches-without-losing-changes","title":"1. Switch Branches Without Losing Changes","text":"<p>Problem: Switching branches with uncommitted changes. Solution: Stash the changes and reapply them later:</p> <pre><code>git stash\ngit stash apply\n</code></pre> <p>For more information on stashing, see git's documentation on stashing.</p>"},{"location":"software_development/Version_Control/git/quick_git/#2-view-commit-history-across-branches","title":"2. View Commit History Across Branches","text":"<p>Problem: Viewing a detailed log of commits and branches. Solution: Git's <code>log</code> command comes with many options to visualize commit history in a pretty way:</p> <pre><code>git log --oneline --graph --decorate --all\n</code></pre> <p>In the handbook, we have a dedicated branch that handles the deployment of the website called <code>gh-pages</code>. To avoid seeing this branch in your git log, you can use the following command:</p> <pre><code>git log --exclude=\"*/gh-pages\" --graph --oneline --all\n</code></pre> <p>For more information on the <code>log</code> command, see git's documentation on log.</p>"},{"location":"software_development/Version_Control/git/quick_git/#3-edit-the-last-commit","title":"3. Edit the Last Commit","text":"<p>Problem: You committed changes but realize you forgot something. Solution: Amend the last commit with:</p> <pre><code>git commit --amend\n</code></pre> <p>This doc by Atlassian does a great job explaining different ways to rewrite history in git.</p> <p>For more information on git commits and their options, see git's documentation commits.</p>"},{"location":"software_development/Version_Control/git/quick_git/#4-check-for-merge-conflicts","title":"4. Check for Merge Conflicts","text":"<p>Problem: Identifying conflicts before merging branches. Solution: Use a dry-run merge to check for conflicts:</p> <pre><code>git merge --no-commit --no-ff &lt;branch&gt;\n</code></pre>"},{"location":"software_development/Version_Control/git/quick_git/#5-simplify-commands-with-aliases","title":"5. Simplify Commands with Aliases","text":"<p>Problem: Repeatedly typing long Git commands. Solution: Create shortcuts for common commands:</p> <pre><code>git config --global alias.co checkout\ngit config --global alias.st status\n</code></pre> <p>This allows you to shorten commands like <code>git checkout</code> to <code>git co</code>.</p> <pre><code>git co &lt;branch&gt; \n</code></pre> <p>You can even add common options to your aliases. For example, to always use the <code>-m</code> flag when committing, you can set up an alias like this:</p> <pre><code>git config --global alias.cm 'commit -m'\n</code></pre> <p>Now to write a commit message, you can use:</p> <pre><code>git cm \"Your commit message here\"\n</code></pre> <p>You can see all the aliases you have set up by running:</p> <pre><code>git config --global --get-regexp alias\n</code></pre>"},{"location":"software_development/Version_Control/git/quick_git/#6-remove-local-branches-that-have-been-deleted-remotely","title":"6. Remove local branches that have been deleted remotely","text":"<p>Problem: You or another lab member have deleted a branch on the remote repository, but it still shows up in your local repository. Solution: Use the following command to remove local branches that have been deleted remotely:</p> <pre><code>git fetch --prune\n</code></pre> <p>This command will remove all local branches that have been deleted on the remote repository.</p>"}]}